{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from tqdm import tqdm\n",
    "import logging as log\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data retrieval from Semantic Scholar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Load your api key\n",
    "with open('api_keys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "key = api_keys['semantic_scholar']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "suffix = '' #'_04_01_2023'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'data', 'requests', suffix)\n",
    "os.mkdir(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "global req_counter\n",
    "req_counter = 0\n",
    "# CLASS calls per second\n",
    "CALLS = 500 # 950\n",
    "RATE_LIMIT = 1\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    \"\"\" Empty function just to check for calls to API \"\"\"\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def request_paper_information(paperId, fields):\n",
    "    \"\"\"requests GENERAL INFO for a given paper in data\"\"\"\n",
    "    global key\n",
    "    global req_counter\n",
    "    req_counter += 1\n",
    "    check_limit()\n",
    "    time.sleep(0.01)\n",
    "    req_str = f'https://api.semanticscholar.org/graph/v1/paper/{paperId}?fields={fields}'\n",
    "    print(req_str)\n",
    "    req = requests.get(req_str, headers={'x-api-key':key}, timeout=600).json()\n",
    "    if 'message' in req.keys():\n",
    "        log.error(req['message'])\n",
    "        raise Exception('Probs too many requests :(')\n",
    "    return paperId, req\n",
    "\n",
    "def request_citation_information(paperId, offset, fields, limit):\n",
    "    \"\"\"requests CITATION INFO for a given paper in data\"\"\"\n",
    "    global key\n",
    "    global req_counter\n",
    "    req_counter += 1\n",
    "    check_limit()\n",
    "    time.sleep(0.01)\n",
    "    req = requests.get(f'https://api.semanticscholar.org/graph/v1/paper/{paperId}/citations?fields={fields}&offset={offset}&limit={limit}', headers={'x-api-key':key}, timeout=600).json()\n",
    "    if 'message' in req.keys():\n",
    "        log.error(req['message'])\n",
    "        raise Exception('Probs too many requests :(')\n",
    "    if 'error' in req.keys():\n",
    "        req = 'error'\n",
    "        return paperId, req\n",
    "    if 'data' in req.keys():\n",
    "        for i in range(len(req['data'])):\n",
    "            req['data'][i] = req['data'][i]['citingPaper']\n",
    "        req = req['data']\n",
    "    else:\n",
    "        log.info(f'data not in keys: Response keys: {req[\"error\"]}')\n",
    "    return paperId, req\n",
    "\n",
    "def request_author_citations(authorId, offset, fields):\n",
    "    \"\"\"returns a paper+year and its citations+year for a given authorId\"\"\"\n",
    "    global key\n",
    "    global req_counter\n",
    "    req_counter += 1\n",
    "    check_limit()\n",
    "    req = requests.get(f'https://api.semanticscholar.org/graph/v1/author/{authorId}/papers?fields={fields}&offset={offset}&limit=1000',headers={'x-api-key':key}, timeout=600).json()\n",
    "    if 'message' in req.keys():\n",
    "        log.error(req['message'])\n",
    "        raise Exception('Probs too many requests :(')\n",
    "    if 'error' in req.keys():\n",
    "        req = 'error'\n",
    "        return authorId, req\n",
    "    if 'data' in req.keys():\n",
    "        log.info(req['data'])\n",
    "        #for id in range(len(req['data'])):\n",
    "        #    req['data'][id] = req['data'][id]['citingPaper']\n",
    "        req = req['data']\n",
    "    else:\n",
    "        log.info(f'data not in keys: Response keys: {req[\"error\"]}')\n",
    "    return authorId, req\n",
    "#request_author_citations(author_id='47237027', fields='year,citations.year')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_author_citations(authorId, fields):\n",
    "    \"\"\"handels the request and ensures that for papers with more than 1k citations all requests are made\"\"\"\n",
    "    offset = 0\n",
    "    limit = 1000\n",
    "    authorId, req = request_author_citations(authorId, offset, fields)\n",
    "    while len(req)%1000 == 0:\n",
    "        offset += 1000\n",
    "        if offset+limit >= 10000:\n",
    "            break\n",
    "        authorId, next_request = request_author_citations(authorId, offset, fields)\n",
    "        req = req + next_request\n",
    "\n",
    "    return authorId, req\n",
    "\n",
    "def get_citation_information(paperId, fields):\n",
    "    \"\"\"handels the request and ensures that for papers with more than 1k citations all requests are made, returns list with the requests\"\"\"\n",
    "    offset = 0\n",
    "    limit = 1000\n",
    "    paperId, req = request_citation_information(paperId, offset, fields, limit)\n",
    "    while len(req)%1000 == 0:\n",
    "        offset += 1000\n",
    "        if offset+limit >= 10000:\n",
    "            break\n",
    "        paperId, next_request = request_citation_information(paperId, offset, fields, limit)\n",
    "        req = req + next_request\n",
    "\n",
    "    return paperId, req"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def original_paper_side(paper):\n",
    "    \"\"\"gets general information, e.g. authors,venue... , on the paper that first presented a dataset\"\"\"\n",
    "    try:\n",
    "        idx, paperInfo = request_paper_information(paper[\"paperId\"], fields='authors,venue,year,isOpenAccess,citationCount,references,references.year')\n",
    "        if 'paperInfo' in paper.keys():\n",
    "            paperInfo.append(paper['paperInfo']) #entire new function that iterates over all potential parts cits, refs, atuhs and adds TODO: ask on slack?\n",
    "        paper.update({'paperInfo': paperInfo})\n",
    "        return  paper, True\n",
    "    except Exception as exec:\n",
    "        log.error(exec)\n",
    "        return paper, False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def requests_w_workers(paper, function=request_paper_information, side='references', field='year'):\n",
    "    \"\"\"coordinates multithreading for many parallel requests for a paper and a function (e.g. author side)\"\"\"\n",
    "    org_number = len(paper['paperInfo'][side])\n",
    "    c=0\n",
    "    if side == 'authors':\n",
    "        interestId = 'authorId'\n",
    "        insertion_key = 'authorsPapers'\n",
    "        insertion_value ='data'\n",
    "    else:\n",
    "        interestId = 'paperId'\n",
    "        insertion_key = 'citations'\n",
    "        insertion_value ='citations'\n",
    "    log.info(f'Starting requests for {side}')\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_url = {executor.submit(function, paper['paperInfo'][side][ref_idx][interestId], fields=field):ref_idx for ref_idx, reference in enumerate(paper['paperInfo'][side])}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            #res = future_to_url[future]\n",
    "            c += 1\n",
    "            try:\n",
    "                 paperId, ls_response = future.result()\n",
    "                 #log.warning(ls_response)\n",
    "                 if ls_response == 'error':\n",
    "                     continue\n",
    "                 for ref_idx, reference in enumerate(paper['paperInfo'][side]):\n",
    "                     if paper['paperInfo'][side][ref_idx][interestId] == paperId:\n",
    "                         #checking for matching reference paperId to insert at correct position because ref_idx is the position in the list of my pata structure and not the sescho id.\n",
    "                         paper['paperInfo'][side][ref_idx][insertion_key] = ls_response # paper['paperInfo']['citations'][0]['citations']\n",
    "            except Exception as exc:\n",
    "                log.error(f'{paper[\"id\"]} generated an exception: {exc}', ls_response)\n",
    "    if c != org_number:\n",
    "        #raise Exception('Somehow lost a request on the way', c-org_number)\n",
    "        log.error('Somehow lost a request on the way')\n",
    "        log.error(c-org_number)\n",
    "    return paper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_all_data_for_paper(req_paper):\n",
    "    \"\"\"\"Retrieves all relevant data and adds it to the json\"\"\"\n",
    "    t1 = time.time()\n",
    "    #array to track if and what is missing. Usually empty\n",
    "    missing_info = []\n",
    "    # get general Information\n",
    "    req_paper, has_paper_info = original_paper_side(req_paper)\n",
    "    # get all citations for paper (also if there is more than 1k)\n",
    "    paperId, req = get_citation_information(req_paper['paperId'], 'year')\n",
    "    req_paper['paperInfo']['citations'] = req\n",
    "    #check if we got all citations that are counted in the citationCount by SemanticScholar\n",
    "    if req_paper['paperInfo']['citationCount'] != len(req_paper['paperInfo']['citations']):\n",
    "        log.error('Missing a/some citation! Semantic scholar implied', req_paper['paperInfo']['citationCount'] , 'but only got', len(req_paper['paperInfo']['citations']))\n",
    "        log.error(req_paper['paperInfo']['citationCount'] != len(req_paper['paperInfo']['citations']))\n",
    "    if has_paper_info:\n",
    "        if 'citations' in req_paper['paperInfo'].keys():\n",
    "           req_paper = requests_w_workers(req_paper, function=get_citation_information, side='citations', field='year')\n",
    "        else:\n",
    "            missing_info.append((req_paper['id'], 'citations'))\n",
    "        if 'references' in req_paper['paperInfo'].keys():\n",
    "            req_paper = requests_w_workers(req_paper, function=get_citation_information, side='references', field='year')\n",
    "        else:\n",
    "            missing_info.append((req_paper['id'], 'references'))\n",
    "        if 'citations' in req_paper['paperInfo'].keys():\n",
    "           req_paper = requests_w_workers(req_paper, function=get_author_citations, side='authors', field='year,citations.year')\n",
    "        else:\n",
    "            missing_info.append((req_paper['id'], 'citations'))\n",
    "    else:\n",
    "        missing_info.append((req_paper['id'], 'paperInfo'))\n",
    "    t2 = time.time()\n",
    "    log.info(f\"Took {t2 - t1} seconds for {req_paper['id']}\")\n",
    "    return req_paper, missing_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Set verbosity level\n",
    "verbose = False\n",
    "if verbose:\n",
    "    log.basicConfig(format=\"%(levelname)s: %(message)s\", level=log.DEBUG)\n",
    "    log.info(\"Verbose output.\")\n",
    "else:\n",
    "    log.basicConfig(format=\"%(levelname)s: %(message)s\")\n",
    "    log.getLogger(\"requests\").setLevel(log.WARNING)\n",
    "log.getLogger(\"requests\").setLevel(log.WARNING)\n",
    "log.getLogger(\"urllib3\").setLevel(log.WARNING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.semanticscholar.org/graph/v1/paper/4f0b8f730273e9f11b2bfad2415485414b96299f?fields=authors,venue,year,isOpenAccess,citationCount,references,references.year\n"
     ]
    }
   ],
   "source": [
    "# Open json with datasets\n",
    "\n",
    "file_name = f'data/data_sorted_only_w_ids{suffix}_altmetrics.json'\n",
    "with open(file_name, \"r\") as ds:\n",
    "    data = json.load(ds)\n",
    "res, missing_info = get_all_data_for_paper(data[9])\n",
    "#res['paperInfo']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(file_name, \"r\") as ds:\n",
    "    data = json.load(ds)\n",
    "for o_paper_idx in tqdm(range(len(data))):\n",
    "    log.info(f'starting for paper {data[o_paper_idx][\"id\"]}')\n",
    "    try:\n",
    "        res, missing_info = get_all_data_for_paper(data[o_paper_idx])\n",
    "        name = ''.join(e for e in data[o_paper_idx][\"id\"] if e.isalnum())\n",
    "        with open(f'data/requests{suffix}/req{name}.json', \"w\") as f:\n",
    "            json.dump(res, f, indent=4, sort_keys=True)\n",
    "    except Exception as exec:\n",
    "        log.error('Failed with req', data[o_paper_idx]['id'], exec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}