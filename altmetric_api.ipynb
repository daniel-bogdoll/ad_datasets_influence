{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Load your api key\n",
    "with open('api_keys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "alt_metric_key = api_keys['alt_metric']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m doi_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m10.1177\u001B[39m\u001B[38;5;132;01m%2F\u001B[39;00m\u001B[38;5;124m0278364913491297\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://api.altmetric.com/v1/doi/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdoi_test\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m?key=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malt_metric_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m json_url \u001B[38;5;241m=\u001B[39m \u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(json_url\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m      5\u001B[0m data\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:214\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    213\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:523\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_response\u001B[38;5;241m.\u001B[39mget(protocol, []):\n\u001B[0;32m    522\u001B[0m     meth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[1;32m--> 523\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:632\u001B[0m, in \u001B[0;36mHTTPErrorProcessor.http_response\u001B[1;34m(self, request, response)\u001B[0m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001B[39;00m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;66;03m# request was successfully received, understood, and accepted.\u001B[39;00m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m code \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m):\n\u001B[1;32m--> 632\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttp\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:561\u001B[0m, in \u001B[0;36mOpenerDirector.error\u001B[1;34m(self, proto, *args)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_err:\n\u001B[0;32m    560\u001B[0m     args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_error_default\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m orig_args\n\u001B[1;32m--> 561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:494\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    493\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 494\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    496\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py:641\u001B[0m, in \u001B[0;36mHTTPDefaultErrorHandler.http_error_default\u001B[1;34m(self, req, fp, code, msg, hdrs)\u001B[0m\n\u001B[0;32m    640\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[1;32m--> 641\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req\u001B[38;5;241m.\u001B[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001B[1;31mHTTPError\u001B[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "doi_test = '10.1177%2F0278364913491297'\n",
    "url = f'https://api.altmetric.com/v1/doi/{doi_test}?key={alt_metric_key}'\n",
    "json_url = urlopen(url)\n",
    "data = json.loads(json_url.read())\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "doi_test = '10.1177%2F0278364913491297'\n",
    "url = f'https://api.altmetric.com/v1/fetch/doi/{doi_test}?key={alt_metric_key}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'altmetric_id': 3801312,\n 'counts': {'readers': {'citeulike': '0', 'mendeley': '1969', 'connotea': '0'},\n  'total': {'posts_count': 60},\n  'twitter': {'unique_users_count': 1, 'posts_count': 11},\n  'patent': {'unique_users_count': 49,\n   'unique_users': ['patent:US-10019801-B2',\n    'patent:US-10328769-B2',\n    'patent:US-10331124-B2',\n    'patent:US-10354406-B2',\n    'patent:US-10380889-B2',\n    'patent:US-10469828-B2',\n    'patent:US-10507787-B2',\n    'patent:US-10520593-B2',\n    'patent:US-10599156-B2',\n    'patent:US-10705193-B2',\n    'patent:WO-2020142328-A1',\n    'patent:US-10824862-B2',\n    'patent:US-10859682-B2',\n    'patent:WO-2020247075-A1',\n    'patent:US-10867210-B2',\n    'patent:US-20210004566-A1',\n    'patent:US-10929996-B2',\n    'patent:US-10949989-B2',\n    'patent:US-10970871-B2',\n    'patent:US-10977501-B2',\n    'patent:US-11009868-B2',\n    'patent:US-11037051-B2',\n    'patent:FR-3104292-A1',\n    'patent:US-11061116-B2',\n    'patent:US-11094072-B2',\n    'patent:US-11107230-B2',\n    'patent:US-11138751-B2',\n    'patent:US-11145074-B2',\n    'patent:US-11164012-B2',\n    'patent:US-11176709-B2',\n    'patent:US-11210802-B2',\n    'patent:US-11250489-B2',\n    'patent:US-11294035-B2',\n    'patent:US-11295517-B2',\n    'patent:US-11321863-B2',\n    'patent:EP-4002215-A1',\n    'patent:US-11361187-B1',\n    'patent:WO-2022117199-A1',\n    'patent:US-11370423-B2',\n    'patent:US-11386567-B2',\n    'patent:US-11403776-B2',\n    'patent:US-11423255-B2',\n    'patent:US-11422265-B2',\n    'patent:US-11436743-B2',\n    'patent:US-11450120-B2',\n    'patent:US-11449050-B2',\n    'patent:WO-2022201212-A1',\n    'patent:US-11467574-B2',\n    'patent:US-11481912-B2'],\n   'posts_count': 49}},\n 'citation': {'altmetric_jid': '4f6fa4ed3cf058f610002b9d',\n  'authors': ['A Geiger', 'P Lenz', 'C Stiller', 'R Urtasun'],\n  'doi': '10.1177/0278364913491297',\n  'endpage': '1237',\n  'first_seen_on': '2015-03-18T02:00:36+00:00',\n  'issns': ['0278-3649', '1741-3176'],\n  'issue': '11',\n  'journal': 'The International Journal of Robotics Research',\n  'last_mentioned_on': 1666656000,\n  'links': ['http://ijr.sagepub.com/content/32/11/1231.short?rss=1&ssource=mfr',\n   'https://doi.org/10.1177/0278364913491297',\n   'https://journals.sagepub.com/doi/full/10.1177/0278364913491297?utm_source=SAGE_social&utm_medium=&utm_campaign=&hootPostID=6c7c7b7f830cf734882ae2e7ea804321'],\n  'pdf_url': 'http://ijr.sagepub.com/content/32/11/1231.full.pdf',\n  'pubdate': '2013-09-01T00:00:00+00:00',\n  'epubdate': '2013-08-23T00:00:00+00:00',\n  'published_on': '2013-08-23T00:00:00+00:00',\n  'publisher_subjects': [{'name': 'Artificial Intelligence And Image Processing',\n    'scheme': 'era'},\n   {'name': 'Electrical And Electronic Engineering', 'scheme': 'era'},\n   {'name': 'Mechanical Engineering', 'scheme': 'era'}],\n  'scopus_subjects': ['Mathematics',\n   'Physical Sciences',\n   'Computer Science',\n   'Engineering'],\n  'startpage': '1231',\n  'title': 'Vision meets robotics: The KITTI dataset',\n  'type': 'article',\n  'volume': '32',\n  'mendeley_url': 'https://www.mendeley.com/catalogue/20d7e081-1dff-3dcc-aafb-8452f935fda7/'},\n 'altmetric_score': {'score': 12.25,\n  'score_history': {'1y': 9,\n   '6m': 9,\n   '3m': 6,\n   '1m': 3,\n   '1w': 0,\n   '6d': 0,\n   '5d': 0,\n   '4d': 0,\n   '3d': 0,\n   '2d': 0,\n   '1d': 0,\n   'at': 12.25},\n  'context_for_score': {'all': {'total_number_of_other_articles': 22385407,\n    'mean': 10.131079131788221,\n    'rank': 2363256,\n    'this_scored_higher_than_pct': 89,\n    'this_scored_higher_than': 20013241,\n    'rank_type': 'exact',\n    'sample_size': 22385407,\n    'percentile': 89},\n   'similar_age_3m': {'total_number_of_other_articles': 178733,\n    'mean': 9.17483100490693,\n    'rank': 20193,\n    'this_scored_higher_than_pct': 88,\n    'this_scored_higher_than': 158428,\n    'rank_type': 'exact',\n    'sample_size': 178733,\n    'percentile': 88},\n   'this_journal': {'total_number_of_other_articles': 914,\n    'mean': 4.974297592997812,\n    'rank': 54,\n    'this_scored_higher_than_pct': 93,\n    'this_scored_higher_than': 859,\n    'rank_type': 'exact',\n    'sample_size': 914,\n    'percentile': 93},\n   'similar_age_this_journal_3m': {'total_number_of_other_articles': 15,\n    'mean': 5.068666666666667,\n    'rank': 2,\n    'this_scored_higher_than_pct': 86,\n    'this_scored_higher_than': 13,\n    'rank_type': 'exact',\n    'sample_size': 15,\n    'percentile': 86}}},\n 'demographics': {'poster_types': {'member_of_the_public': 1},\n  'users': {'twitter': {'cohorts': {'Members of the public': 1}},\n   'mendeley': {'by_status': {'Student  > Postgraduate': 48,\n     'Professor > Associate Professor': 20,\n     'Researcher': 176,\n     'Student  > Master': 489,\n     'Student  > Ph. D. Student': 465,\n     'Professor': 19,\n     'Student  > Bachelor': 158,\n     'Student  > Doctoral Student': 71,\n     'Lecturer': 18,\n     'Other': 40,\n     'Librarian': 3,\n     'Lecturer > Senior Lecturer': 5,\n     'Unspecified': 18},\n    'by_discipline': {'Materials Science': 2,\n     'Medicine and Dentistry': 3,\n     'Social Sciences': 2,\n     'Decision Sciences': 1,\n     'Physics and Astronomy': 11,\n     'Psychology': 8,\n     'Mathematics': 10,\n     'Environmental Science': 2,\n     'Unspecified': 19,\n     'Pharmacology, Toxicology and Pharmaceutical Science': 1,\n     'Chemical Engineering': 4,\n     'Arts and Humanities': 4,\n     'Design': 2,\n     'Engineering': 613,\n     'Chemistry': 5,\n     'Neuroscience': 5,\n     'Energy': 1,\n     'Earth and Planetary Sciences': 6,\n     'Economics, Econometrics and Finance': 3,\n     'Computer Science': 768,\n     'Agricultural and Biological Sciences': 8,\n     'Nursing and Health Professions': 1,\n     'Business, Management and Accounting': 3}}},\n  'geo': {'mendeley': {'HR': 1,\n    'US': 3,\n    'JP': 2,\n    'ES': 4,\n    'CN': 1,\n    'DK': 1,\n    'MX': 1,\n    'SG': 1,\n    'CA': 1,\n    'GB': 3,\n    'CZ': 2,\n    'FI': 2,\n    'IL': 1,\n    'SE': 1,\n    'BR': 1,\n    'AU': 2,\n    'AT': 1,\n    'KR': 2,\n    'FR': 6,\n    'NL': 1,\n    'DE': 2,\n    'PT': 1}}},\n 'posts': {'twitter': [{'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '578012977690898432'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '599922342576328705'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '600752815833677825'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '606636585245736960'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '615686216814477312'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '617040138292432896'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '618907442462613504'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '637802069261348864'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '639880678063017984'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '648195444321841153'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '666274602956386305'}],\n  'patent': [{'title': 'Image analysis system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-10019801-B2',\n    'license': 'public',\n    'citation_ids': [49782009,\n     35641125,\n     12902049,\n     12906237,\n     49782035,\n     3801312,\n     49782029,\n     49782026,\n     49782022],\n    'posted_on': '2018-07-10T00:00:00+00:00',\n    'summary': 'A method of determining a camera movement expressed in terms of an absolute transformation of a camera state from a reference state, including receiving a stream of video images captured from a camera; producing a stream of odometry measurements from the v',\n    'ucid': 'US-10019801-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Methods for interacting with autonomous or semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-10328769-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a system for precise delivery of an item to a consumer by an autonomous or semi-autonomous vehicle. The system performs image recognition algorithms to detect a primary entrance, navigate from a street address to the primary entrance, an',\n    'ucid': 'US-10328769-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Autonomous vehicle repositioning',\n    'url': 'https://app.dimensions.ai/details/patent/US-10331124-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a platform for distributing and navigating an autonomous or semi-autonomous fleet throughout a plurality of pathways. The platform may employ demand distribution prediction algorithms, and interim repositioning algorithms to distribute t',\n    'ucid': 'US-10331124-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Method of detecting objects within a 3D environment',\n    'url': 'https://app.dimensions.ai/details/patent/US-10354406-B2',\n    'license': 'public',\n    'citation_ids': [41865941, 43004858, 35312133, 3801312, 15431844],\n    'posted_on': '2019-07-16T00:00:00+00:00',\n    'summary': 'A method and system for detecting objects within a three-dimensional (3D) environment, comprising obtaining a 3D point-cloud representation of the environment, the point-cloud comprising a set of point locations, and converting the point-cloud to a 3D feat',\n    'ucid': 'US-10354406-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Determining car positions',\n    'url': 'https://app.dimensions.ai/details/patent/US-10380889-B2',\n    'license': 'public',\n    'citation_ids': [35433612, 3801312, 81094544, 80495985, 267002],\n    'posted_on': '2019-08-13T00:00:00+00:00',\n    'summary': 'Examples provided herein describe a method for determining car positions. For example, a physical processor of an edge computing device may receive position data for a legacy car and information about a make and model of the legacy car. The first edge devi',\n    'ucid': 'US-10380889-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional dense structure from motion with stereo vision',\n    'url': 'https://app.dimensions.ai/details/patent/US-10469828-B2',\n    'license': 'public',\n    'citation_ids': [81181891, 35182124, 3801312, 24853916, 12059625],\n    'posted_on': '2019-11-05T00:00:00+00:00',\n    'summary': 'Disclosed examples include three-dimensional imaging systems and methods to reconstruct a three-dimensional scene from first and second image data sets obtained from a single camera at first and second times, including computing feature point correspondenc',\n    'ucid': 'US-10469828-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'System and mechanism for upselling products on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-10507787-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-12-17T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles for containing, securing, and delivering at least one of a first item and at least one of a second item after a customer plac',\n    'ucid': 'US-10507787-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Installation and use of vehicle light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10520593-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2019-12-31T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10520593-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Advertising on autonomous or semi-autonomous vehicle exterior',\n    'url': 'https://app.dimensions.ai/details/patent/US-10599156-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-03-24T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of electric autonomous vehicle for apportioned display of a media, operating autonomously and a fleet management module for coordination of the autonomous vehicle flee',\n    'ucid': 'US-10599156-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Monitoring of vehicles using light ranging systems',\n    'url': 'https://app.dimensions.ai/details/patent/US-10705193-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-07-07T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10705193-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'IMAGE BOUNDING SHAPE USING 3D ENVIRONMENT REPRESENTATION',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020142328-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 99637399, 99637402],\n    'posted_on': '2020-07-09T00:00:00+00:00',\n    'summary': 'A computing system is provided, including one or more optical sensors, a display, one or more user input devices, and a processor. The processor may receive optical data of a physical environment. Based on the optical data, the processor may generate a thr',\n    'ucid': 'WO-2020142328-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional object detection for autonomous robotic systems using image proposals',\n    'url': 'https://app.dimensions.ai/details/patent/US-10824862-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-11-03T00:00:00+00:00',\n    'summary': 'Provided herein are methods and systems for implementing three-dimensional perception in an autonomous robotic system comprising an end-to-end neural network architecture that directly consumes large-scale raw sparse point cloud data and performs such task',\n    'ucid': 'US-10824862-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Telematics using a light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10859682-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-12-08T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10859682-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'NOVEL POSE SYNTHESIS',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020247075-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 100116890, 62459169, 81095701, 100116898],\n    'posted_on': '2020-12-10T00:00:00+00:00',\n    'summary': 'Examples are disclosed that relate to computing devices and methods for synthesizing a novel pose of an object. One example provides a method comprising receiving a reference image of an object corresponding to an original viewpoint. The reference image of',\n    'ucid': 'WO-2020247075-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-10867210-B2',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     30140746,\n     10897749,\n     39315381],\n    'posted_on': '2020-12-15T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-10867210-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD AND APPARATUS FOR 3D OBJECT BOUNDING FOR 2D IMAGE DATA',\n    'url': 'https://app.dimensions.ai/details/patent/US-20210004566-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 27031120, 100279283, 100279290, 35407856],\n    'posted_on': '2021-01-07T00:00:00+00:00',\n    'summary': 'Methods and apparatus are provided for 3D object bounding for 2D image data for use in an assisted driving equipped vehicle. In various embodiments, an apparatus includes a camera operative to capture a two dimensional image of a field of view, a lidar ope',\n    'ucid': 'US-20210004566-A1',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Image depth prediction neural networks',\n    'url': 'https://app.dimensions.ai/details/patent/US-10929996-B2',\n    'license': 'public',\n    'citation_ids': [835489, 3801312, 35416550, 35349078, 42711774, 99823301],\n    'posted_on': '2021-02-23T00:00:00+00:00',\n    'summary': 'A system includes an image depth prediction neural network implemented by one or more computers. The image depth prediction neural network is a recurrent neural network that is configured to receive a sequence of images and, for each image in the sequence:',\n    'ucid': 'US-10929996-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Concept for determining a confidence/uncertainty measure for disparity measurement',\n    'url': 'https://app.dimensions.ai/details/patent/US-10949989-B2',\n    'license': 'public',\n    'citation_ids': [914846,\n     81129890,\n     105570844,\n     33315001,\n     81129920,\n     3801312,\n     81129879],\n    'posted_on': '2021-03-16T00:00:00+00:00',\n    'summary': 'A more effective confidence/uncertainty measure determination for disparity measurements is achieved by performing the determination on an evaluation of a set of disparity candidates for a predetermined position of a first picture at which the measurement',\n    'ucid': 'US-10949989-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': \"Estimating two-dimensional object bounding box information based on bird's-eye view point cloud\",\n    'url': 'https://app.dimensions.ai/details/patent/US-10970871-B2',\n    'license': 'public',\n    'citation_ids': [43402617, 105411112, 3801312],\n    'posted_on': '2021-04-06T00:00:00+00:00',\n    'summary': 'Upon receiving a set of two-dimensional data points representing an object in an environment, a bounding box estimator estimates a bounding box vector representative of a two-dimensional version of the object that is represented by the two-dimensional data',\n    'ucid': 'US-10970871-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object classification using extra-regional context',\n    'url': 'https://app.dimensions.ai/details/patent/US-10977501-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 99631377, 10897749],\n    'posted_on': '2021-04-13T00:00:00+00:00',\n    'summary': 'Some aspects of the subject matter disclosed herein include a system implemented on one or more data processing apparatuses. The system can include an interface configured to obtain, from one or more sensor subsystems, sensor data describing an environment',\n    'ucid': 'US-10977501-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Fleet of autonomous vehicles with lane positioning and platooning behaviors',\n    'url': 'https://app.dimensions.ai/details/patent/US-11009868-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2021-05-18T00:00:00+00:00',\n    'summary': 'Disclosed herein are systems for navigating an autonomous or semi-autonomous fleet comprising a plurality of autonomous or semi-autonomous vehicles within a plurality of navigable pathways within an unstructured open environment.',\n    'ucid': 'US-11009868-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': '3D plane detection and reconstruction using a monocular image',\n    'url': 'https://app.dimensions.ai/details/patent/US-11037051-B2',\n    'license': 'public',\n    'citation_ids': [108127066,\n     108127070,\n     1821746,\n     10027341,\n     2465542,\n     3737169,\n     108127080,\n     32069491,\n     3801312,\n     108127089],\n    'posted_on': '2021-06-15T00:00:00+00:00',\n    'summary': 'Planar regions in three-dimensional scenes offer important geometric cues in a variety of three-dimensional perception tasks such as scene understanding, scene reconstruction, and robot navigation. Image analysis to detect planar regions can be performed b',\n    'ucid': 'US-11037051-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Method of configuring an imaging device of a motor vehicle comprising an optical image capture device',\n    'url': 'https://app.dimensions.ai/details/patent/FR-3104292-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 112451882, 112451886],\n    'posted_on': '2021-06-11T00:00:00+00:00',\n    'summary': 'The method comprises the training of a neural network RNe1 associated with the optical device C1 comprising, for each index i: A) supply of N images IMn, i respectively to N neural networks RNen, including the target network RNe1 and N- 1 ancillary network',\n    'ucid': 'FR-3104292-A1',\n    'jurisdiction': 'FR',\n    'filing_status': 'Application'},\n   {'title': 'Lidar system with image size compensation mechanism',\n    'url': 'https://app.dimensions.ai/details/patent/US-11061116-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2021-07-13T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems including an apparatus configured to translate one or more spherical lenses, an array of light sources, an array of photodetectors, or any combination thereof of a collection optical system in the Z direction (optical axis) to m',\n    'ucid': 'US-11061116-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'System and method for providing single image depth estimation based on deep neural network',\n    'url': 'https://app.dimensions.ai/details/patent/US-11094072-B2',\n    'license': 'public',\n    'citation_ids': [30174235, 32069491, 112817529, 35312688, 3801312],\n    'posted_on': '2021-08-17T00:00:00+00:00',\n    'summary': 'A method and system for determining depth information of an image are herein provided. According to one embodiment, the method includes receiving an image input, classifying the input image into a depth range of a plurality of depth ranges, and determining',\n    'ucid': 'US-11094072-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11107230-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 35252783, 12863070, 3801312],\n    'posted_on': '2021-08-31T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates from a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, flipping, by a disparity model, the monocular image to',\n    'ucid': 'US-11107230-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised training using reprojected distance loss',\n    'url': 'https://app.dimensions.ai/details/patent/US-11138751-B2',\n    'license': 'public',\n    'citation_ids': [115351779, 3801312],\n    'posted_on': '2021-10-05T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to training a depth model for monocular depth estimation. In one embodiment, a method includes generating, as part of training the depth model according to a supervised training stage, a depth',\n    'ucid': 'US-11138751-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11145074-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 28320951],\n    'posted_on': '2021-10-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, processing the monocular image',\n    'ucid': 'US-11145074-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Advanced driver assistance system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-11164012-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 35251246],\n    'posted_on': '2021-11-02T00:00:00+00:00',\n    'summary': 'A driver assistance system detects lane markings in a perspective image of a road in front of a vehicle. The driver assistance system extracts a plurality of features, in particular lane markings, from the perspective image for generating a set of feature',\n    'ucid': 'US-11164012-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for self-supervised scale-aware training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11176709-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3569738, 3801312, 28320951],\n    'posted_on': '2021-11-16T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes processing a first image of a pair according to the depth model to generate a d',\n    'ucid': 'US-11176709-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for conditioning training data to avoid learned aberrations',\n    'url': 'https://app.dimensions.ai/details/patent/US-11210802-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2021-12-28T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training for monocular depth estimation. In one embodiment, a method includes filtering disfavored images from first training data to produce second training data that is a s',\n    'ucid': 'US-11210802-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Flexible compartment design on autonomous and semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-11250489-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-02-15T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles coordinated by a fleet management module. Each vehicle may be configured to receive a modular unit, wherein the modular unit',\n    'ucid': 'US-11250489-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'LiDAR system with cylindrical lenses',\n    'url': 'https://app.dimensions.ai/details/patent/US-11294035-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems comprising a transmission optical system and a collection optical system utilizing a common lens or pieces derived from the same lens to reduce or eliminate image mismatch between the transmission optical system and the collecti',\n    'ucid': 'US-11294035-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Generating realistic point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11295517-B2',\n    'license': 'public',\n    'citation_ids': [49749311, 3801312],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating realistic full-scene point clouds. One of the methods includes obtaining an initial scene point cloud characterizing an initial scene in an envir',\n    'ucid': 'US-11295517-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using semantic features',\n    'url': 'https://app.dimensions.ai/details/patent/US-11321863-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2022-05-03T00:00:00+00:00',\n    'summary': 'Systems, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes identifying semantic features in the monocular image according to a semantic',\n    'ucid': 'US-11321863-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD TO IMPROVE SCALE CONSISTENCY AND/OR SCALE AWARENESS IN A MODEL OF SELF-SUPERVISED DEPTH AND EGO-MOTION PREDICTION NEURAL NETWORKS',\n    'url': 'https://app.dimensions.ai/details/patent/EP-4002215-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     65334273,\n     118139115,\n     3296035,\n     129819701,\n     129819720,\n     129819726,\n     81135856],\n    'posted_on': '2022-05-25T00:00:00+00:00',\n    'summary': 'A method to improve scale consistency and/or scale awareness in a model of self-supervised depth and ego-motion prediction neural networks processing a video stream of monocular images, wherein complementary GPS coordinates synchronized with the images are',\n    'ucid': 'EP-4002215-A1',\n    'jurisdiction': 'EP',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-11361187-B1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     10897749,\n     39315381,\n     30140746],\n    'posted_on': '2022-06-14T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-11361187-B1',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'A DEVICE AND A METHOD FOR REPRESENTING THE YAW ORIENTATION OF A VEHICLE VISIBLE ON AN IMAGE',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022117199-A1',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-09T00:00:00+00:00',\n    'summary': 'A device and a method for representing the yaw orientation of a vehicle visible on an image, comprising: obtaining the position in the image of a rectangular bounding box (200) surrounding the vehicle (201), obtaining the position of a vertical split line',\n    'ucid': 'WO-2022117199-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Multi-task machine-learned models for object intention determination in autonomous driving',\n    'url': 'https://app.dimensions.ai/details/patent/US-11370423-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-28T00:00:00+00:00',\n    'summary': 'Generally, the disclosed systems and methods utilize multi-task machine-learned models for object intention determination in autonomous driving applications. For example, a computing system can receive sensor data obtained relative to an autonomous vehicle',\n    'ucid': 'US-11370423-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for weakly supervised training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11386567-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312],\n    'posted_on': '2022-07-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes training the depth model according to a first stage that is self-supervised and',\n    'ucid': 'US-11386567-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Depth extraction',\n    'url': 'https://app.dimensions.ai/details/patent/US-11403776-B2',\n    'license': 'public',\n    'citation_ids': [47515225, 5064731, 9698667, 134919368, 3801312],\n    'posted_on': '2022-08-02T00:00:00+00:00',\n    'summary': 'A computer-implemented method of training a depth uncertainty estimator comprises receiving, at a training computer system, a set of training examples, each training example comprising (i) a stereo image pair and (ii) an estimated disparity map computed fr',\n    'ucid': 'US-11403776-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Image processing',\n    'url': 'https://app.dimensions.ai/details/patent/US-11423255-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'The present disclosure pertains generally to image feature extraction. Both transfer-learning and multi-task training approaches are considered. In one example, a machine learning model is trained to perform a geographic classification task of distinguishi',\n    'ucid': 'US-11423255-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Driver visualization and semantic monitoring of a vehicle using LiDAR data',\n    'url': 'https://app.dimensions.ai/details/patent/US-11422265-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'Methods are provided for using a light ranging system of a vehicle. A computing system receives, from light ranging devices, ranging data including distance vectors to environmental surfaces. A distance vector can correspond to a pixel of a three-dimension',\n    'ucid': 'US-11422265-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised depth estimation according to an arbitrary camera',\n    'url': 'https://app.dimensions.ai/details/patent/US-11436743-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 20839578, 35453469, 3801312],\n    'posted_on': '2022-09-06T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model using a neural camera model that is independent of a camera type. In one embodiment, a method includes acquiring training data including at least a',\n    'ucid': 'US-11436743-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object detection in point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11450120-B2',\n    'license': 'public',\n    'citation_ids': [35264175,\n     49293181,\n     35518902,\n     50229939,\n     4063957,\n     49773333,\n     49801958,\n     99595494,\n     3801312,\n     15431844,\n     3124500,\n     6758458,\n     755649,\n     6588544],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing point cloud data representing a sensor measurement of a scene captured by one or more sensors to generate an object detection output that identif',\n    'ucid': 'US-11450120-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Real-time violations and safety monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11449050-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a real-time human behavior analysis of an unmanned vehicle by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment. The platforms determine a real-time pa',\n    'ucid': 'US-11449050-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD FOR DETERMINING THE DEPTH FROM A SINGLE IMAGE AND SYSTEM THEREOF',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022201212-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     75440920,\n     35252783,\n     12059419,\n     81101466,\n     58400615,\n     3737169,\n     133366195,\n     30027655,\n     65334273,\n     137709556,\n     34274690,\n     5971430,\n     132845677,\n     2687501,\n     129259979,\n     30174235,\n     35590071,\n     118139115,\n     109788541,\n     137709597,\n     4850006,\n     137709617,\n     137709629,\n     129818727,\n     81210194,\n     100661533,\n     137709649,\n     137709662,\n     26382884,\n     81135856,\n     32068659],\n    'posted_on': '2022-09-29T00:00:00+00:00',\n    'summary': 'Method for determining the depth from a single image and system thereof The present invention relates to a computer-implemented method (2) for determining the depth of a digital image (I), wherein said method (2) comprises the step of training (20) a neura',\n    'ucid': 'WO-2022201212-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Infrastructure monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11467574-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-10-11T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a non-navigational quality of at least one infrastructure by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment.',\n    'ucid': 'US-11467574-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Device for generating a depth map',\n    'url': 'https://app.dimensions.ai/details/patent/US-11481912-B2',\n    'license': 'public',\n    'citation_ids': [112453217, 137975534, 3737169, 80667646, 3801312],\n    'posted_on': '2022-10-25T00:00:00+00:00',\n    'summary': 'A device includes an encoder configured to generate a plurality of feature data by encoding an image; a bottleneck circuit configured to generate enhanced feature data from first bottleneck data among the plurality of feature data; and a decoder configured',\n    'ucid': 'US-11481912-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'}]},\n 'score': 12.25,\n 'images': {'small': 'https://badges.altmetric.com/?size=64&score=13&types=aaaaaaat',\n  'medium': 'https://badges.altmetric.com/?size=100&score=13&types=aaaaaaat',\n  'large': 'https://badges.altmetric.com/?size=180&score=13&types=aaaaaaat'}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url).json()\n",
    "#publishing dates only for patents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_altmetrics(retrieved_data, o_paper_idx):\n",
    "    url = f'https://api.altmetric.com/v1/fetch/doi/{retrieved_data[o_paper_idx][\"DOI\"]}?key={alt_metric_key}'\n",
    "    #print(url)\n",
    "    return requests.get(url).json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open('data/data_sorted_w_ids.json', \"r\") as ds:\n",
    "    data = json.load(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.25\n",
      "3\n",
      "9\n",
      "3\n",
      "4\n",
      "1.5\n",
      "9\n",
      "3.5\n",
      "1.5\n",
      "3\n",
      "10\n",
      "3\n",
      "12\n",
      "13\n",
      "12\n",
      "15\n",
      "0\n",
      "0\n",
      "3\n",
      "19\n",
      "20\n",
      "9\n",
      "27.200000000000003\n",
      "23\n",
      "0.5\n",
      "6\n",
      "26\n",
      "27\n",
      "6\n",
      "29\n",
      "3\n",
      "3\n",
      "32\n",
      "3\n",
      "34\n",
      "0\n",
      "36\n",
      "3\n",
      "6\n",
      "39\n",
      "3\n",
      "41\n",
      "42\n",
      "0\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "3\n",
      "50\n",
      "0.25\n",
      "52\n",
      "53\n",
      "13.399999999999999\n",
      "0.5\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "0\n",
      "10.08\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "0\n",
      "0\n",
      "69\n",
      "0\n",
      "71\n",
      "0\n",
      "73\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "78\n",
      "0\n",
      "3\n",
      "81\n",
      "82\n",
      "83\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "88\n",
      "89\n",
      "5\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "13.75\n",
      "99\n",
      "2\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "2.85\n",
      "108\n",
      "1\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "for o_paper_index, paper in enumerate(data):\n",
    "    try:\n",
    "        r = get_altmetrics(data, o_paper_index)\n",
    "        print(r['altmetric_score']['score_history']['at'])\n",
    "    except:\n",
    "        print(o_paper_index)\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding: Altmetrics grow for more recent papers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for idx, paper in enumerate(data):\n",
    "    print(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'altmetric_id': 3801312,\n 'counts': {'readers': {'citeulike': '0', 'mendeley': '1969', 'connotea': '0'},\n  'total': {'posts_count': 60},\n  'twitter': {'unique_users_count': 1, 'posts_count': 11},\n  'patent': {'unique_users_count': 49,\n   'unique_users': ['patent:US-10019801-B2',\n    'patent:US-10328769-B2',\n    'patent:US-10331124-B2',\n    'patent:US-10354406-B2',\n    'patent:US-10380889-B2',\n    'patent:US-10469828-B2',\n    'patent:US-10507787-B2',\n    'patent:US-10520593-B2',\n    'patent:US-10599156-B2',\n    'patent:US-10705193-B2',\n    'patent:WO-2020142328-A1',\n    'patent:US-10824862-B2',\n    'patent:US-10859682-B2',\n    'patent:WO-2020247075-A1',\n    'patent:US-10867210-B2',\n    'patent:US-20210004566-A1',\n    'patent:US-10929996-B2',\n    'patent:US-10949989-B2',\n    'patent:US-10970871-B2',\n    'patent:US-10977501-B2',\n    'patent:US-11009868-B2',\n    'patent:US-11037051-B2',\n    'patent:FR-3104292-A1',\n    'patent:US-11061116-B2',\n    'patent:US-11094072-B2',\n    'patent:US-11107230-B2',\n    'patent:US-11138751-B2',\n    'patent:US-11145074-B2',\n    'patent:US-11164012-B2',\n    'patent:US-11176709-B2',\n    'patent:US-11210802-B2',\n    'patent:US-11250489-B2',\n    'patent:US-11294035-B2',\n    'patent:US-11295517-B2',\n    'patent:US-11321863-B2',\n    'patent:EP-4002215-A1',\n    'patent:US-11361187-B1',\n    'patent:WO-2022117199-A1',\n    'patent:US-11370423-B2',\n    'patent:US-11386567-B2',\n    'patent:US-11403776-B2',\n    'patent:US-11423255-B2',\n    'patent:US-11422265-B2',\n    'patent:US-11436743-B2',\n    'patent:US-11450120-B2',\n    'patent:US-11449050-B2',\n    'patent:WO-2022201212-A1',\n    'patent:US-11467574-B2',\n    'patent:US-11481912-B2'],\n   'posts_count': 49}},\n 'citation': {'altmetric_jid': '4f6fa4ed3cf058f610002b9d',\n  'authors': ['A Geiger', 'P Lenz', 'C Stiller', 'R Urtasun'],\n  'doi': '10.1177/0278364913491297',\n  'endpage': '1237',\n  'first_seen_on': '2015-03-18T02:00:36+00:00',\n  'issns': ['0278-3649', '1741-3176'],\n  'issue': '11',\n  'journal': 'The International Journal of Robotics Research',\n  'last_mentioned_on': 1666656000,\n  'links': ['http://ijr.sagepub.com/content/32/11/1231.short?rss=1&ssource=mfr',\n   'https://doi.org/10.1177/0278364913491297',\n   'https://journals.sagepub.com/doi/full/10.1177/0278364913491297?utm_source=SAGE_social&utm_medium=&utm_campaign=&hootPostID=6c7c7b7f830cf734882ae2e7ea804321'],\n  'pdf_url': 'http://ijr.sagepub.com/content/32/11/1231.full.pdf',\n  'pubdate': '2013-09-01T00:00:00+00:00',\n  'epubdate': '2013-08-23T00:00:00+00:00',\n  'published_on': '2013-08-23T00:00:00+00:00',\n  'publisher_subjects': [{'name': 'Artificial Intelligence And Image Processing',\n    'scheme': 'era'},\n   {'name': 'Electrical And Electronic Engineering', 'scheme': 'era'},\n   {'name': 'Mechanical Engineering', 'scheme': 'era'}],\n  'scopus_subjects': ['Mathematics',\n   'Physical Sciences',\n   'Computer Science',\n   'Engineering'],\n  'startpage': '1231',\n  'title': 'Vision meets robotics: The KITTI dataset',\n  'type': 'article',\n  'volume': '32',\n  'mendeley_url': 'https://www.mendeley.com/catalogue/20d7e081-1dff-3dcc-aafb-8452f935fda7/'},\n 'altmetric_score': {'score': 12.25,\n  'score_history': {'1y': 9,\n   '6m': 9,\n   '3m': 6,\n   '1m': 3,\n   '1w': 0,\n   '6d': 0,\n   '5d': 0,\n   '4d': 0,\n   '3d': 0,\n   '2d': 0,\n   '1d': 0,\n   'at': 12.25},\n  'context_for_score': {'all': {'total_number_of_other_articles': 22385407,\n    'mean': 10.131079131788221,\n    'rank': 2363256,\n    'this_scored_higher_than_pct': 89,\n    'this_scored_higher_than': 20013241,\n    'rank_type': 'exact',\n    'sample_size': 22385407,\n    'percentile': 89},\n   'similar_age_3m': {'total_number_of_other_articles': 178733,\n    'mean': 9.17483100490693,\n    'rank': 20193,\n    'this_scored_higher_than_pct': 88,\n    'this_scored_higher_than': 158428,\n    'rank_type': 'exact',\n    'sample_size': 178733,\n    'percentile': 88},\n   'this_journal': {'total_number_of_other_articles': 914,\n    'mean': 4.974297592997812,\n    'rank': 54,\n    'this_scored_higher_than_pct': 93,\n    'this_scored_higher_than': 859,\n    'rank_type': 'exact',\n    'sample_size': 914,\n    'percentile': 93},\n   'similar_age_this_journal_3m': {'total_number_of_other_articles': 15,\n    'mean': 5.068666666666667,\n    'rank': 2,\n    'this_scored_higher_than_pct': 86,\n    'this_scored_higher_than': 13,\n    'rank_type': 'exact',\n    'sample_size': 15,\n    'percentile': 86}}},\n 'demographics': {'poster_types': {'member_of_the_public': 1},\n  'users': {'twitter': {'cohorts': {'Members of the public': 1}},\n   'mendeley': {'by_status': {'Student  > Postgraduate': 48,\n     'Professor > Associate Professor': 20,\n     'Researcher': 176,\n     'Student  > Master': 489,\n     'Student  > Ph. D. Student': 465,\n     'Professor': 19,\n     'Student  > Bachelor': 158,\n     'Student  > Doctoral Student': 71,\n     'Lecturer': 18,\n     'Other': 40,\n     'Librarian': 3,\n     'Lecturer > Senior Lecturer': 5,\n     'Unspecified': 18},\n    'by_discipline': {'Materials Science': 2,\n     'Medicine and Dentistry': 3,\n     'Social Sciences': 2,\n     'Decision Sciences': 1,\n     'Physics and Astronomy': 11,\n     'Psychology': 8,\n     'Mathematics': 10,\n     'Environmental Science': 2,\n     'Unspecified': 19,\n     'Pharmacology, Toxicology and Pharmaceutical Science': 1,\n     'Chemical Engineering': 4,\n     'Arts and Humanities': 4,\n     'Design': 2,\n     'Engineering': 613,\n     'Chemistry': 5,\n     'Neuroscience': 5,\n     'Energy': 1,\n     'Earth and Planetary Sciences': 6,\n     'Economics, Econometrics and Finance': 3,\n     'Computer Science': 768,\n     'Agricultural and Biological Sciences': 8,\n     'Nursing and Health Professions': 1,\n     'Business, Management and Accounting': 3}}},\n  'geo': {'mendeley': {'HR': 1,\n    'US': 3,\n    'JP': 2,\n    'ES': 4,\n    'CN': 1,\n    'DK': 1,\n    'MX': 1,\n    'SG': 1,\n    'CA': 1,\n    'GB': 3,\n    'CZ': 2,\n    'FI': 2,\n    'IL': 1,\n    'SE': 1,\n    'BR': 1,\n    'AU': 2,\n    'AT': 1,\n    'KR': 2,\n    'FR': 6,\n    'NL': 1,\n    'DE': 2,\n    'PT': 1}}},\n 'posts': {'twitter': [{'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '578012977690898432'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '599922342576328705'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '600752815833677825'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '606636585245736960'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '615686216814477312'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '617040138292432896'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '618907442462613504'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '637802069261348864'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '639880678063017984'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '648195444321841153'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '666274602956386305'}],\n  'patent': [{'title': 'Image analysis system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-10019801-B2',\n    'license': 'public',\n    'citation_ids': [49782009,\n     35641125,\n     12902049,\n     12906237,\n     49782035,\n     3801312,\n     49782029,\n     49782026,\n     49782022],\n    'posted_on': '2018-07-10T00:00:00+00:00',\n    'summary': 'A method of determining a camera movement expressed in terms of an absolute transformation of a camera state from a reference state, including receiving a stream of video images captured from a camera; producing a stream of odometry measurements from the v',\n    'ucid': 'US-10019801-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Methods for interacting with autonomous or semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-10328769-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a system for precise delivery of an item to a consumer by an autonomous or semi-autonomous vehicle. The system performs image recognition algorithms to detect a primary entrance, navigate from a street address to the primary entrance, an',\n    'ucid': 'US-10328769-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Autonomous vehicle repositioning',\n    'url': 'https://app.dimensions.ai/details/patent/US-10331124-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a platform for distributing and navigating an autonomous or semi-autonomous fleet throughout a plurality of pathways. The platform may employ demand distribution prediction algorithms, and interim repositioning algorithms to distribute t',\n    'ucid': 'US-10331124-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Method of detecting objects within a 3D environment',\n    'url': 'https://app.dimensions.ai/details/patent/US-10354406-B2',\n    'license': 'public',\n    'citation_ids': [41865941, 43004858, 35312133, 3801312, 15431844],\n    'posted_on': '2019-07-16T00:00:00+00:00',\n    'summary': 'A method and system for detecting objects within a three-dimensional (3D) environment, comprising obtaining a 3D point-cloud representation of the environment, the point-cloud comprising a set of point locations, and converting the point-cloud to a 3D feat',\n    'ucid': 'US-10354406-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Determining car positions',\n    'url': 'https://app.dimensions.ai/details/patent/US-10380889-B2',\n    'license': 'public',\n    'citation_ids': [35433612, 3801312, 81094544, 80495985, 267002],\n    'posted_on': '2019-08-13T00:00:00+00:00',\n    'summary': 'Examples provided herein describe a method for determining car positions. For example, a physical processor of an edge computing device may receive position data for a legacy car and information about a make and model of the legacy car. The first edge devi',\n    'ucid': 'US-10380889-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional dense structure from motion with stereo vision',\n    'url': 'https://app.dimensions.ai/details/patent/US-10469828-B2',\n    'license': 'public',\n    'citation_ids': [81181891, 35182124, 3801312, 24853916, 12059625],\n    'posted_on': '2019-11-05T00:00:00+00:00',\n    'summary': 'Disclosed examples include three-dimensional imaging systems and methods to reconstruct a three-dimensional scene from first and second image data sets obtained from a single camera at first and second times, including computing feature point correspondenc',\n    'ucid': 'US-10469828-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'System and mechanism for upselling products on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-10507787-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-12-17T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles for containing, securing, and delivering at least one of a first item and at least one of a second item after a customer plac',\n    'ucid': 'US-10507787-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Installation and use of vehicle light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10520593-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2019-12-31T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10520593-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Advertising on autonomous or semi-autonomous vehicle exterior',\n    'url': 'https://app.dimensions.ai/details/patent/US-10599156-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-03-24T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of electric autonomous vehicle for apportioned display of a media, operating autonomously and a fleet management module for coordination of the autonomous vehicle flee',\n    'ucid': 'US-10599156-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Monitoring of vehicles using light ranging systems',\n    'url': 'https://app.dimensions.ai/details/patent/US-10705193-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-07-07T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10705193-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'IMAGE BOUNDING SHAPE USING 3D ENVIRONMENT REPRESENTATION',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020142328-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 99637399, 99637402],\n    'posted_on': '2020-07-09T00:00:00+00:00',\n    'summary': 'A computing system is provided, including one or more optical sensors, a display, one or more user input devices, and a processor. The processor may receive optical data of a physical environment. Based on the optical data, the processor may generate a thr',\n    'ucid': 'WO-2020142328-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional object detection for autonomous robotic systems using image proposals',\n    'url': 'https://app.dimensions.ai/details/patent/US-10824862-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-11-03T00:00:00+00:00',\n    'summary': 'Provided herein are methods and systems for implementing three-dimensional perception in an autonomous robotic system comprising an end-to-end neural network architecture that directly consumes large-scale raw sparse point cloud data and performs such task',\n    'ucid': 'US-10824862-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Telematics using a light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10859682-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-12-08T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10859682-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'NOVEL POSE SYNTHESIS',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020247075-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 100116890, 62459169, 81095701, 100116898],\n    'posted_on': '2020-12-10T00:00:00+00:00',\n    'summary': 'Examples are disclosed that relate to computing devices and methods for synthesizing a novel pose of an object. One example provides a method comprising receiving a reference image of an object corresponding to an original viewpoint. The reference image of',\n    'ucid': 'WO-2020247075-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-10867210-B2',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     30140746,\n     10897749,\n     39315381],\n    'posted_on': '2020-12-15T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-10867210-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD AND APPARATUS FOR 3D OBJECT BOUNDING FOR 2D IMAGE DATA',\n    'url': 'https://app.dimensions.ai/details/patent/US-20210004566-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 27031120, 100279283, 100279290, 35407856],\n    'posted_on': '2021-01-07T00:00:00+00:00',\n    'summary': 'Methods and apparatus are provided for 3D object bounding for 2D image data for use in an assisted driving equipped vehicle. In various embodiments, an apparatus includes a camera operative to capture a two dimensional image of a field of view, a lidar ope',\n    'ucid': 'US-20210004566-A1',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Image depth prediction neural networks',\n    'url': 'https://app.dimensions.ai/details/patent/US-10929996-B2',\n    'license': 'public',\n    'citation_ids': [835489, 3801312, 35416550, 35349078, 42711774, 99823301],\n    'posted_on': '2021-02-23T00:00:00+00:00',\n    'summary': 'A system includes an image depth prediction neural network implemented by one or more computers. The image depth prediction neural network is a recurrent neural network that is configured to receive a sequence of images and, for each image in the sequence:',\n    'ucid': 'US-10929996-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Concept for determining a confidence/uncertainty measure for disparity measurement',\n    'url': 'https://app.dimensions.ai/details/patent/US-10949989-B2',\n    'license': 'public',\n    'citation_ids': [914846,\n     81129890,\n     105570844,\n     33315001,\n     81129920,\n     3801312,\n     81129879],\n    'posted_on': '2021-03-16T00:00:00+00:00',\n    'summary': 'A more effective confidence/uncertainty measure determination for disparity measurements is achieved by performing the determination on an evaluation of a set of disparity candidates for a predetermined position of a first picture at which the measurement',\n    'ucid': 'US-10949989-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': \"Estimating two-dimensional object bounding box information based on bird's-eye view point cloud\",\n    'url': 'https://app.dimensions.ai/details/patent/US-10970871-B2',\n    'license': 'public',\n    'citation_ids': [43402617, 105411112, 3801312],\n    'posted_on': '2021-04-06T00:00:00+00:00',\n    'summary': 'Upon receiving a set of two-dimensional data points representing an object in an environment, a bounding box estimator estimates a bounding box vector representative of a two-dimensional version of the object that is represented by the two-dimensional data',\n    'ucid': 'US-10970871-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object classification using extra-regional context',\n    'url': 'https://app.dimensions.ai/details/patent/US-10977501-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 99631377, 10897749],\n    'posted_on': '2021-04-13T00:00:00+00:00',\n    'summary': 'Some aspects of the subject matter disclosed herein include a system implemented on one or more data processing apparatuses. The system can include an interface configured to obtain, from one or more sensor subsystems, sensor data describing an environment',\n    'ucid': 'US-10977501-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Fleet of autonomous vehicles with lane positioning and platooning behaviors',\n    'url': 'https://app.dimensions.ai/details/patent/US-11009868-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2021-05-18T00:00:00+00:00',\n    'summary': 'Disclosed herein are systems for navigating an autonomous or semi-autonomous fleet comprising a plurality of autonomous or semi-autonomous vehicles within a plurality of navigable pathways within an unstructured open environment.',\n    'ucid': 'US-11009868-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': '3D plane detection and reconstruction using a monocular image',\n    'url': 'https://app.dimensions.ai/details/patent/US-11037051-B2',\n    'license': 'public',\n    'citation_ids': [108127066,\n     108127070,\n     1821746,\n     10027341,\n     2465542,\n     3737169,\n     108127080,\n     32069491,\n     3801312,\n     108127089],\n    'posted_on': '2021-06-15T00:00:00+00:00',\n    'summary': 'Planar regions in three-dimensional scenes offer important geometric cues in a variety of three-dimensional perception tasks such as scene understanding, scene reconstruction, and robot navigation. Image analysis to detect planar regions can be performed b',\n    'ucid': 'US-11037051-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Method of configuring an imaging device of a motor vehicle comprising an optical image capture device',\n    'url': 'https://app.dimensions.ai/details/patent/FR-3104292-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 112451882, 112451886],\n    'posted_on': '2021-06-11T00:00:00+00:00',\n    'summary': 'The method comprises the training of a neural network RNe1 associated with the optical device C1 comprising, for each index i: A) supply of N images IMn, i respectively to N neural networks RNen, including the target network RNe1 and N- 1 ancillary network',\n    'ucid': 'FR-3104292-A1',\n    'jurisdiction': 'FR',\n    'filing_status': 'Application'},\n   {'title': 'Lidar system with image size compensation mechanism',\n    'url': 'https://app.dimensions.ai/details/patent/US-11061116-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2021-07-13T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems including an apparatus configured to translate one or more spherical lenses, an array of light sources, an array of photodetectors, or any combination thereof of a collection optical system in the Z direction (optical axis) to m',\n    'ucid': 'US-11061116-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'System and method for providing single image depth estimation based on deep neural network',\n    'url': 'https://app.dimensions.ai/details/patent/US-11094072-B2',\n    'license': 'public',\n    'citation_ids': [30174235, 32069491, 112817529, 35312688, 3801312],\n    'posted_on': '2021-08-17T00:00:00+00:00',\n    'summary': 'A method and system for determining depth information of an image are herein provided. According to one embodiment, the method includes receiving an image input, classifying the input image into a depth range of a plurality of depth ranges, and determining',\n    'ucid': 'US-11094072-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11107230-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 35252783, 12863070, 3801312],\n    'posted_on': '2021-08-31T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates from a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, flipping, by a disparity model, the monocular image to',\n    'ucid': 'US-11107230-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised training using reprojected distance loss',\n    'url': 'https://app.dimensions.ai/details/patent/US-11138751-B2',\n    'license': 'public',\n    'citation_ids': [115351779, 3801312],\n    'posted_on': '2021-10-05T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to training a depth model for monocular depth estimation. In one embodiment, a method includes generating, as part of training the depth model according to a supervised training stage, a depth',\n    'ucid': 'US-11138751-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11145074-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 28320951],\n    'posted_on': '2021-10-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, processing the monocular image',\n    'ucid': 'US-11145074-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Advanced driver assistance system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-11164012-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 35251246],\n    'posted_on': '2021-11-02T00:00:00+00:00',\n    'summary': 'A driver assistance system detects lane markings in a perspective image of a road in front of a vehicle. The driver assistance system extracts a plurality of features, in particular lane markings, from the perspective image for generating a set of feature',\n    'ucid': 'US-11164012-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for self-supervised scale-aware training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11176709-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3569738, 3801312, 28320951],\n    'posted_on': '2021-11-16T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes processing a first image of a pair according to the depth model to generate a d',\n    'ucid': 'US-11176709-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for conditioning training data to avoid learned aberrations',\n    'url': 'https://app.dimensions.ai/details/patent/US-11210802-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2021-12-28T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training for monocular depth estimation. In one embodiment, a method includes filtering disfavored images from first training data to produce second training data that is a s',\n    'ucid': 'US-11210802-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Flexible compartment design on autonomous and semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-11250489-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-02-15T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles coordinated by a fleet management module. Each vehicle may be configured to receive a modular unit, wherein the modular unit',\n    'ucid': 'US-11250489-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'LiDAR system with cylindrical lenses',\n    'url': 'https://app.dimensions.ai/details/patent/US-11294035-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems comprising a transmission optical system and a collection optical system utilizing a common lens or pieces derived from the same lens to reduce or eliminate image mismatch between the transmission optical system and the collecti',\n    'ucid': 'US-11294035-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Generating realistic point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11295517-B2',\n    'license': 'public',\n    'citation_ids': [49749311, 3801312],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating realistic full-scene point clouds. One of the methods includes obtaining an initial scene point cloud characterizing an initial scene in an envir',\n    'ucid': 'US-11295517-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using semantic features',\n    'url': 'https://app.dimensions.ai/details/patent/US-11321863-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2022-05-03T00:00:00+00:00',\n    'summary': 'Systems, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes identifying semantic features in the monocular image according to a semantic',\n    'ucid': 'US-11321863-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD TO IMPROVE SCALE CONSISTENCY AND/OR SCALE AWARENESS IN A MODEL OF SELF-SUPERVISED DEPTH AND EGO-MOTION PREDICTION NEURAL NETWORKS',\n    'url': 'https://app.dimensions.ai/details/patent/EP-4002215-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     65334273,\n     118139115,\n     3296035,\n     129819701,\n     129819720,\n     129819726,\n     81135856],\n    'posted_on': '2022-05-25T00:00:00+00:00',\n    'summary': 'A method to improve scale consistency and/or scale awareness in a model of self-supervised depth and ego-motion prediction neural networks processing a video stream of monocular images, wherein complementary GPS coordinates synchronized with the images are',\n    'ucid': 'EP-4002215-A1',\n    'jurisdiction': 'EP',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-11361187-B1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     10897749,\n     39315381,\n     30140746],\n    'posted_on': '2022-06-14T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-11361187-B1',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'A DEVICE AND A METHOD FOR REPRESENTING THE YAW ORIENTATION OF A VEHICLE VISIBLE ON AN IMAGE',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022117199-A1',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-09T00:00:00+00:00',\n    'summary': 'A device and a method for representing the yaw orientation of a vehicle visible on an image, comprising: obtaining the position in the image of a rectangular bounding box (200) surrounding the vehicle (201), obtaining the position of a vertical split line',\n    'ucid': 'WO-2022117199-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Multi-task machine-learned models for object intention determination in autonomous driving',\n    'url': 'https://app.dimensions.ai/details/patent/US-11370423-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-28T00:00:00+00:00',\n    'summary': 'Generally, the disclosed systems and methods utilize multi-task machine-learned models for object intention determination in autonomous driving applications. For example, a computing system can receive sensor data obtained relative to an autonomous vehicle',\n    'ucid': 'US-11370423-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for weakly supervised training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11386567-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312],\n    'posted_on': '2022-07-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes training the depth model according to a first stage that is self-supervised and',\n    'ucid': 'US-11386567-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Depth extraction',\n    'url': 'https://app.dimensions.ai/details/patent/US-11403776-B2',\n    'license': 'public',\n    'citation_ids': [47515225, 5064731, 9698667, 134919368, 3801312],\n    'posted_on': '2022-08-02T00:00:00+00:00',\n    'summary': 'A computer-implemented method of training a depth uncertainty estimator comprises receiving, at a training computer system, a set of training examples, each training example comprising (i) a stereo image pair and (ii) an estimated disparity map computed fr',\n    'ucid': 'US-11403776-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Image processing',\n    'url': 'https://app.dimensions.ai/details/patent/US-11423255-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'The present disclosure pertains generally to image feature extraction. Both transfer-learning and multi-task training approaches are considered. In one example, a machine learning model is trained to perform a geographic classification task of distinguishi',\n    'ucid': 'US-11423255-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Driver visualization and semantic monitoring of a vehicle using LiDAR data',\n    'url': 'https://app.dimensions.ai/details/patent/US-11422265-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'Methods are provided for using a light ranging system of a vehicle. A computing system receives, from light ranging devices, ranging data including distance vectors to environmental surfaces. A distance vector can correspond to a pixel of a three-dimension',\n    'ucid': 'US-11422265-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised depth estimation according to an arbitrary camera',\n    'url': 'https://app.dimensions.ai/details/patent/US-11436743-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 20839578, 35453469, 3801312],\n    'posted_on': '2022-09-06T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model using a neural camera model that is independent of a camera type. In one embodiment, a method includes acquiring training data including at least a',\n    'ucid': 'US-11436743-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object detection in point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11450120-B2',\n    'license': 'public',\n    'citation_ids': [35264175,\n     49293181,\n     35518902,\n     50229939,\n     4063957,\n     49773333,\n     49801958,\n     99595494,\n     3801312,\n     15431844,\n     3124500,\n     6758458,\n     755649,\n     6588544],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing point cloud data representing a sensor measurement of a scene captured by one or more sensors to generate an object detection output that identif',\n    'ucid': 'US-11450120-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Real-time violations and safety monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11449050-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a real-time human behavior analysis of an unmanned vehicle by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment. The platforms determine a real-time pa',\n    'ucid': 'US-11449050-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD FOR DETERMINING THE DEPTH FROM A SINGLE IMAGE AND SYSTEM THEREOF',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022201212-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     75440920,\n     35252783,\n     12059419,\n     81101466,\n     58400615,\n     3737169,\n     133366195,\n     30027655,\n     65334273,\n     137709556,\n     34274690,\n     5971430,\n     132845677,\n     2687501,\n     129259979,\n     30174235,\n     35590071,\n     118139115,\n     109788541,\n     137709597,\n     4850006,\n     137709617,\n     137709629,\n     129818727,\n     81210194,\n     100661533,\n     137709649,\n     137709662,\n     26382884,\n     81135856,\n     32068659],\n    'posted_on': '2022-09-29T00:00:00+00:00',\n    'summary': 'Method for determining the depth from a single image and system thereof The present invention relates to a computer-implemented method (2) for determining the depth of a digital image (I), wherein said method (2) comprises the step of training (20) a neura',\n    'ucid': 'WO-2022201212-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Infrastructure monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11467574-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-10-11T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a non-navigational quality of at least one infrastructure by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment.',\n    'ucid': 'US-11467574-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Device for generating a depth map',\n    'url': 'https://app.dimensions.ai/details/patent/US-11481912-B2',\n    'license': 'public',\n    'citation_ids': [112453217, 137975534, 3737169, 80667646, 3801312],\n    'posted_on': '2022-10-25T00:00:00+00:00',\n    'summary': 'A device includes an encoder configured to generate a plurality of feature data by encoding an image; a bottleneck circuit configured to generate enhanced feature data from first bottleneck data among the plurality of feature data; and a decoder configured',\n    'ucid': 'US-11481912-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'}]},\n 'score': 12.25,\n 'images': {'small': 'https://badges.altmetric.com/?size=64&score=13&types=aaaaaaat',\n  'medium': 'https://badges.altmetric.com/?size=100&score=13&types=aaaaaaat',\n  'large': 'https://badges.altmetric.com/?size=180&score=13&types=aaaaaaat'}}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def altmetric_doi_requests(doi):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/doi/{doi}?key={alt_metric_key}').json()\n",
    "    return r\n",
    "altmetric_doi_requests('10.1177%2F0278364913491297')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "def altmetric_arxiv_requests(arxiv):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/arxiv_id/{arxiv}?key={alt_metric_key}').json()\n",
    "    return r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  req4Seasons.json\n",
      "Added  reqA2D2.json\n",
      "Added  reqA3D.json\n",
      "Added  reqA9.json\n",
      "Added  reqAmodalCityscapes.json\n",
      "Added  reqAMUSE.json\n",
      "Added  reqApolloScape.json\n",
      "Added  reqArgoverse3DTracking.json\n",
      "Added  reqArgoverseMotionForecasting.json\n",
      "Added  reqAugKITTI.json\n",
      "Added  reqBayAreaDataset.json\n",
      "Added  reqBDD100k.json\n",
      "Added  reqBDD100KAPS.json\n",
      "Added  reqBeyondPASCAL.json\n",
      "Added  reqBLVD.json\n",
      "Added  reqBoreas.json\n",
      "Added  reqBoschSmallTrafficLightsDataset.json\n",
      "Added  reqBoschTL.json\n",
      "Added  reqBoxy.json\n",
      "Added  reqBrain4Cars.json\n",
      "Added  reqCADC.json\n",
      "Added  reqCADP.json\n",
      "Added  reqCaltechLanes.json\n",
      "Added  reqCaltechPedestrian.json\n",
      "Added  reqCamVid.json\n",
      "Added  reqCARLA100.json\n",
      "Added  reqCarlaScenes.json\n",
      "Added  reqCARRADADataset.json\n",
      "Added  reqCarsDataset.json\n",
      "Added  reqCityscapes3D.json\n",
      "Added  reqCityscapesDVPS.json\n",
      "Added  reqCODA.json\n",
      "Added  reqComma2k19.json\n",
      "Added  reqcommaai.json\n",
      "Added  reqComplexUrbanDataset.json\n",
      "Added  reqCooperativeDrivingDatasetCODD.json\n",
      "Added  reqCrashD.json\n",
      "Added  reqCULaneDataset.json\n",
      "Added  reqC_O_D_A.json\n",
      "Added  reqD2City.json\n",
      "Added  reqDBNet.json\n",
      "Added  reqDDAD.json\n",
      "Added  reqDDD20.json\n",
      "Added  reqDDD20DAVISDrivingDataset2020.json\n",
      "Added  reqDGLMOTS.json\n",
      "Added  reqDIPLECSAutonomousDrivingDatasets.json\n",
      "Added  reqDReyeVE.json\n",
      "Added  reqDRIV100.json\n",
      "Added  reqDriveUTrafficLightDataset.json\n",
      "Added  reqexiD.json\n",
      "Added  reqFishyscapes.json\n",
      "Added  reqFiveRoundaboutsDataset.json\n",
      "Added  reqFordAutonomousVehicleDataset.json\n",
      "Added  reqGated2Depth.json\n",
      "Added  reqGated2Gated.json\n",
      "Added  reqGROUNDED.json\n",
      "Added  reqGroundTruthStixel.json\n",
      "Added  reqGTA5.json\n",
      "Added  reqGTSDB.json\n",
      "Added  reqGTSRB.json\n",
      "Added  reqH3D.json\n",
      "Added  reqHCIChallengingStereo.json\n",
      "Added  reqHD1K.json\n",
      "Added  reqhighD.json\n",
      "Added  reqIDDA.json\n",
      "Added  reqIlluminationChangesinaday.json\n",
      "Added  reqinD.json\n",
      "Added  reqIndiaDrivingDataset.json\n",
      "Added  reqINTERACTIONdataset.json\n",
      "Added  reqJAAD.json\n",
      "Added  reqKAISTMultiSpectralDayNight.json\n",
      "Added  reqKAISTUrban.json\n",
      "Added  reqKITTI.json\n",
      "Added  reqKITTI360.json\n",
      "Added  reqKITTI360APS.json\n",
      "Added  reqKITTI360PanopticBEV.json\n",
      "Added  reqKRadar.json\n",
      "Added  reqLIBRE.json\n",
      "Added  reqLiDARVideoDrivingDataset.json\n",
      "Added  reqLISATrafficLightDataset.json\n",
      "Added  reqLISATrafficSignDataset.json\n",
      "Added  reqLostAndFound.json\n",
      "Added  reqLyftLevel5Perception.json\n",
      "Added  reqLyftLevel5Prediction.json\n",
      "Added  reqMalagaStereoandLaserUrban.json\n",
      "Added  reqMapillaryVistas.json\n",
      "Added  reqMAVDMultimodalAudioVisualDetection.json\n",
      "Added  reqMCityDataCollection.json\n",
      "Added  reqMITAVTClusteredDrivingSceneDataset.json\n",
      "Added  reqMOTSynth.json\n",
      "Added  reqMTSD.json\n",
      "Added  reqMultiVehicleStereoEventCamera.json\n",
      "Added  reqNightOwls.json\n",
      "Added  reqnuImages.json\n",
      "Added  reqnuPlan.json\n",
      "Added  reqnuScenes.json\n",
      "Added  reqONCE.json\n",
      "Added  reqOneThousandandOneHours.json\n",
      "Added  reqOnSalienceSensitiveSignClassificationinAutonomousVehiclePathPlanning.json\n",
      "Added  reqopenDD.json\n",
      "Added  reqOpenMPD.json\n",
      "Added  reqOxfordRadarRobotCar.json\n",
      "Added  reqOxfordRobotCar.json\n",
      "Added  reqPepScenes.json\n",
      "Added  reqPointCloudDeNoising.json\n",
      "Added  reqPREVENTION.json\n",
      "Added  reqR3DrivingDataset.json\n",
      "Added  reqRadarScenes.json\n",
      "Added  reqRADIATE.json\n",
      "Added  reqRANUS.json\n",
      "Added  reqRELLIS3DDataset.json\n",
      "Added  reqROAD.json\n",
      "Added  reqRoadAnomaly21.json\n",
      "Added  reqRoadDamage.json\n",
      "Added  reqRope3D.json\n",
      "Added  reqrounD.json\n",
      "Added  reqRUGDRobotUnstructuredGroundDriving.json\n",
      "Added  reqScribbleKITTI.json\n",
      "Added  reqSeasonalVariationDataset.json\n",
      "Added  reqSeeingThroughFog.json\n",
      "Added  reqSemanticKITTI.json\n",
      "Added  reqSemKITTIDVPS.json\n",
      "Added  reqSHIFT.json\n",
      "Added  reqSmallObstacle.json\n",
      "Added  reqSODA10M.json\n",
      "Added  reqStanfordDroneDataset.json\n",
      "Added  reqStanfordTrackCollection.json\n",
      "Added  reqStreetHazards.json\n",
      "Added  reqStreetLearn.json\n",
      "Added  reqSynscapes.json\n",
      "Added  reqSyntheticDiscrepancyDatasets.json\n",
      "Added  reqSynthia.json\n",
      "Added  reqS_O_D_A_1_0_M.json\n",
      "Added  reqTalk2Car.json\n",
      "Added  reqTheAutonomousPlatformInertialDataset.json\n",
      "Added  reqTheEuroCityPersonsDataset.json\n",
      "Added  reqTheUSydCampusDataset.json\n",
      "Added  reqTMEMotorway.json\n",
      "Added  reqToronto3D.json\n",
      "Added  reqTRoM.json\n",
      "Added  reqTsinghuaDaimlerCyclistDetection.json\n",
      "Added  reqTT100K.json\n",
      "Added  reqTUDBrussels.json\n",
      "Added  reqUAHDriveSet.json\n",
      "Added  reqUnsupervisedLlamas.json\n",
      "Added  reqUTBMEULTD.json\n",
      "Added  reqVPGNet.json\n",
      "Added  reqWaymoBlockNeRF.json\n",
      "Added  reqWaymoOpenMotion.json\n",
      "Added  reqWaymoOpenPerception.json\n",
      "Added  reqWildDash.json\n",
      "Added  reqWoodScape.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "wd = os.getcwd()\n",
    "wd = wd.replace('.ipynb_checkpoints', '')\n",
    "directory = os.path.join(wd, 'data', 'requests')\n",
    "os.listdir(directory)\n",
    "retrieved_data =  []\n",
    "for file in os.listdir(directory):\n",
    "    with open(os.path.join(directory, file), \"r\") as j:\n",
    "        a = json.loads(j.read())\n",
    "        retrieved_data.append(a)\n",
    "        print('Added ', file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_alt_values(r):\n",
    "    \"\"\"extracts relevant information from the response object\"\"\"\n",
    "    total_readers = 0\n",
    "    for key in r['counts']['readers'].keys():\n",
    "        total_readers += int(r['counts']['readers'][key])\n",
    "    score = r['score']\n",
    "    percentile = r['altmetric_score']['context_for_score']['all']['percentile']\n",
    "    similar_age_3m_percentile= r['altmetric_score']['context_for_score']['similar_age_3m']['percentile']\n",
    "    alt_values = {'score': score}, {'percentile': percentile}, {'similar_age_3m_percentile': similar_age_3m_percentile}, {'total_readers': total_readers}\n",
    "    return alt_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpaper[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m caused: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mex\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     alt_values \u001B[38;5;241m=\u001B[39m get_alt_values(\u001B[43mr\u001B[49m)\n\u001B[0;32m     16\u001B[0m     retrieved_data[idx_paper]\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maltmetrics\u001B[39m\u001B[38;5;124m'\u001B[39m: alt_values})\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfound:\u001B[39m\u001B[38;5;124m'\u001B[39m, c_doi, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m, c)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "c = 0\n",
    "c_doi = 0\n",
    "for idx_paper, paper in enumerate(retrieved_data):\n",
    "    c += 1\n",
    "    try:\n",
    "        doi = paper['DOI']\n",
    "        c_doi += 1\n",
    "    except:\n",
    "        try:\n",
    "            r = altmetric_arxiv_requests(paper['arxivId'])\n",
    "            c_doi += 1\n",
    "        except Exception as ex:\n",
    "            warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
    "        continue\n",
    "    alt_values = get_alt_values(r)\n",
    "    retrieved_data[idx_paper].update({'altmetrics': alt_values})\n",
    "\n",
    "print('found:', c_doi, '/', c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R3 Driving Dataset is not on altmetric.com"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': 'KITTI',\n  'href': 'http://www.cvlibs.net/datasets/kitti/',\n  'size_hours': '6',\n  'size_storage': '180',\n  'frames': '-',\n  'numberOfScenes': '50',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360 10Hz, 1 inertial and GPS navigation system',\n  'benchmark': 'stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking',\n  'annotations': '3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0',\n  'relatedDatasets': 'Semantic KITTI, KITTI-360',\n  'publishDate': '2012-03-01',\n  'lastUpdate': '2021-02-01',\n  'paperTitle': 'Vision meets Robotics: The KITTI Dataset',\n  'relatedPaper': 'http://www.cvlibs.net/publications/Geiger2013IJRR.pdf',\n  'location': 'Karlsruhe, Germany',\n  'rawData': 'Yes',\n  'DOI': '10.1177/0278364913491297',\n  'citationCount': 4737,\n  'completionStatus': 'complete',\n  'paperId': '79b949d9b35c3f51dd20fb5c746cc81fc87147eb',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Cars Dataset',\n  'href': 'https://ai.stanford.edu/~jkrause/cars/car_dataset.html',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '16185',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'camera',\n  'recordingPerspective': '-',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'car make, model, year',\n  'licensing': 'freely available for non-commercial research and educational use',\n  'relatedDatasets': 'BMW-10',\n  'publishDate': '2013',\n  'lastUpdate': '-',\n  'paperTitle': '3D Object Representations for Fine-Grained Categorization',\n  'relatedPaper': 'https://ai.stanford.edu/~jkrause/papers/3drr13.pdf',\n  'location': 'Sourced from internet',\n  'rawData': '-',\n  'DOI': '10.1109/ICCVW.2013.77',\n  'citationCount': 1625,\n  'completionStatus': 'complete',\n  'paperId': 'a83cec6a91701bd8500f8c43ad731d4353c71d55',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Synthia',\n  'href': 'https://synthia-dataset.net/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '213400',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x rgb camera 960x720 100',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Synthetic',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0',\n  'relatedDatasets': '-',\n  'publishDate': '2016-06-27',\n  'lastUpdate': '2019-10-27',\n  'paperTitle': 'The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes',\n  'relatedPaper': 'https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf',\n  'location': 'Unity development platform',\n  'rawData': '-',\n  'DOI': '10.1109/CVPR.2016.352',\n  'citationCount': 1376,\n  'completionStatus': 'complete',\n  'paperId': '9358d2ae944cfbdcb4b48e2e0c5f7ad97118b74e',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'nuScenes',\n  'href': 'https://www.nuscenes.org/',\n  'size_hours': '15',\n  'size_storage': '-',\n  'frames': '1400000',\n  'numberOfScenes': '1000',\n  'samplingRate': '-',\n  'lengthOfScenes': '20',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '1x lidar 32 channels 360 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz',\n  'benchmark': '3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking',\n  'annotations': 'semantic category, attributes, 3d bounding boxes ',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'nuImages',\n  'publishDate': '2019-03-01',\n  'lastUpdate': '2020-12-01',\n  'paperTitle': 'nuScenes: A multimodal dataset for autonomous driving',\n  'relatedPaper': 'https://arxiv.org/pdf/1903.11027.pdf',\n  'location': 'Boston, USA and Singapore',\n  'rawData': 'Yes',\n  'DOI': '10.1109/cvpr42600.2020.01164',\n  'citationCount': 1357,\n  'completionStatus': 'complete',\n  'paperId': '9e475a514f54665478aac6038c262e5a6bac5e64',\n  'arxivId': '1903.11027',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'nuImages',\n  'href': 'https://www.nuscenes.org/nuimages',\n  'size_hours': '150',\n  'size_storage': '-',\n  'frames': '1200000',\n  'numberOfScenes': '93000',\n  'samplingRate': '2',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '1x lidar 32 channels 360 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz',\n  'benchmark': '-',\n  'annotations': 'instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'nuScenes',\n  'publishDate': '2020-07-01',\n  'lastUpdate': '-',\n  'location': 'Boston, USA and Singapore',\n  'rawData': 'Yes',\n  'DOI': '10.1109/cvpr42600.2020.01164',\n  'citationCount': 1357,\n  'completionStatus': 'partially Complete',\n  'paperId': '9e475a514f54665478aac6038c262e5a6bac5e64',\n  'arxivId': '1903.11027',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'GTA5',\n  'href': 'https://download.visinf.tu-darmstadt.de/data/from_games/',\n  'size_storage': '57.05',\n  'size_hours': '-',\n  'frames': '24966',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'camera 1914x1052',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Synthetic',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation of different classes',\n  'licensing': 'Freely available for research and educational purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2016-04-08',\n  'lastUpdate': '-',\n  'paperTitle': 'Playing for Data: Ground Truth from Computer Games',\n  'relatedPaper': 'https://arxiv.org/pdf/1608.02192.pdf',\n  'location': 'Game: Grand Theft Auto 5',\n  'rawData': '-',\n  'DOI': '10.1007/978-3-319-46475-6_7',\n  'citationCount': 1148,\n  'completionStatus': 'complete',\n  'paperId': '4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd',\n  'arxivId': '1608.02192',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'CamVid',\n  'href': 'https://www.kaggle.com/carlolepelaars/camvid',\n  'size_storage': '-',\n  'size_hours': '0.16',\n  'frames': '701',\n  'numberOfScenes': '1',\n  'samplingRate': '30',\n  'lengthOfScenes': '600',\n  'sensors': 'camera',\n  'sensorDetail': '1x CCD Panasonic HVX200 digital camera 960x720 30Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation',\n  'licensing': 'Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2009-01-15',\n  'lastUpdate': '-',\n  'paperTitle': 'Semantic object classes in video: A high-definition ground truth database',\n  'relatedPaper': 'https://pdf.sciencedirectassets.com/271524/1-s2.0-S0167865508X00169/1-s2.0-S0167865508001220/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGEaCXVzLWVhc3QtMSJHMEUCIQCA11sHV8h2EpCAzXyQ0V4VP%2F%2FtSTtdmwWBVRbF4T8AMwIgXu%2BDzZ9%2FZ5Matmw%2BNuJxkagSqNnDCZObjFIe3pMY2sgq2wQIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDF3yb0q0O6GdJqshbiqvBGiWqPb8EEGrHPUFfAVoMjtnfz4EEc6DVL5rwsMRYK7K5%2FddZ3n3M82ULw%2FjzUKCkbxBijZtK8o2tl%2BLKeIZ5y3Feep%2FIjIIhQ%2FjiZeni0N07G1%2FCQq6SOZivayRtp7CUjdgTXDq4zduoNEYNE1CZ4hhTLWK2RFjlPNmdOGUxCZbT2MNy27kbcwnjQw6u%2BF5Ro6L1q%2FI9bul4%2Foc0W40c0LnEQXMe5z8hTBKEFMwT5cEUf9%2FpnS%2BskWugGgsSNFiACeEJ0NETcVhuF6FLbDICqLE%2F7qw8wG0gzbDm2SljpYiw0gkW5gxczQE3Ru3sZ%2BQuUmh1zdCk4kt%2FazqaKZJqlvCnLgiQWuF%2BRNK4Hpteh%2B4i517UrWEAU8fpXuAyMVZG8JVG6xR%2FpCHWU7H1VNEA%2FbrL5zLkYp5SzX5xFzumsVeIJ37u69HuBgFFvKOn%2BmPtasGf5TudFpcA%2FE7d7C7lcES0AvfYbhyh%2FFqHCfG4s1MwfU6qJ61nZcYSMZgEFIiTlZGR80vgkYxCvIP0sNlgdOCQHArjxYHJ6yzjkPBFQ90c2snBDkfYfWcKrvWY%2FdGp4x8TBdaqrY07YEASRAOdd%2BJJUonHQDSRUJKmXibsNBM40z1qGflHPVL0wWAM3YITapAaLPxSagAqTPJ5AoOSXElsogZYoTHHXJU8KNIblOaz6XnKx32TLnw4FRkEeVBTwjOEFr5EA4srS1qbXlJsjV0aObvKBsr9Ja1UItx3CowtcCKlgY6qQFO6pFryXLKH4V7tldhJn%2BcQ35KUZOua1QBWSBkg38vxrjhQfEyvKCns7X21TxnzARlljVwzj9rL3HtZ7co4%2BT6pxwkYvj%2FKsTotHp5qtV5dYAI4w0dLNCsPz33OdLYqbBoIfr1OQ5USj9MvuO2k2e2voHKW1SmusBW5goBvuPNuWe4dHdFZAYtvtwGAs6iCSxbAYCPv6I%2BdPySt7vGydS05xTW%2FcPUZNGE&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220704T090151Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWIZ3RZPY%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f4dcb0c82201f26036b5850b0f38e19def847297bc9cc18b614ee094af344449&hash=8eef19412becfacf9bb21603db158b613ef51da58582217142ebc507be134724&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167865508001220&tid=spdf-ec513c9a-f60a-4935-a0ec-6f936e44745a&sid=a0422e5a30875748cf6a74770c17f8e2edf3gxrqb&type=client&ua=4d50575004060303040a&rr=7256adbaee0c694c',\n  'location': 'United Kingdom',\n  'rawData': 'Yes',\n  'DOI': '10.1016/j.patrec.2008.04.005',\n  'citationCount': 918,\n  'completionStatus': 'complete',\n  'paperId': 'e03bbca03dc10c4dfb10eca7439aa1a19233aa5a',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Oxford Robot Car',\n  'href': 'https://robotcar-dataset.robots.ox.ac.uk/',\n  'size_hours': '210',\n  'size_storage': '23150',\n  'frames': '-',\n  'numberOfScenes': '100',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, ins/gps',\n  'sensorDetail': '1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270 50Hz, 1x lidar SICK LD-MRS 90 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International',\n  'relatedDatasets': 'Oxford Radar Robot Car',\n  'publishDate': '2016-11-01',\n  'lastUpdate': '2020-02-01',\n  'paperTitle': '1 Year, 1000km: The Oxford RobotCar Dataset',\n  'relatedPaper': 'https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf',\n  'location': 'Oxford, UK',\n  'rawData': 'Yes',\n  'DOI': '10.1177/0278364916679498',\n  'citationCount': 847,\n  'completionStatus': 'complete',\n  'paperId': 'd74ed94c7a83298a4b4f0e1c70200126e464cc05',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'GTSRB',\n  'href': 'https://benchmark.ini.rub.de/gtsrb_news.html',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '50000',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x Prosilica GC 1380CH 1360x1024 25Hz',\n  'recordingPerspective': 'ego-perspetive',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'Traffic Sign Recognition',\n  'annotations': '2d bounding box',\n  'licensing': 'Free to use',\n  'relatedDatasets': 'GTSDB',\n  'publishDate': '2010-12-01',\n  'lastUpdate': '2011-01-19',\n  'paperTitle': 'The German Traffic Sign Recognition Benchmark: A multi-class classification competition',\n  'relatedPaper': 'https://www.ini.rub.de/upload/file/1470692848_f03494010c16c36bab9e/StallkampEtAl_GTSRB_IJCNN2011.pdf',\n  'location': 'Germany',\n  'rawData': '-',\n  'DOI': '10.1109/IJCNN.2011.6033395',\n  'citationCount': 696,\n  'completionStatus': 'complete',\n  'paperId': '22fe619996b59c09cb73be40103a123d2e328111',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Caltech Lanes',\n  'href': 'http://www.mohamedaly.info/datasets/caltech-lanes',\n  'size_storage': '0.55',\n  'size_hours': '-',\n  'frames': '1225',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d splines for lane markings',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2008-06-04',\n  'lastUpdate': '-',\n  'paperTitle': 'Real time Detection of Lane Markers in Urban Streets',\n  'relatedPaper': 'https://ieeexplore.ieee.org/abstract/document/4621152?casa_token=V3xXln8DOeUAAAAA:h4ALqmxoJhgv3D2Szr9llQIc0UwNNkGNXzHR2486xb478J9dm_Pthf2ay4Zdl3-uPtl-BMi762BM',\n  'location': 'Pasadena, California, USA',\n  'rawData': '-',\n  'DOI': '10.1109/IVS.2008.4621152',\n  'citationCount': 685,\n  'completionStatus': 'complete',\n  'paperId': 'e6c40fd5adf616b31766f7578fd8c18e18158b78',\n  'arxivId': '1411.7113',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Mapillary Vistas',\n  'href': 'https://www.mapillary.com/dataset/vistas',\n  'size_storage': '',\n  'size_hours': '-',\n  'frames': '25000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': '-',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'semantic image segmentation and instance-specific image segmentation',\n  'annotations': 'instance specific object annotations',\n  'licensing': ' Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)',\n  'relatedDatasets': 'Mapillary Traffic Sign',\n  'publishDate': '2017-12-25',\n  'lastUpdate': '2021-01-18',\n  'paperTitle': 'The Mapillary Vistas Dataset for Semantic Understanding of Street Scene',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8237796',\n  'location': 'Europe, North and South America, Asia, Africa and Oceania',\n  'rawData': '-',\n  'DOI': '10.1109/ICCV.2017.534',\n  'citationCount': 651,\n  'completionStatus': 'complete',\n  'paperId': '79828e6e9f137a583082b8b5a9dfce0c301989b8',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Caltech Pedestrian',\n  'href': 'http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/',\n  'size_storage': '-',\n  'size_hours': '10',\n  'frames': '1000000',\n  'numberOfScenes': '137',\n  'samplingRate': '30',\n  'lengthOfScenes': '60',\n  'sensors': 'camera',\n  'sensorDetail': '1x camera 640x480 30Hz',\n  'benchmark': 'pedestrian detection',\n  'annotations': 'bounding boxes',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2010-03-01',\n  'lastUpdate': '2019-01-01',\n  'paperTitle': 'Pedestrian Detection: A Benchmark',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206631',\n  'location': 'Loa Angeles, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/CVPR.2009.5206631',\n  'citationCount': 643,\n  'completionStatus': 'complete',\n  'paperId': '3e083dc8aeb7983a5cdff146985363d38caf0886',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Waymo Open Perception',\n  'href': 'https://waymo.com/open/data/perception/',\n  'size_hours': '10.83',\n  'size_storage': '-',\n  'frames': '390000',\n  'numberOfScenes': '1950',\n  'samplingRate': '10',\n  'lengthOfScenes': '20',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars',\n  'benchmark': '2d detection, 3d detection, 2d tracking, 3d tracking',\n  'annotations': '3d bounding boxes (lidar), 2d bounding boxes (camera)',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': 'Waymo Open Motion',\n  'publishDate': '2019-08-01',\n  'lastUpdate': '2020-03-01',\n  'paperTitle': 'Scalability in Perception for Autonomous Driving: Waymo Open Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/1912.04838.pdf',\n  'location': 'San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/CVPR42600.2020.00252',\n  'citationCount': 628,\n  'completionStatus': 'complete',\n  'paperId': '2f7e8d0cfe601b9bb3d07d7783ecd80424994517',\n  'arxivId': '1912.04838',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Beyond PASCAL',\n  'href': 'https://yuxng.github.io/Xiang_WACV_03242014.pdf',\n  'size_storage': '8.5',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'recordingPerspective': '-',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'sensorDetail': '-',\n  'benchmark': 'anomaly detection, 3D object detection and pose estimation',\n  'annotations': 'label landmarks of the CAD model on the 2D image',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2014-03-26',\n  'lastUpdate': '-',\n  'paperTitle': 'Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild',\n  'relatedPaper': 'https://cvgl.stanford.edu/papers/xiang_wacv14.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/WACV.2014.6836101',\n  'citationCount': 622,\n  'completionStatus': 'complete',\n  'paperId': '60c7a5d919806e18e31f139b7df9f1172b776f17',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'GTSDB',\n  'href': 'https://benchmark.ini.rub.de/gtsdb_news.html',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '900',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x Prosilica GC 1380CH camera 1360x1024',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'Traffic Sign Detection',\n  'annotations': '2d bounding box',\n  'licensing': '-',\n  'relatedDatasets': 'GTSRB',\n  'publishDate': '2012-12-01',\n  'lastUpdate': '2013-08-09',\n  'paperTitle': 'Detection of traffic signs in real-world images: The German traffic sign detection benchmark',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/6706807',\n  'location': 'Bochum, Germany',\n  'rawData': '-',\n  'DOI': '10.1109/IJCNN.2013.6706807',\n  'citationCount': 525,\n  'completionStatus': 'complete',\n  'paperId': 'ba640d55b77407f3170e9c1bd5f2cfbcbfd67df5',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'LISA Traffic Sign Dataset',\n  'href': 'http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '6610',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes with description',\n  'licensing': 'Academic license',\n  'relatedDatasets': 'LISA Traffic Light Dataset',\n  'publishDate': '2012-10-19',\n  'lastUpdate': '-',\n  'paperTitle': 'Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335478&casa_token=iD5J89e1Y0MAAAAA:OKU-qJJPvSmZk1wJGlPb4G3Z7SF12nHZdr7mEV03pwgl2Q8ASZlH7T2zbo5n65e4yBniT_S5jQfn&tag=1',\n  'location': 'USA',\n  'rawData': '-',\n  'DOI': '10.1109/TITS.2012.2209421',\n  'citationCount': 515,\n  'completionStatus': 'complete',\n  'paperId': '7a26c15a97142b05796738f34901b98a37e65b36',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'BDD100k',\n  'href': 'https://www.bdd100k.com/',\n  'size_storage': '1800',\n  'size_hours': '1111',\n  'frames': '120000000',\n  'numberOfScenes': '100000',\n  'samplingRate': '30',\n  'lengthOfScenes': '40',\n  'sensors': 'camera, gps/imu',\n  'sensorDetail': 'crowd-sourced therefore no fixed setup, camera (720p) and gps/imu',\n  'benchmark': 'object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption',\n  'annotations': 'bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area',\n  'licensing': 'BSD 3-Clause',\n  'relatedDatasets': '-',\n  'publishDate': '2020-04-01',\n  'lastUpdate': '-',\n  'paperTitle': 'BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning',\n  'relatedPaper': 'https://arxiv.org/pdf/1805.04687.pdf',\n  'location': 'New York, Berkeley, San Francisco and Bay Area, USA',\n  'rawData': 'Yes',\n  'citationCount': 500,\n  'completionStatus': 'complete',\n  'paperId': '4f0b8f730273e9f11b2bfad2415485414b96299f',\n  'arxivId': '1805.04687',\n  'DOI': '10.1109/cvpr42600.2020.00271',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Argoverse Motion Forecasting',\n  'href': 'https://www.argoverse.org/',\n  'size_storage': '4.81',\n  'size_hours': '320',\n  'frames': '16227850',\n  'numberOfScenes': '324557',\n  'samplingRate': '10',\n  'lengthOfScenes': '5',\n  'sensors': 'camera, lidar, gps',\n  'sensorDetail': '2x lidar 32 beam 40 10Hz, 7x ring cameras 1920x1200 combined 360 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz',\n  'benchmark': 'forecasting',\n  'annotations': 'semantic vector map, rasterized map, trajectories',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'Argoverse 3D Tracking',\n  'publishDate': '2019-06-01',\n  'lastUpdate': '-',\n  'paperTitle': 'Argoverse: 3D Tracking and Forecasting with Rich Maps',\n  'relatedPaper': 'https://arxiv.org/pdf/1911.02620.pdf',\n  'location': 'Miami and Pittsburgh, USA',\n  'rawData': 'No',\n  'DOI': '10.1109/CVPR.2019.00895',\n  'citationCount': 476,\n  'completionStatus': 'complete',\n  'paperId': '8e66c7e494476eb0dee846349df1bd705ceac6c3',\n  'arxivId': '1911.02620',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Argoverse 3D Tracking',\n  'href': 'https://www.argoverse.org/',\n  'size_storage': '254.4',\n  'size_hours': '1',\n  'frames': '44000',\n  'numberOfScenes': '113',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps',\n  'sensorDetail': '2x lidar 40 10Hz, 7x ring cameras 1920x1200 combined 360 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz',\n  'benchmark': 'tracking',\n  'annotations': 'semantic vector map, rasterized map, 3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'Argoverse Motion Forecasting',\n  'publishDate': '2019-06-01',\n  'lastUpdate': '-',\n  'paperTitle': 'Argoverse: 3D Tracking and Forecasting with Rich Maps',\n  'relatedPaper': 'https://arxiv.org/pdf/1911.02620.pdf',\n  'location': 'Miami and Pittsburgh, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/CVPR.2019.00895',\n  'citationCount': 476,\n  'completionStatus': 'complete',\n  'paperId': '8e66c7e494476eb0dee846349df1bd705ceac6c3',\n  'arxivId': '1911.02620',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Stanford Drone Dataset',\n  'href': 'https://cvgl.stanford.edu/projects/uav_data/',\n  'size_storage': '69',\n  'size_hours': '-',\n  'frames': '929499',\n  'numberOfScenes': '8',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x Quadcopter camera 1400x1904',\n  'recordingPerspective': 'top-view',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes for objects',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License',\n  'relatedDatasets': '-',\n  'publishDate': '2016-08-01',\n  'lastUpdate': '-',\n  'paperTitle': 'Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes',\n  'relatedPaper': 'https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33',\n  'location': 'Stanford campus, USA',\n  'rawData': '-',\n  'DOI': '10.1007/978-3-319-46484-8_33',\n  'citationCount': 464,\n  'completionStatus': 'complete',\n  'paperId': '6b2af377fd4876a3feeea9c9a9eec88e911af382',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'TT100K',\n  'href': 'http://cg.cs.tsinghua.edu.cn/traffic-sign/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '100000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'traffic-sign detection and classification',\n  'annotations': '2d bounding boxes and class labels',\n  'licensing': 'Creative Commons Attribution-NonCommercial (CC-BY-NC',\n  'relatedDatasets': '-',\n  'publishDate': '2016-06-27',\n  'lastUpdate': '-',\n  'paperTitle': 'Traffic-Sign Detection and Classification in the Wild',\n  'relatedPaper': 'http://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf',\n  'location': 'China',\n  'rawData': '-',\n  'DOI': '10.1109/CVPR.2016.232',\n  'citationCount': 377,\n  'completionStatus': 'complete',\n  'paperId': 'cdd8bad29b5e90a1f92080eaca51ba123f34ada5',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'CULane Dataset',\n  'href': 'https://xingangpan.github.io/projects/CULane.html',\n  'size_storage': '42.5',\n  'size_hours': '-',\n  'frames': '133235',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'traffic lanes with cubic splines',\n  'licensing': 'freely available for non-commercial purpose',\n  'relatedDatasets': '-',\n  'publishDate': '2018-04-27',\n  'lastUpdate': '-',\n  'paperTitle': 'Spatial as Deep: Spatial CNN for Traffic Scene Understanding',\n  'relatedPaper': 'https://arxiv.org/pdf/1712.06080',\n  'location': 'Beijing, China',\n  'rawData': '-',\n  'DOI': '10.1609/aaai.v32i1.12301',\n  'citationCount': 329,\n  'completionStatus': 'complete',\n  'paperId': '5383ee00a5c0c3372081fccfeceb812b7854c9ea',\n  'arxivId': '1712.06080',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'highD',\n  'href': 'https://www.highd-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '16.5',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x quadcopter DJI Phantom 4 Pro Plus camera 4096x2160 25Hz',\n  'recordingPerspective': 'top-view',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes with vehicle speed and id',\n  'licensing': 'Freely available for non-commercial use only upon registration',\n  'relatedDatasets': 'inD, rounD, exiD and uniD datasets',\n  'publishDate': '2018-11-04',\n  'lastUpdate': '-',\n  'paperTitle': 'The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems',\n  'relatedPaper': 'https://arxiv.org/ftp/arxiv/papers/1810/1810.05642.pdf',\n  'location': 'Cologne, Germany',\n  'rawData': '-',\n  'DOI': '10.1109/ITSC.2018.8569552',\n  'citationCount': 307,\n  'completionStatus': 'complete',\n  'paperId': '8c2c4e3db2de3305b1d25368695c258fea6a6076',\n  'arxivId': '1810.05642',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Road Damage',\n  'href': 'https://github.com/sekilab/RoadDamageDetector/',\n  'size_storage': '2.4',\n  'size_hours': '-',\n  'frames': '13135',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding box with class labels',\n  'licensing': 'Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)',\n  'relatedDatasets': 'Road Damage Dataset 2018',\n  'publishDate': '2018-06-30',\n  'lastUpdate': '2020-06-02',\n  'paperTitle': 'Road Damage Detection and Classification Using Deep Neural Networks with Smartphone Images',\n  'relatedPaper': 'https://arxiv.org/pdf/1801.09454.pdf',\n  'location': 'Japan',\n  'rawData': '-',\n  'DOI': '10.1111/mice.12387',\n  'citationCount': 240,\n  'completionStatus': 'complete',\n  'paperId': 'a45770aa3d77a53ffb81fef6cdec35f19f1f614d',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'VPGNet',\n  'href': 'https://arxiv.org/abs/1710.06288',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '21097',\n  'numberOfScenes': '-',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'lane and road markings, vanishing point',\n  'licensing': 'Available for non commercial research purposes on registration',\n  'relatedDatasets': '-',\n  'publishDate': '2017-10-22',\n  'lastUpdate': '-',\n  'paperTitle': 'VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_ICCV_2017/papers/Lee_VPGNet_Vanishing_Point_ICCV_2017_paper.pdf',\n  'location': 'Seoul, South Korea',\n  'rawData': '-',\n  'DOI': '10.1109/ICCV.2017.215',\n  'citationCount': 237,\n  'completionStatus': 'complete',\n  'paperId': '22d046e926d3ed12d00063b8668cb11fed26e1f1',\n  'arxivId': '1710.06288',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'ApolloScape',\n  'href': 'http://apolloscape.auto/',\n  'size_hours': '100',\n  'size_storage': '-',\n  'frames': '143906',\n  'numberOfScenes': '-',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, imu/gnss',\n  'sensorDetail': '2x VUX-1HA laser scanners 360, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 2710',\n  'benchmark': '2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo',\n  'annotations': 'high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark label semantic instance segmentation, geo-tagged',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2018-03-01',\n  'lastUpdate': '2020-09-01',\n  'paperTitle': 'The ApolloScape Open Dataset for Autonomous Driving and its Application',\n  'relatedPaper': 'https://arxiv.org/pdf/1803.06184.pdf',\n  'location': 'Beijing, Shanghai and Shenzhen, China',\n  'rawData': 'Yes',\n  'citationCount': 233,\n  'completionStatus': 'complete',\n  'paperId': '6c0270f1ca5afa0b50f47952e9215c6f362aea40',\n  'arxivId': '1803.06184',\n  'DOI': '10.1109/TPAMI.2019.2926463',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Malaga Stereo and Laser Urban',\n  'href': 'https://www.mrpt.org/MalagaUrbanDataset',\n  'size_storage': '4.76',\n  'size_hours': '1.55',\n  'frames': '-',\n  'numberOfScenes': '15',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '1x Point Grey Research Bumblebee 2 stereo camera 1024x768 20Hz, 3x Hokuyo UTM-30LX laser scanners 270, 2x SICK LMS-200 laser scanners, 1x xSens MTi imu 100Hz, 2x mmGPS devices from Topcon gps',\n  'recordingPerspective': \"ego-perspective and bird's eye\",\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2013-10-09',\n  'lastUpdate': '2018-09-13',\n  'paperTitle': 'The Mlaga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario',\n  'relatedPaper': 'https://journals.sagepub.com/doi/10.1177/0278364913507326',\n  'location': 'Mlaga, Spain',\n  'rawData': '-',\n  'DOI': '10.1177/0278364913507326',\n  'citationCount': 200,\n  'completionStatus': 'complete',\n  'paperId': 'a64fc28e37b01a89a3c352827db2838d85f3134d',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'DDAD',\n  'href': 'https://github.com/AdrienGaidon-TRI/DDAD',\n  'size_storage': '254',\n  'size_hours': '-',\n  'frames': '21200',\n  'numberOfScenes': '435',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '6x cameras 2.4MP 1936x1216 10Hz, 1x Luminar-H2 Lidar sensor 360 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License',\n  'relatedDatasets': '-',\n  'publishDate': '2020-06-19',\n  'lastUpdate': '-',\n  'paperTitle': '3D Packing for Self-Supervised Monocular Depth Estimation',\n  'relatedPaper': 'https://arxiv.org/pdf/1905.02693.pdf',\n  'location': 'USA (Ann Arbor, San Francisco Bay Area, Detroit, Cambridge and Massachusetts), Japan (Tokyo and Odaiba)',\n  'rawData': '-',\n  'DOI': '10.1109/cvpr42600.2020.00256',\n  'citationCount': 195,\n  'completionStatus': 'complete',\n  'paperId': '44a462b3240a1987756cf6071427b29653446af1',\n  'arxivId': '1905.02693',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'TUD Brussels',\n  'href': 'https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/people-detection-pose-estimation-and-tracking/multi-cue-onboard-pedestrian-detection/',\n  'size_storage': '3.0',\n  'size_hours': '-',\n  'frames': '1016',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'hand held camera 720x576',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes for pedestrians',\n  'licensing': '-',\n  'relatedDatasets': 'TUD-MotionPairs',\n  'publishDate': '2009-06-20',\n  'lastUpdate': '2010-04-13',\n  'paperTitle': 'Multi-cue onboard pedestrian detection',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/5206638',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/CVPR.2009.5206638',\n  'citationCount': 188,\n  'completionStatus': 'complete',\n  'paperId': '6c8bddd67871a45bc2f0cd008648de1104b25df1',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Brain4Cars',\n  'href': 'http://brain4cars.com/',\n  'size_storage': '16',\n  'size_hours': '-',\n  'frames': '2000000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, speed logger, gps',\n  'sensorDetail': '-',\n  'recordingPerspective': 'Face camera, ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'driving events (lane changes, turns, driving straight)',\n  'licensing': 'free for educational and non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2015-12-13',\n  'lastUpdate': '-',\n  'paperTitle': 'Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models',\n  'relatedPaper': 'http://brain4cars.com/pdfs/iccv2015.pdf',\n  'location': 'USA',\n  'rawData': '-',\n  'DOI': '10.1109/ICCV.2015.364',\n  'citationCount': 180,\n  'completionStatus': 'complete',\n  'paperId': '186336fb15a47ebdc6f0730d0cf4f56c58c5b906',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'comma.ai',\n  'href': 'http://research.comma.ai/',\n  'size_storage': '80',\n  'size_hours': '7.25',\n  'frames': '-',\n  'numberOfScenes': '10',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License',\n  'relatedDatasets': '-',\n  'publishDate': '2016-08-03',\n  'lastUpdate': '-',\n  'paperTitle': 'Learning a Driving Simulator',\n  'relatedPaper': 'https://arxiv.org/pdf/1608.01230',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.1608.01230',\n  'citationCount': 179,\n  'completionStatus': 'complete',\n  'paperId': 'd16e66f0e4a3bcbfac5ea4c19e60ed53f6608f45',\n  'arxivId': '1608.01230',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'CARLA100',\n  'href': 'https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md',\n  'relatedPaper': 'https://arxiv.org/pdf/1904.08980.pdf',\n  'citationCount': 177,\n  'completionStatus': 'partially Complete',\n  'paperId': 'fcfc344628c7604232804317eb90db268dfc85e8',\n  'arxivId': '1904.08980',\n  'DOI': '10.1109/ICCV.2019.00942',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Multi Vehicle Stereo Event Camera',\n  'href': 'https://daniilidis-group.github.io/mvsec/',\n  'size_storage': '187',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '11',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, VI-Sensor, gps',\n  'sensorDetail': '2x DAVIS m346B camera 346x60 83 50Hz, 1x Velodyne Puck LITE 360 20Hz, 1x Skybotix integrated VI-sensor stereo camera: 2 x Aptina MT9V034, 1x UBLOX NEO-M8N gps',\n  'recordingPerspective': \"ego-perspective, bird's eye\",\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'ground truth pose',\n  'licensing': 'Creative Commons Attribution-ShareAlike 4.0 International License',\n  'relatedDatasets': '-',\n  'publishDate': '2017-12-10',\n  'lastUpdate': '2018-09-26',\n  'paperTitle': 'The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8288670',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/LRA.2018.2800793',\n  'citationCount': 167,\n  'completionStatus': 'complete',\n  'paperId': 'f1e92f09209c7f50e05599c7551520ca129a6de4',\n  'arxivId': '1801.10202',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'Stanford Track Collection',\n  'href': 'https://cs.stanford.edu/people/teichman/stc/',\n  'size_storage': '2',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar, gps/imu',\n  'sensorDetail': '1x Velodyne HDL-64E S2 lidar 360 10Hz, 1x Applanix LV-420 gps 200Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available for non-commercial research',\n  'relatedDatasets': '-',\n  'publishDate': '2011-05-09',\n  'lastUpdate': '2012-06-01',\n  'paperTitle': 'Towards 3D object recognition via classification of arbitrary object tracks',\n  'relatedPaper': 'https://cs.stanford.edu/people/teichman/papers/icra2011.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA.2011.5979636',\n  'citationCount': 159,\n  'completionStatus': 'complete',\n  'paperId': 'ebd4d0ca94b86de68ff37a15f7cf1b6a577e7cd5',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'LISA Traffic Light Dataset',\n  'href': 'http://cvrr.ucsd.edu/LISA/datasets.html',\n  'size_storage': '5',\n  'size_hours': '0.75',\n  'frames': '43007',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': \"1x Point Grey's Bumblebee XB3 (BBX3-13S2C-60) stereo camera 1280x960 \",\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes with the description of state of traffic light',\n  'licensing': 'Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'LISA Traffic Sign Dataset',\n  'publishDate': '2016-02-03',\n  'lastUpdate': '-',\n  'paperTitle': 'Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/7398055',\n  'location': 'San Diego, California, USA',\n  'rawData': '-',\n  'DOI': '10.1109/TITS.2015.2509509',\n  'citationCount': 148,\n  'completionStatus': 'complete',\n  'paperId': '31553c941660ad6170ae83bb003e6a47fce300e9',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'INTERACTION dataset',\n  'href': 'https://interaction-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '16.5',\n  'frames': '594588',\n  'numberOfScenes': '-',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz',\n  'benchmark': 'motion prediction',\n  'annotations': '2d bounding boxes, semantic map, motion/trajectories',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2019-09-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1910.03088.pdf',\n  'location': 'USA, China, Germany and Bulgaria',\n  'rawData': 'Yes',\n  'citationCount': 141,\n  'completionStatus': 'partially Complete',\n  'paperId': '0e12be89cfe05315a18aaa6d43ef1d336b7e476e',\n  'arxivId': '1910.03088',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Bosch Small Traffic Lights Dataset',\n  'href': 'https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '13427',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720',\n  'benchmark': '-',\n  'annotations': 'bounding boxes, state',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2017-05-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163',\n  'location': '-',\n  'rawData': 'Yes',\n  'DOI': '10.1109/ICRA.2017.7989163',\n  'citationCount': 139,\n  'completionStatus': 'partially Complete',\n  'paperId': 'e316d7b26f7e459a2350793c5e1b9224a4fd485f',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Bosch TL',\n  'href': 'https://github.com/asimonov/Bosch-TL-Dataset',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '13427',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'camera 1280x720',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes',\n  'licensing': 'freely available for non-commercial use',\n  'relatedDatasets': '-',\n  'publishDate': '2017-07-24',\n  'lastUpdate': '-',\n  'paperTitle': 'A deep learning approach to traffic lights: Detection, tracking, and classification',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/7989163',\n  'location': 'El Camino Real in the San Francisco Bay Area, California',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA.2017.7989163',\n  'citationCount': 139,\n  'completionStatus': 'complete',\n  'paperId': 'e316d7b26f7e459a2350793c5e1b9224a4fd485f',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Oxford Radar Robot Car',\n  'href': 'https://oxford-robotics-institute.github.io/radar-robotcar-dataset/',\n  'size_storage': '4700',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '32',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 36032 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 12809603 16 Hz 663 x Point Grey Grasshopper2 10241024 11.1 Hz 180, 2 x SICK LMS-151 2D LIDAR 270 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,',\n  'benchmark': '-',\n  'annotations': 'ground truth data',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International',\n  'relatedDatasets': 'Oxford Robot Car',\n  'publishDate': '2020-02-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1909.01300.pdf',\n  'location': 'Oxford',\n  'rawData': 'Yes',\n  'citationCount': 137,\n  'completionStatus': 'partially Complete',\n  'paperId': '8ceee47c407e1e9a9bc736ca18e0f93a13457e38',\n  'arxivId': '1909.01300',\n  'DOI': '10.1109/ICRA40945.2020.9196884',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'India Driving Dataset',\n  'href': 'https://idd.insaan.iiit.ac.in/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '10004',\n  'numberOfScenes': '182',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1080p & 720p stereo image',\n  'benchmark': 'Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task',\n  'annotations': 'semantic segmentation',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2018-11-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://idd.insaan.iiit.ac.in/media/publications/idd-650.pdf',\n  'location': 'Bangalore and Hyderabad, India',\n  'rawData': 'Yes',\n  'DOI': '10.1109/WACV.2019.00190',\n  'citationCount': 137,\n  'completionStatus': 'partially Complete',\n  'paperId': '9aa3a0f681322f008e151d031d6009965e11b653',\n  'arxivId': '1811.10200',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'TME Motorway',\n  'href': 'http://cmp.felk.cvut.cz/data/motorway/',\n  'size_storage': '-',\n  'size_hours': '0.45',\n  'frames': '30000',\n  'numberOfScenes': '28',\n  'samplingRate': '20',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '1x stereo camera 1024x768 grayscale 32 20Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'bounding boxes around vehicles',\n  'licensing': 'Freely available for commercial and non-commercial research',\n  'relatedDatasets': '-',\n  'publishDate': '2012-09-16',\n  'lastUpdate': '-',\n  'paperTitle': 'A System for Real-time Detection and Tracking of Vehicles from a Single Car-mounted Camera',\n  'relatedPaper': 'http://cmp.felk.cvut.cz/data/motorway/paper/itsc2012.pdf',\n  'location': 'Northern Italy',\n  'rawData': '-',\n  'DOI': '10.1109/ITSC.2012.6338748',\n  'citationCount': 129,\n  'completionStatus': 'complete',\n  'paperId': '6b85cc6fb81d4aa0615a2bde68ba0009e82f6be8',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'KITTI-360',\n  'href': 'http://www.cvlibs.net/datasets/kitti-360/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '400000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '2x 180 fisheye camera, 1x 90 perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration',\n  'benchmark': '-',\n  'annotations': 'semantic instance segmentation',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0',\n  'relatedDatasets': 'KITTI',\n  'publishDate': '2015-11-01',\n  'lastUpdate': '2021-04-01',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf',\n  'location': 'Karlsruhe, Germany',\n  'rawData': 'Yes',\n  'DOI': '10.1109/CVPR.2016.401',\n  'citationCount': 128,\n  'completionStatus': 'partially Complete',\n  'paperId': '4e27e155a81b5f1b5638fe6223c68de133417d7f',\n  'arxivId': '1511.03240',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'DR(eye)VE',\n  'href': 'https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8',\n  'size_storage': '-',\n  'size_hours': '6.16',\n  'frames': '555000',\n  'numberOfScenes': '74',\n  'samplingRate': '-',\n  'lengthOfScenes': '300',\n  'sensors': 'camera, eye tracker',\n  'sensorDetail': '1x roof mounted GARMIN VirbX camera 1080p 25Hz, 1x SMI ETG 2w Eye Tracking Glasses (ETG) 60Hz, 1x frontal camera 720p 30Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'Gaze Maps, GPS, Speed and Course',\n  'licensing': 'Freely available for non-commercial use upon registration',\n  'relatedDatasets': '-',\n  'publishDate': '2018-06-08',\n  'lastUpdate': '-',\n  'paperTitle': \"Predicting the Driver's Focus of Attention: The DR(eye)VE Project\",\n  'relatedPaper': 'https://arxiv.org/pdf/1705.03854.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/TPAMI.2018.2845370',\n  'citationCount': 121,\n  'completionStatus': 'complete',\n  'paperId': '914e98db74f29fc608106ff438edde58965037c5',\n  'arxivId': '1705.03854',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'JAAD',\n  'href': 'https://paperswithcode.com/dataset/jaad',\n  'size_storage': '3.1',\n  'size_hours': '-',\n  'frames': '82032',\n  'numberOfScenes': '346',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x camera 1920x1080 110, 1x camera 1920x1080 170, 1x camera 1280x720 100',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes, behavioral tags, contextual tags',\n  'licensing': 'MIT License',\n  'relatedDatasets': 'PIE Dataset',\n  'publishDate': '2016-09-15',\n  'lastUpdate': '-',\n  'paperTitle': 'Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf',\n  'location': 'North America and Europe',\n  'rawData': '-',\n  'DOI': '10.1109/ICCVW.2017.33',\n  'citationCount': 114,\n  'completionStatus': 'complete',\n  'paperId': '04964e2697778dc843671c7764f0f912e46991ca',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'KAIST Multi-Spectral Day/Night',\n  'href': 'http://multispectral.kaist.ac.kr',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu, thermal camera',\n  'sensorDetail': '2x PointGrey Flea3 RGB camera 1280  960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz',\n  'benchmark': 'object detection, vision sensor enhancement, depth estimation, multi-spectral colorization',\n  'annotations': 'dense depth map, bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0',\n  'relatedDatasets': '-',\n  'publishDate': '2017-12',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689',\n  'location': '-',\n  'rawData': 'Yes',\n  'DOI': '10.1109/TITS.2018.2791533',\n  'citationCount': 112,\n  'completionStatus': 'partially Complete',\n  'paperId': '19bc52323383732c3c7d73e11726f6232515d2f9',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'WoodScape',\n  'href': 'https://paperswithcode.com/dataset/woodscape',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '10000',\n  'numberOfScenes': '-',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu, odometer',\n  'sensorDetail': '4x 1MPx RGB fisheye cameras 190, 1x Velodyne HDL-64E lidar 20Hz, 1x NovAtel Propak6 & SPAN-IGM-A1 gnss/imu, 1x Garmin 18x GNSS Positioning with SPS, Odometry signals from the vehicle bus',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation, monocular depth estimation, 2D & 3D bounding boxes,  visual odometry, visual SLAM, motion segmentation, soiling detection and end-to-end driving (driving controls)',\n  'licensing': 'Freely available for non-commercial research',\n  'relatedDatasets': '-',\n  'publishDate': '2019-10-27',\n  'lastUpdate': '2021-11-16',\n  'paperTitle': 'WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving',\n  'relatedPaper': 'https://arxiv.org/pdf/1905.01489.pdf',\n  'location': 'USA, Europe, and China',\n  'rawData': '-',\n  'DOI': '10.1109/ICCV.2019.00940',\n  'citationCount': 103,\n  'completionStatus': 'complete',\n  'paperId': '57de1a6cc6035d4ac6305121367ad3c6d3fd5104',\n  'arxivId': '1905.01489',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'H3D',\n  'href': 'https://usa.honda-ri.com/H3D',\n  'size_storage': '-',\n  'size_hours': '0.77',\n  'frames': '27721',\n  'numberOfScenes': '160',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '3x color PointGrey Grasshopper3 video cameras 1920x1200 90/80 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '3d bounding boxes',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2019-03-01',\n  'lastUpdate': '-',\n  'paperTitle': 'The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes',\n  'relatedPaper': 'https://arxiv.org/pdf/1903.01568.pdf',\n  'location': 'San Francisco Bay Area, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/ICRA.2019.8793925',\n  'citationCount': 101,\n  'completionStatus': 'complete',\n  'paperId': '76b1fed0cf23aa218bbbde1936f02dc3e07f8eca',\n  'arxivId': '1903.01568',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'KAIST Urban',\n  'href': 'https://irap.kaist.ac.kr/dataset/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '18',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '2x Velodyne VLP-16 16 channel lidar 360 10Hz, 2x SICK LMS-511 1 channel lidar 190 100Hz, 1x stereo camera 1280x560 10Hz',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0',\n  'relatedDatasets': '-',\n  'publishDate': '2017-09-01',\n  'lastUpdate': '2019-06-01',\n  'relatedPaper': 'https://irap.kaist.ac.kr/dataset/papers/IJRR2019_dataset.pdf',\n  'location': 'Seoul, Pangyo, Daejeon, Suwon and Dongtan, Korea',\n  'rawData': 'Yes',\n  'DOI': '10.1177/0278364919843996',\n  'citationCount': 95,\n  'completionStatus': 'partially Complete',\n  'paperId': '08682f0517cdd61b233c1614c48b6865805015a7',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Ground Truth Stixel',\n  'href': 'http://www.6d-vision.com/ground-truth-stixel-dataset',\n  'size_storage': '3.2',\n  'size_hours': '-',\n  'frames': '78500',\n  'numberOfScenes': '318',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, radar',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'stixel measurements',\n  'licensing': 'freely available for of scholarly and technical work',\n  'relatedDatasets': '-',\n  'publishDate': '2013-06-23',\n  'lastUpdate': '-',\n  'paperTitle': 'Exploiting the Power of Stereo Confidences',\n  'relatedPaper': 'http://wwwlehre.dhbw-stuttgart.de/~sgehrig/stixelGroundTruthDataset/stixel.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/CVPR.2013.45',\n  'citationCount': 95,\n  'completionStatus': 'complete',\n  'paperId': 'e6052969b1220a56f73a304a158e7cddfe5612c8',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Complex Urban Dataset',\n  'href': 'https://sites.google.com/view/complex-urban-dataset',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '41',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu, altimeter',\n  'sensorDetail': '2x FLIR FL3-U3-20E4C-C Global shutter color camera 1280x560 10Hz, 2x Velodyne VLP-16 16 CH LiDAR 360 10Hz, 2x SICK LMS-511 1 CH LiDAR 190 100Hz, 1x U-Blox EVK-7P Consumer-level GPS 10Hz, 1x SOKKIA GRX 2 VRS-RTK GPS 1Hz, 1x Xsens MTi-300 Consumer-level AHRS imu 200Hz, 1x  myPressure Altimeter sensor 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 License',\n  'relatedDatasets': '',\n  'publishDate': '2017-09-13',\n  'lastUpdate': '2019-06-17',\n  'paperTitle': 'Complex urban dataset with multi-level sensors from highly diverse urban environments',\n  'relatedPaper': 'https://journals.sagepub.com/doi/pdf/10.1177/0278364919843996',\n  'location': 'South Korea',\n  'rawData': 'Yes',\n  'DOI': '10.1177/0278364919843996',\n  'citationCount': 95,\n  'completionStatus': 'complete',\n  'paperId': '08682f0517cdd61b233c1614c48b6865805015a7',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'inD',\n  'href': 'https://www.ind-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '10',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x quadcopter DJI Phantom 4 Pro camera 4096x2160 25Hz',\n  'recordingPerspective': 'top-view',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d bounding box for objects, trajectory information',\n  'licensing': 'Freely available for non-commercial use only upon registration',\n  'relatedDatasets': 'highD, rounD, exiD and uniD datasets',\n  'publishDate': '2020-10-19',\n  'lastUpdate': '-',\n  'paperTitle': 'The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections',\n  'relatedPaper': 'https://arxiv.org/pdf/1911.07602.pdf',\n  'location': 'Aachen, Germany',\n  'rawData': '-',\n  'DOI': '10.1109/IV47402.2020.9304839',\n  'citationCount': 89,\n  'completionStatus': 'complete',\n  'paperId': '15ef0cb38239a1f74bea8ff9521778cad99420f3',\n  'arxivId': '1911.07602',\n  'altmetrics': ({'score': 2.25},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 51},\n   {'total_readers': 168})},\n {'id': 'Lyft Level5 Prediction',\n  'href': 'https://level-5.global/data/prediction/',\n  'size_hours': '1118',\n  'size_storage': '-',\n  'frames': '42500000',\n  'numberOfScenes': '170000',\n  'samplingRate': '10',\n  'lengthOfScenes': '25',\n  'sensors': 'camera, lidar, radar',\n  'sensorDetail': '7 cameras with 360 view, 3 lidars with 40-64 channels at 10Hz, 5 radars',\n  'benchmark': '-',\n  'annotations': 'semantic map \"annotations\", trajectories',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)',\n  'relatedDatasets': 'Lyft Level5 Perception',\n  'publishDate': '2020-06-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2006.14480v1.pdf',\n  'location': 'Palo Alto, USA',\n  'rawData': 'No',\n  'citationCount': 87,\n  'completionStatus': 'partially Complete',\n  'paperId': '5ff9429efe7b6c4e7c039064920600e2b7664828',\n  'arxivId': '2006.14480',\n  'altmetrics': ({'score': 20.23},\n   {'percentile': 93},\n   {'similar_age_3m_percentile': 88},\n   {'total_readers': 182})},\n {'id': 'Lyft Level5 Perception',\n  'href': 'https://level-5.global/data/perception/',\n  'size_hours': '2.5',\n  'size_storage': '-',\n  'frames': '-',\n  'numberOfScenes': '366',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '-',\n  'benchmark': '-',\n  'annotations': '3d bounding boxes, rasterised road geometry',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)',\n  'relatedDatasets': 'Lyft Level5 Prediction',\n  'publishDate': '2019-07-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2006.14480v1.pdf',\n  'location': 'Palo Alto, USA',\n  'rawData': 'Yes',\n  'citationCount': 87,\n  'completionStatus': 'partially Complete',\n  'paperId': '5ff9429efe7b6c4e7c039064920600e2b7664828',\n  'arxivId': '2006.14480',\n  'altmetrics': ({'score': 20.23},\n   {'percentile': 93},\n   {'similar_age_3m_percentile': 88},\n   {'total_readers': 182})},\n {'id': 'Synscapes',\n  'href': 'https://7dlabs.com/synscapes-overview',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '25000',\n  'numberOfScenes': '25000',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'RGB images in PNG format 1440x720 & upscaled version 2048x1024',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2018-10-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1810.08705v1.pdf',\n  'location': '-',\n  'rawData': '-',\n  'citationCount': 87,\n  'completionStatus': 'partially Complete',\n  'paperId': '1373195c26eab581138579f7389cdf8b7a94a4bb',\n  'arxivId': '1810.08705',\n  'altmetrics': ({'score': 15.45},\n   {'percentile': 91},\n   {'similar_age_3m_percentile': 86},\n   {'total_readers': 96})},\n {'id': 'One Thousand and One Hours',\n  'href': 'https://level-5.global/data/prediction/',\n  'size_storage': '-',\n  'size_hours': '1118',\n  'frames': '-',\n  'numberOfScenes': '170000',\n  'samplingRate': '-',\n  'lengthOfScenes': '25',\n  'sensors': 'camera, lidar, radar',\n  'sensorDetail': '-',\n  'recordingPerspective': 'top view',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'trajectories',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA-4.0)',\n  'relatedDatasets': 'Level 5 Perception Dataset',\n  'publishDate': '2020-06-25',\n  'lastUpdate': '-',\n  'paperTitle': 'One Thousand and One Hours: Self-driving Motion Prediction Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/2006.14480.pdf',\n  'location': 'Palo Alto, California, USA',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.2006.14480',\n  'citationCount': 87,\n  'completionStatus': 'complete',\n  'paperId': '5ff9429efe7b6c4e7c039064920600e2b7664828',\n  'arxivId': '2006.14480',\n  'altmetrics': ({'score': 15.45},\n   {'percentile': 91},\n   {'similar_age_3m_percentile': 86},\n   {'total_readers': 96})},\n {'id': 'WildDash',\n  'href': 'https://wilddash.cc/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '156',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'various sources, e.g. YouTube',\n  'benchmark': 'semantic segmentation, instance segmentation, panoptic segmentation',\n  'annotations': 'semantic segmentation, instance segmentation',\n  'licensing': 'CC-BY-NC 4.0 ',\n  'relatedDatasets': '-',\n  'publishDate': '2018-02-01',\n  'lastUpdate': '2020-06-01',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf',\n  'location': 'All over the world',\n  'rawData': 'Yes',\n  'DOI': '10.1007/978-3-030-01231-1_25',\n  'citationCount': 86,\n  'completionStatus': 'partially Complete',\n  'paperId': '229e105fd4d34815e476702dd5ca4362943c475d',\n  'altmetrics': ({'score': 15.45},\n   {'percentile': 91},\n   {'similar_age_3m_percentile': 86},\n   {'total_readers': 96})},\n {'id': 'A2D2',\n  'href': 'https://www.a2d2.audi/a2d2/en.html',\n  'size_storage': '2300',\n  'size_hours': '-',\n  'frames': '433833',\n  'numberOfScenes': '3',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '5x lidar 16 channels 360 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2020-04-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2004.06320.pdf',\n  'location': 'Three cities in the south of Germany',\n  'rawData': 'Yes',\n  'citationCount': 82,\n  'completionStatus': 'partially Complete',\n  'paperId': '07e3db9c2034b522c3cf04399602cc7124964306',\n  'arxivId': '2004.06320',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Five Roundabouts Dataset',\n  'href': 'http://its.acfr.usyd.edu.au/datasets-2/five-roundabouts-dataset/',\n  'size_storage': '0.5',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar',\n  'sensorDetail': '6x 4 beam ibeo LUX lidars 110 25Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'trajectory tracking information',\n  'licensing': 'Creative Commons Attribution 4.0',\n  'relatedDatasets': 'naturalistic-intersection-driving-dataset',\n  'publishDate': '2019-05-13',\n  'lastUpdate': '-',\n  'paperTitle': 'Naturalistic Driver Intention and Path Prediction Using Recurrent Neural Networks',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8713418',\n  'location': 'Sydney, Australia',\n  'rawData': '-',\n  'DOI': '10.1109/TITS.2019.2913166',\n  'citationCount': 80,\n  'completionStatus': 'complete',\n  'paperId': '1e3635f8c1c78f224a0c1c4d3a2197eaeb38723f',\n  'arxivId': '1807.09995',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'UAH-DriveSet',\n  'href': 'http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/',\n  'size_storage': '-',\n  'size_hours': '8.33',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, gps',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation',\n  'relatedDatasets': '-',\n  'publishDate': '2016-11-01',\n  'lastUpdate': '-',\n  'paperTitle': 'Need Data for Driver Behaviour Analysis? Presenting the Public UAH-DriveSet',\n  'relatedPaper': 'http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera16itsc.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'DOI': '10.1109/ITSC.2016.7795584',\n  'citationCount': 79,\n  'completionStatus': 'complete',\n  'paperId': '3d0e1272dae0b8edefd02121a1164e1828120fc9',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Tsinghua Daimler Cyclist Detection',\n  'href': 'http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '11467',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'cyclist detection',\n  'annotations': '2d bounding boxes',\n  'licensing': 'Freely available non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2016-06-19',\n  'lastUpdate': '-',\n  'paperTitle': 'A new benchmark for vision-based cyclist detection',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/7535515',\n  'location': 'Beijing, China',\n  'rawData': '-',\n  'DOI': '10.1109/IVS.2016.7535515',\n  'citationCount': 74,\n  'completionStatus': 'complete',\n  'paperId': '48b7b474af1e86ee6e9db66972155c10cbbdace6',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'DBNet',\n  'href': 'http://www.dbehavior.net/',\n  'size_storage': '1610',\n  'size_hours': '10',\n  'frames': '56800',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar, camera, steering angle meter, gps',\n  'sensorDetail': '1x Velodyne HDL-32E laser scanner 360 10Hz, 1x Velodyne VLP-16 laser scanner, 1x color dashboard camera 1920x1080 30Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'Driving policy prediction',\n  'annotations': '-',\n  'licensing': 'Freely available for non-commercial use upon registration',\n  'relatedDatasets': '-',\n  'publishDate': '2018-06-23',\n  'lastUpdate': '-',\n  'paperTitle': 'LiDAR-Video Driving Dataset: Learning Driving Policies Effectively',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8578713',\n  'location': '-',\n  'rawData': 'Yes',\n  'DOI': '10.1109/CVPR.2018.00615',\n  'citationCount': 73,\n  'completionStatus': 'complete',\n  'paperId': 'c40bfdc874cf4f8d3931f21e3975054ce8a75513',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'LiDAR-Video Driving Dataset',\n  'href': 'https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf',\n  'size_storage': '1000',\n  'size_hours': '-',\n  'frames': '10000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, accelerometer, steering angle meter',\n  'sensorDetail': '1x HDL-32E Velodyne lidar 360 10Hz, 1x VLP-16 Velodyne lidar, 1x Dashboard camera 1920x1080 30Hz ',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'driver behaviour',\n  'annotations': '-',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2018-06-18',\n  'lastUpdate': '-',\n  'paperTitle': 'LiDAR-Video Driving Dataset: Learning Driving Policies Effectively',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/CVPR.2018.00615',\n  'citationCount': 73,\n  'completionStatus': 'complete',\n  'paperId': 'c40bfdc874cf4f8d3931f21e3975054ce8a75513',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'LostAndFound',\n  'href': 'http://www.6d-vision.com/lostandfounddataset',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '21040',\n  'numberOfScenes': '112',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'stereo camera setup baseline 21cm 2048x1024',\n  'benchmark': 'anomaly detection',\n  'annotations': 'semantic segmentation',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2016-09-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1609.04653.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'citationCount': 70,\n  'completionStatus': 'partially Complete',\n  'paperId': '927ee108115e03cc14c70e567b044e66423fb54b',\n  'arxivId': '1609.04653',\n  'DOI': '10.1109/IROS.2016.7759186',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'HD1K',\n  'href': 'http://hci-benchmark.iwr.uni-heidelberg.de/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '55',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar, camera, gps/imu',\n  'sensorDetail': '1x RIEGL VMX-250-CS6 laser scanner, 1x stereo system with 2 cameras 2560x1080 69.5 200Hz, 1x Applanix POS-LV 510 gnss & imu unit',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'stereo, optical flow, single image depth prediction, object detection, and semantic segmentation',\n  'annotations': 'depth and optical flow',\n  'licensing': 'Freely available for research purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2018-02-01',\n  'lastUpdate': '2018-03-05',\n  'paperTitle': 'The HCI Benchmark Suite: Stereo And Flow Ground Truth With Uncertainties for Urban Autonomous Driving',\n  'relatedPaper': 'http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/CVPRW.2016.10',\n  'citationCount': 70,\n  'completionStatus': 'complete',\n  'paperId': 'e24f9317ad8e391902c43f6b160730e98ad9904d',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'HCI Challenging Stereo',\n  'href': 'https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '10000',\n  'numberOfScenes': '11',\n  'samplingRate': '100',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '2 x Photon Focus MV1-D1312-160-CL-12',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available for research purposes only',\n  'relatedDatasets': '-',\n  'publishDate': '2012-03-07',\n  'lastUpdate': '-',\n  'paperTitle': 'Outdoor stereo camera system for the generation of real-world benchmark data sets',\n  'relatedPaper': 'https://www.spiedigitallibrary.org/journals/Optical-Engineering/volume-51/issue-2/021107/Outdoor-stereo-camera-system-for-the-generation-of-real-world/10.1117/1.OE.51.2.021107.short?SSO=1',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1117/1.OE.51.2.021107',\n  'citationCount': 67,\n  'completionStatus': 'complete',\n  'paperId': '892f0d45074f1d8053ddcd6b8be2f50ebbcf014a',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'SeeingThroughFog',\n  'href': 'https://github.com/princeton-computational-imaging/SeeingThroughFog',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '1400000',\n  'numberOfScenes': '-',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, radar, lidar, imu, weather sensor',\n  'sensorDetail': '2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'weather of scenes in frames',\n  'licensing': 'Freely available for research and teaching purposes',\n  'relatedDatasets': 'Gated2Gated, Gated2Depth, PointCloudDeNoising',\n  'publishDate': '2019-02-24',\n  'lastUpdate': '-',\n  'paperTitle': 'Seeing Through FogWithout Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather',\n  'relatedPaper': 'https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/figures/AdverseWeatherFusion.pdf',\n  'location': 'Germany, Sweden, Denmark and Finland',\n  'rawData': '-',\n  'DOI': '10.1109/cvpr42600.2020.01170',\n  'citationCount': 67,\n  'completionStatus': 'complete',\n  'paperId': '3773df00a1ac019fbda36ad965248e60b93cd76e',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Waymo Open Motion',\n  'href': 'https://waymo.com/open/data/motion/',\n  'size_hours': '574',\n  'size_storage': '-',\n  'frames': '20670800',\n  'numberOfScenes': '103354',\n  'samplingRate': '10',\n  'lengthOfScenes': '20',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '5x cameras, 5x lidar, ',\n  'benchmark': 'motion prediction, interaction prediction',\n  'annotations': '3d bounding boxes, 3d hd map information',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': 'Waymo Open Perception',\n  'publishDate': '2021-03-01',\n  'lastUpdate': '2021-09-01',\n  'relatedPaper': 'https://arxiv.org/pdf/2104.10133.pdf',\n  'location': 'San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA',\n  'rawData': 'No',\n  'citationCount': 54,\n  'completionStatus': 'partially Complete',\n  'paperId': 'b07fb2dbaeda171c5f1d821f492dd0cb8bd4e668',\n  'arxivId': '2104.10133',\n  'DOI': '10.1109/iccv48922.2021.00957',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'CADC',\n  'href': 'http://cadcd.uwaterloo.ca/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '75',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'cameras, lidar, gps',\n  'sensorDetail': '8x camera Ximea MQ013CG-E2 1280x1024 10Hz,  1x lidar Veldyne VLP-32C 360 10Hz, 1x NovAtel OEM638 Triple-Frequency GPS, 1x Sensonor STIM300 MEMS 100Hz IMU, 2x Xsens 200Hz IMU',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d/3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2020-01-27',\n  'lastUpdate': '-',\n  'paperTitle': 'Canadian Adverse Driving Conditions Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/2001.10117.pdf',\n  'location': 'Waterloo region in Ontorio, Canada',\n  'rawData': '-',\n  'DOI': '10.1177/0278364920979368',\n  'citationCount': 51,\n  'completionStatus': 'complete',\n  'paperId': '3fa5acebaa27401976ed900157ebce1c2d9fd0cb',\n  'arxivId': '2001.10117',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Fishyscapes',\n  'href': 'https://fishyscapes.com/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations',\n  'benchmark': 'anomaly detection, semantic segmentation',\n  'annotations': 'semantic segmentation',\n  'licensing': '-',\n  'relatedDatasets': 'Cityscapes, LostAndFound',\n  'publishDate': '2019-09-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1904.03215.pdf',\n  'location': '-',\n  'rawData': 'No',\n  'citationCount': 50,\n  'completionStatus': 'partially Complete',\n  'paperId': '10a83f9a07c803ceef8e09e12ad606dc74ea76b8',\n  'arxivId': '1904.03215',\n  'DOI': '10.1007/s11263-021-01511-6',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'SemKITTI-DVPS',\n  'href': 'https://github.com/joe-siyuan-qiao/ViP-DeepLab',\n  'relatedDatasets': 'SemanticKITTI',\n  'relatedPaper': 'https://arxiv.org/pdf/2012.05258',\n  'location': 'Karlsruhe, Germany',\n  'rawData': 'Yes',\n  'citationCount': 48,\n  'completionStatus': 'partially Complete',\n  'paperId': '86f88bc71034122eb9d4f8ea16371ebd3efd42cc',\n  'arxivId': '2012.05258',\n  'DOI': '10.1109/CVPR46437.2021.00399',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Cityscapes-DVPS',\n  'href': 'https://github.com/joe-siyuan-qiao/ViP-DeepLab',\n  'relatedDatasets': 'Cityscapes',\n  'relatedPaper': 'https://arxiv.org/pdf/2012.05258',\n  'rawData': 'Yes',\n  'citationCount': 48,\n  'completionStatus': 'partially Complete',\n  'paperId': '86f88bc71034122eb9d4f8ea16371ebd3efd42cc',\n  'arxivId': '2012.05258',\n  'DOI': '10.1109/CVPR46437.2021.00399',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Synthetic Discrepancy Datasets',\n  'href': 'https://github.com/cvlab-epfl/detecting-the-unexpected',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf',\n  'DOI': '10.1109/ICCV.2019.00224',\n  'citationCount': 48,\n  'completionStatus': 'partially Complete',\n  'paperId': 'ce05ebc31e7aa8e4152673bb240b9f8d27ebea21',\n  'arxivId': '1904.07595',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'A*3D',\n  'href': 'https://github.com/I2RDL2/ASTAR-3D',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '39179',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '2x PointGrey Chameleon3 USB3 Global shutter color cameras 2048x1536 57.3 55Hz, 1x Velodyne HDL-64ES3 3D-LiDAR 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '3d object detection',\n  'annotations': '3d bounding boxes',\n  'licensing': 'Freely available for noncommercial academic research purposes only',\n  'relatedDatasets': '-',\n  'publishDate': '2019-10-04',\n  'lastUpdate': '-',\n  'paperTitle': 'A 3D Dataset: Towards Autonomous Driving in Challenging Environments',\n  'relatedPaper': 'https://arxiv.org/pdf/1909.07541.pdf',\n  'location': 'Singapore',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA40945.2020.9197385',\n  'citationCount': 47,\n  'completionStatus': 'complete',\n  'paperId': 'ff3835b7b2d3f62e25564536dc519616d364f773',\n  'arxivId': '1909.07541',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Toronto 3D',\n  'href': 'https://github.com/WeikaiTan/Toronto-3D',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps',\n  'sensorDetail': '1x Teledyne Optech Maverick consists of a 32-line LiDAR sensor, a Ladybug 5 panoramic camera, a GNSS system, and a SLAM system',\n  'recordingPerspective': \"bird's eye view\",\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'object class labels',\n  'licensing': 'Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2020-03-22',\n  'lastUpdate': '2020-04-23',\n  'paperTitle': 'Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways',\n  'relatedPaper': 'https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf',\n  'location': 'Toronto, Canada',\n  'rawData': '-',\n  'DOI': '10.1109/CVPRW50498.2020.00109',\n  'citationCount': 47,\n  'completionStatus': 'complete',\n  'paperId': '128360ee782b8078f5410f4cfa7b63a582abb1bd',\n  'arxivId': '2003.08284',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'NightOwls',\n  'href': 'https://www.nightowls-dataset.org/',\n  'size_storage': '-',\n  'size_hours': '5.17',\n  'frames': '279000',\n  'numberOfScenes': '40',\n  'samplingRate': '15',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x industry standard 1024x640 camera',\n  'benchmark': 'pedestrian detection, object detection',\n  'annotations': 'bounding boxes, attributes, temporal tracking annotations',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2018-12-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf',\n  'location': 'Several cities across Europe',\n  'rawData': 'Yes',\n  'DOI': '10.1007/978-3-030-20887-5_43',\n  'citationCount': 41,\n  'completionStatus': 'partially Complete',\n  'paperId': 'aa797b600d0e66b331088db38ad9d1958ae6ac74',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'The EuroCity Persons Dataset',\n  'href': 'https://arxiv.org/abs/1805.07193',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '47300',\n  'numberOfScenes': '-',\n  'samplingRate': '20',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x camera 1920x1080 20Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'object detection',\n  'annotations': '2d bounding boxes',\n  'licensing': 'freely available for research use by eligiible persons',\n  'relatedDatasets': 'ECP2.5D - Person Localization in Traffic Scenes',\n  'publishDate': '2019-03-31',\n  'lastUpdate': '2020-11-11',\n  'paperTitle': 'The EuroCity Persons Dataset: A Novel Benchmark for Object Detection',\n  'relatedPaper': 'https://arxiv.org/pdf/1805.07193.pdf',\n  'location': '12 countries and 31 cities across Europe',\n  'rawData': '-',\n  'DOI': '10.1109/TPAMI.2019.2897684',\n  'citationCount': 38,\n  'completionStatus': 'complete',\n  'paperId': '5d2f340beac2389a6f0ac27672faf60fd86e3c2a',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Street Hazards',\n  'href': 'https://github.com/hendrycks/anomaly-seg',\n  'size_storage': '2.0',\n  'size_hours': '-',\n  'frames': '7656',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'No',\n  'mapData': 'No',\n  'benchmark': 'Anamoly object segmentation',\n  'annotations': 'semantic segmentation of anamolies',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2019-11-25',\n  'lastUpdate': '-',\n  'paperTitle': 'Scaling Out-of-Distribution Detection for Real-World Settings',\n  'relatedPaper': 'https://arxiv.org/pdf/1911.11132.pdf',\n  'location': 'Carla',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.1911.11132',\n  'citationCount': 36,\n  'completionStatus': 'complete',\n  'paperId': '0e4b0d177e550d365f456375781cd0e4f7a04979',\n  'arxivId': '1911.11132',\n  'altmetrics': ({'score': 14.08},\n   {'percentile': 90},\n   {'similar_age_3m_percentile': 84},\n   {'total_readers': 204})},\n {'id': 'Comma2k19',\n  'href': 'https://github.com/commaai/comma2k19',\n  'size_storage': '100',\n  'size_hours': '33.65',\n  'frames': '-',\n  'numberOfScenes': '2019',\n  'samplingRate': '-',\n  'lengthOfScenes': '60',\n  'sensors': 'camera, radar, gnss/imu ',\n  'sensorDetail': 'two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'MIT',\n  'relatedDatasets': '-',\n  'publishDate': '2018-12-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'http://export.arxiv.org/pdf/1812.05752',\n  'location': \"California's 280 highway, USA\",\n  'rawData': 'Yes',\n  'citationCount': 33,\n  'completionStatus': 'partially Complete',\n  'paperId': '50c1ab22f442470efbe3198f0b338fb699416bc5',\n  'arxivId': '1812.05752',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'rounD',\n  'href': 'https://www.round-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '6',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x DJI Phantom 4 Pro quadcopter camera 4096x2160 25Hz',\n  'recordingPerspective': 'top-view',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes for objects, trajectory tracking information',\n  'licensing': 'Freely available for non-commercial use only upon registration',\n  'relatedDatasets': 'highD, inD, exiD and uniD',\n  'publishDate': '2020-09-20',\n  'lastUpdate': '-',\n  'paperTitle': 'The rounD Dataset: A Drone Dataset of Road User Trajectories at Roundabouts in Germany',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9294728',\n  'location': 'Aachen and Alsdorf, Germany',\n  'rawData': '-',\n  'DOI': '10.1109/ITSC45102.2020.9294728',\n  'citationCount': 32,\n  'completionStatus': 'complete',\n  'paperId': '57a0f2c6fc3c03e483f2b4ba8528b3c200b01b32',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'RUGD: Robot Unstructured Ground Driving',\n  'href': 'http://rugd.vision/',\n  'size_storage': '5.4',\n  'size_hours': '-',\n  'frames': '37000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '1x Prosilica GT2750C camera 1376x1110, 1x Velodyne HDL-32 LiDAR, 1x Garmin GPS receiver, 1x Microstrain GX3-25 IMU',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'semantic segmentation',\n  'annotations': '',\n  'licensing': 'Freely available for research purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2019-11-08',\n  'lastUpdate': '-',\n  'paperTitle': 'A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments',\n  'relatedPaper': 'http://rugd.vision/pdfs/RUGD_IROS2019.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/IROS40897.2019.8968283',\n  'citationCount': 32,\n  'completionStatus': 'complete',\n  'paperId': 'e38ab881e5de9d191f08e532d33edd486e239503',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'Ford Autonomous Vehicle Dataset',\n  'href': 'https://avdata.ford.com/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80 15Hz, 1x Flea3 GigE Point Grey Camera 40 7Hz, 1x Applanix POS LV gps/imu',\n  'benchmark': '-',\n  'annotations': '3d point cloud maps, ground reflectivity map',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International',\n  'relatedDatasets': '-',\n  'publishDate': '2020-03-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://s23.q4cdn.com/258866874/files/doc_downloads/2020/03/2003.07969.pdf',\n  'location': 'Michigan, USA',\n  'rawData': 'True',\n  'DOI': '10.1177/0278364920961451',\n  'citationCount': 31,\n  'completionStatus': 'partially Complete',\n  'paperId': '4427aaadc8d88f21f9201f703050ecbd0225c050',\n  'arxivId': '2003.07969',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'D^2 City',\n  'href': 'https://outreach.didichuxing.com/d2city',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '11211',\n  'samplingRate': '25',\n  'lengthOfScenes': '30',\n  'sensors': 'camera',\n  'sensorDetail': '',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes and inter-frame tracking labels',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2019-04-03',\n  'lastUpdate': '-',\n  'paperTitle': 'D^2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios',\n  'relatedPaper': 'https://arxiv.org/pdf/1904.01975v1.pdf',\n  'location': '5 Chinese cities',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.1904.01975',\n  'citationCount': 31,\n  'completionStatus': 'complete',\n  'paperId': '85e3a8a6c46788bd01ded879bcca8f8eed0ad4f7',\n  'arxivId': '1904.01975',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'ONCE',\n  'href': 'https://once-for-auto-driving.github.io/index.html',\n  'size_storage': '-',\n  'size_hours': '144',\n  'frames': '1000000',\n  'numberOfScenes': '581',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '7x color cameras 1920x1020 10Hz, 1x 40-beam lidar 360 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'benchmark': '3d object detection',\n  'annotations': '2d/3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)',\n  'relatedDatasets': 'ONCE-3DLanes',\n  'publishDate': '2021-05-18',\n  'lastUpdate': '2021-08-05',\n  'paperTitle': 'One Million Scenes for Autonomous Driving: ONCE Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/2106.11037.pdf',\n  'location': 'China',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.2106.11037',\n  'citationCount': 29,\n  'completionStatus': 'complete',\n  'paperId': '5f7d77117767417116a3cfbd9ee0c86000002fed',\n  'arxivId': '2106.11037',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'RELLIS-3D Dataset',\n  'href': 'https://unmannedlab.github.io/research/RELLIS-3D',\n  'size_storage': '58.1',\n  'size_hours': '-',\n  'frames': '13556',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '1x Nerian Karmin2 + Nerian SceneScan: 3D StereoCamera 10Hz, 1x RGB Camera: Basler acA1920-50gc camera with 16mm/F18 EDMUND Optics lens 1920x1200 10Hz, 1x Ouster OS1 LiDAR 64 Channels 10 Hz, 1x Velodyne Ultra Puck: 32 Channels 10Hz, Vectornav VN-300 Dual Antenna GNSS/INS 300Hz GPS, 100Hz IMU',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation of 2d image and 3d point clouds',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License',\n  'relatedDatasets': 'SemanticUSL: A Dataset for LiDAR Semantic Segmentation Domain Adaptation',\n  'publishDate': '2020-11-26',\n  'lastUpdate': '2022-01-24',\n  'paperTitle': 'RELLIS-3D Dataset: Data, Benchmarks and Analysis',\n  'relatedPaper': 'https://arxiv.org/pdf/2011.12954.pdf',\n  'location': 'Rellis Campus of Texas A&M University',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA48506.2021.9561251',\n  'citationCount': 29,\n  'completionStatus': 'complete',\n  'paperId': '7c125b68099bed3d1ef010840c46f8b1b6510a62',\n  'arxivId': '2011.12954',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'DIPLECS Autonomous Driving Datasets',\n  'href': 'https://cvssp.org/data/diplecs/',\n  'size_storage': '-',\n  'size_hours': '4',\n  'frames': '207364',\n  'numberOfScenes': '4',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, steering angle meter, eye tracker',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available for academic purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2015-10-07',\n  'lastUpdate': '-',\n  'paperTitle': 'How Much of Driving Is Preattentive?',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7293673',\n  'location': 'Sweden and United Kingdom',\n  'rawData': '-',\n  'DOI': '10.1109/TVT.2015.2487826',\n  'citationCount': 29,\n  'completionStatus': 'complete',\n  'paperId': '11a092c80e64e36f41178e779345d17d03fb9c6f',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'Street Learn',\n  'href': 'https://sites.google.com/view/streetlearn/dataset',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '143,000',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'The StreetLearn dataset is a limited set of Google Street View images',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'The dataset is freely available upon request with a license agreement',\n  'relatedDatasets': '-',\n  'publishDate': '2019-03-04',\n  'lastUpdate': '',\n  'paperTitle': 'The StreetLearn Environment and Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/1903.01292.pdf',\n  'location': 'Manhattan and Pittsburgh, USA',\n  'rawData': '-',\n  'DOI': '-',\n  'citationCount': 29,\n  'completionStatus': 'complete',\n  'paperId': 'aa279e8e9c37d0926c45681565bf1b6d4acd65df',\n  'arxivId': '1903.01292',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'LIBRE',\n  'href': 'https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, odometry, gps/imu',\n  'sensorDetail': '10 different lidars compared (VLS-128, HDL-64S2, HDL-32E, VLP-32C, VLP-16, Pandar64, Pandar40P, OS1-64, OS1-16, RS-Lidar32)',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'Lidar sensors',\n  'annotations': '-',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2020-10-19',\n  'lastUpdate': '-',\n  'paperTitle': 'LIBRE: The Multiple 3D LiDAR Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/2003.06129.pdf',\n  'location': 'Nagoya, Japan',\n  'rawData': '-',\n  'DOI': '10.1109/IV47402.2020.9304681',\n  'citationCount': 28,\n  'completionStatus': 'complete',\n  'paperId': '809043703da2103b61accdbdcfd5e633bf4ce76c',\n  'arxivId': '2003.06129',\n  'altmetrics': ({'score': 1.75},\n   {'percentile': 38},\n   {'similar_age_3m_percentile': 45},\n   {'total_readers': 41})},\n {'id': 'RADIATE',\n  'href': 'http://pro.hw.ac.uk/radiate/',\n  'size_storage': '-',\n  'size_hours': '5',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360 10Hz, 1x Navtech CTS350-X radar 360, 1x Advanced Navigation Spatial Dual GPS/IMU',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International',\n  'relatedDatasets': '-',\n  'publishDate': '2020-10-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2010.09076.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'citationCount': 27,\n  'completionStatus': 'partially Complete',\n  'paperId': '981e0cc0c4162aec6ce160f9f7607d5e7ad94322',\n  'arxivId': '2010.09076',\n  'altmetrics': ({'score': 3.85},\n   {'percentile': 70},\n   {'similar_age_3m_percentile': 62},\n   {'total_readers': 99})},\n {'id': 'CADP',\n  'href': 'https://ankitshah009.github.io/accident_forecasting_traffic_camera',\n  'size_storage': '-',\n  'size_hours': '5.2',\n  'frames': '-',\n  'numberOfScenes': '1416',\n  'samplingRate': '20',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': \"Bird's Eye\",\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'Object detectiona and accident forecasting',\n  'licensing': 'freely available for non-commercial use',\n  'relatedDatasets': '-',\n  'publishDate': '2018-10-04',\n  'lastUpdate': '-',\n  'paperTitle': 'CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis',\n  'relatedPaper': 'https://ppms.cit.cmu.edu/media/project_files/CADP_IEEE_Camera_Ready_Final.pdf',\n  'location': 'Videos sampled from YouTube',\n  'rawData': '-',\n  'DOI': '10.1109/AVSS.2018.8639160',\n  'citationCount': 26,\n  'completionStatus': 'complete',\n  'paperId': '4dde27160c8155b209b6630cd3797b6eea04c792',\n  'arxivId': '1809.05782',\n  'altmetrics': ({'score': 3.85},\n   {'percentile': 70},\n   {'similar_age_3m_percentile': 62},\n   {'total_readers': 99})},\n {'id': 'UTBM EU LTD',\n  'href': 'https://epan-utbm.github.io/utbm_robocar_dataset/',\n  'relatedPaper': 'https://arxiv.org/pdf/1909.03330.pdf',\n  'rawData': 'Yes',\n  'publishDate': '2020-08-01',\n  'citationCount': 26,\n  'completionStatus': 'partially Complete',\n  'paperId': '397660e626a0bd016c22c6639ffb15c9a4d91f9c',\n  'arxivId': '1909.03330',\n  'DOI': '10.1109/IROS45743.2020.9341406',\n  'altmetrics': ({'score': 3.85},\n   {'percentile': 70},\n   {'similar_age_3m_percentile': 62},\n   {'total_readers': 99})},\n {'id': 'PointCloudDeNoising',\n  'href': 'https://github.com/rheinzler/PointCloudDeNoising',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '175941',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar',\n  'sensorDetail': '1 Velodyne VLP32c lidar sensor',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'pointwise annotations',\n  'licensing': 'Freely available for research and teaching purposes',\n  'relatedDatasets': 'Gated2Depth, Gated2Gated, SeeingThroughFog',\n  'publishDate': '2019-12-09',\n  'lastUpdate': '-',\n  'paperTitle': 'CNN-based Lidar Point Cloud De-Noising in Adverse Weather',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8990038',\n  'location': \"CEREMA's climatic chamber\",\n  'rawData': '-',\n  'DOI': '10.1109/LRA.2020.2972865',\n  'citationCount': 25,\n  'completionStatus': 'complete',\n  'paperId': 'daace40473375f90283b2449760968c63c4e3870',\n  'arxivId': '1912.03874',\n  'altmetrics': ({'score': 3.85},\n   {'percentile': 70},\n   {'similar_age_3m_percentile': 62},\n   {'total_readers': 99})},\n {'id': 'Waymo Block-NeRF',\n  'href': 'https://waymo.com/research/block-nerf/',\n  'relatedPaper': 'https://arxiv.org/abs/2202.05263',\n  'citationCount': 25,\n  'completionStatus': 'partially Complete',\n  'paperId': 'd7d1bbade9453f0348fac8a5c60d131528b87fcf',\n  'arxivId': '2202.05263',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Seasonal Variation Dataset',\n  'href': 'http://www.cs.cmu.edu/~aayushb/localization/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, imu/gps',\n  'sensorDetail': '1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Publicly available for research community',\n  'relatedDatasets': 'Bay Area Dataset, Illumination Changes in a day',\n  'publishDate': '2014-06-11',\n  'lastUpdate': '-',\n  'paperTitle': 'Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization',\n  'relatedPaper': 'http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf',\n  'location': 'California bay area and Pittsburgh, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/IVS.2014.6856605',\n  'citationCount': 24,\n  'completionStatus': 'complete',\n  'paperId': '176e8eeefce2039189bfd65a8d0346e480591db7',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Bay Area Dataset',\n  'href': 'http://www.cs.cmu.edu/~aayushb/localization/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, imu/gps',\n  'sensorDetail': '1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Available on request',\n  'relatedDatasets': 'Seasonal Variation Dataset, Illumination Changes in a day',\n  'publishDate': '2014-06-11',\n  'lastUpdate': '-',\n  'paperTitle': 'Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization',\n  'relatedPaper': 'http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf',\n  'location': 'California bay area and Pittsburgh, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/IVS.2014.6856605',\n  'citationCount': 24,\n  'completionStatus': 'complete',\n  'paperId': '176e8eeefce2039189bfd65a8d0346e480591db7',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Illumination Changes in a day',\n  'href': 'http://www.cs.cmu.edu/~aayushb/localization/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, imu/gps',\n  'sensorDetail': '1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Available on request',\n  'relatedDatasets': 'Bay Area Dataset, Seasonal Variation Dataset',\n  'publishDate': '2014-06-11',\n  'lastUpdate': '-',\n  'paperTitle': 'Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization',\n  'relatedPaper': 'http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf',\n  'location': 'California bay area and Pittsburgh, USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/IVS.2014.6856605',\n  'citationCount': 24,\n  'completionStatus': 'complete',\n  'paperId': '176e8eeefce2039189bfd65a8d0346e480591db7',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'DDD 20',\n  'href': 'https://sites.google.com/view/davis-driving-dataset-2020/home',\n  'size_storage': '1300',\n  'size_hours': '51',\n  'frames': '-',\n  'numberOfScenes': '216',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, car parameters',\n  'sensorDetail': '1x DAVIS346B 346x260 up to 50Hz, vehicle bus data',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-ShareAlike 4.0 International',\n  'relatedDatasets': 'DDD 17',\n  'publishDate': '2020-02-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2005.08605.pdf',\n  'location': 'California, USA',\n  'rawData': 'Yes',\n  'citationCount': 23,\n  'completionStatus': 'partially Complete',\n  'paperId': '15d351dbd12891b3118ed40b107b685118fe7c86',\n  'arxivId': '2005.08605',\n  'DOI': '10.1109/ITSC45102.2020.9294515',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Talk2Car',\n  'href': 'https://talk2car.github.io/',\n  'size_storage': '300',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '850',\n  'samplingRate': '-',\n  'lengthOfScenes': '20',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '1x lidar 32 channels 360 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'find bounding box for objects based on user commands in natural language',\n  'annotations': 'bounding box and command in natural language related to the bounding box',\n  'licensing': 'MIT license',\n  'relatedDatasets': '-',\n  'publishDate': '2020-03-18',\n  'lastUpdate': '2021-10-06',\n  'paperTitle': 'Talk2Car: Taking Control of Your Self-Driving Car',\n  'relatedPaper': 'https://arxiv.org/pdf/1909.10838.pdf',\n  'location': 'Boston and Singapore',\n  'rawData': '-',\n  'DOI': '10.18653/v1/D19-1215',\n  'citationCount': 23,\n  'completionStatus': 'complete',\n  'paperId': '61ac1603c0ad5268b202506bb2cddbfe10c45d9f',\n  'arxivId': '1909.10838',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'DDD20: DAVIS Driving Dataset 2020',\n  'href': 'https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub',\n  'size_storage': '1300',\n  'size_hours': '51',\n  'frames': '-',\n  'numberOfScenes': '216',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x 346x260-pixel DAVIS346 camera 56',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-ShareAlike 4.0 International License',\n  'relatedDatasets': 'DDD17',\n  'publishDate': '2017-06-05',\n  'lastUpdate': '2020-02-01',\n  'paperTitle': 'DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction',\n  'relatedPaper': 'https://arxiv.org/pdf/2005.08605.pdf',\n  'location': 'Various states of USA, Switzerland and Germany',\n  'rawData': 'Yes',\n  'DOI': '10.1109/ITSC45102.2020.9294515',\n  'citationCount': 23,\n  'completionStatus': 'complete',\n  'paperId': '15d351dbd12891b3118ed40b107b685118fe7c86',\n  'arxivId': '2005.08605',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'BLVD',\n  'href': 'https://github.com/VCCIV/BLVD',\n  'size_storage': '42.7',\n  'size_hours': '-',\n  'frames': '120000',\n  'numberOfScenes': '654',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '2x multi-view color cameras 1920x500 30Hz, 1x Velodyne HDL-64E lidar 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction',\n  'annotations': '5D semantics, 3D bounding boxes, 4D object IDs, 5D interactive event and 5D intention',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2019-05-20',\n  'lastUpdate': '-',\n  'paperTitle': 'BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8793523',\n  'location': 'Changshu, Jiangsu Province, China',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA.2019.8793523',\n  'citationCount': 23,\n  'completionStatus': 'complete',\n  'paperId': '5d52bdbb478f31749f33ead669cab87107af6b52',\n  'arxivId': '1903.06405',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Gated2Depth',\n  'href': 'https://github.com/gruberto/Gated2Depth',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '17686',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '1x Aptina AR0230 stereo camera 1920x1080 30Hz, 1x Velodyne HDL64-S3 lidar, 1x gated camera 10bit images 1280x720 120Hz',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available for research and teaching purposes',\n  'relatedDatasets': 'SeeingThroughFog, PointCloudDeNoising, Gated2Gated',\n  'publishDate': '2020-02-13',\n  'lastUpdate': '2020-04-12',\n  'paperTitle': 'Gated2Depth: Real-Time Dense Lidar From Gated Images',\n  'relatedPaper': 'https://arxiv.org/pdf/1902.04997.pdf',\n  'location': 'Germany, Denmark and Sweden',\n  'rawData': '-',\n  'DOI': '10.1109/ICCV.2019.00159',\n  'citationCount': 23,\n  'completionStatus': 'complete',\n  'paperId': '5e5af1388c8e168c898b8f0475a0b9b77abf26fa',\n  'arxivId': '1902.04997',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'MAVD Multimodal Audio-Visual Detection',\n  'href': 'http://multimodal-distill.cs.uni-freiburg.de/',\n  'relatedPaper': 'https://arxiv.org/abs/2103.01353',\n  'citationCount': 22,\n  'completionStatus': 'partially Complete',\n  'paperId': '1614645a0fa9689a187dedefa36bdf5be43734f6',\n  'arxivId': '2103.01353',\n  'DOI': '10.1109/CVPR46437.2021.01144',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'CARRADA Dataset',\n  'href': 'https://github.com/valeoai/carrada_dataset',\n  'size_storage': '288',\n  'size_hours': '-',\n  'frames': '12666',\n  'numberOfScenes': '30',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, radar',\n  'sensorDetail': '1x camera 1238x1024,  1x radar 180',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'range-angle Doppler annotations',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2020-05-04',\n  'lastUpdate': '2021-07',\n  'paperTitle': 'CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations',\n  'relatedPaper': 'https://arxiv.org/pdf/2005.01456.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/ICPR48806.2021.9413181',\n  'citationCount': 21,\n  'completionStatus': 'complete',\n  'paperId': '6d422b430bda380fef9eec49b0ee7e0ec190077f',\n  'arxivId': '2005.01456',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': '4Seasons',\n  'href': 'https://www.4seasons-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '30',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, imu/rtk-gnss',\n  'sensorDetail': '2x cameras stereo baseline 30cm 800x400 (after cropping)',\n  'benchmark': 'globally consistent reference poses',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2020-10-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2009.06364.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'citationCount': 20,\n  'completionStatus': 'partially Complete',\n  'paperId': '1de7bc87547b1162fddcc7b69dd21023d4844e9c',\n  'arxivId': '2009.06364',\n  'DOI': '10.1007/978-3-030-71278-5_29',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'DriveU Traffic Light Dataset',\n  'href': 'https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '15',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x stereo camera 60, 1x stereo camera 130',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes',\n  'licensing': 'freely available for non-commercial research and teaching activities',\n  'relatedDatasets': '-',\n  'publishDate': '2018-11-27',\n  'lastUpdate': '2021-04',\n  'paperTitle': 'The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets',\n  'relatedPaper': 'https://www.researchgate.net/profile/Julian-Mueller-14/publication/327808220_The_DriveU_Traffic_Light_Dataset_Introduction_and_Comparison_with_Existing_Datasets/links/5c1910e4a6fdccfc7056b787/The-DriveU-Traffic-Light-Dataset-Introduction-and-Comparison-with-Existing-Datasets.pdf ',\n  'location': '10 cities across Germany',\n  'rawData': '-',\n  'DOI': '10.1109/ICRA.2018.8460737',\n  'citationCount': 20,\n  'completionStatus': 'complete',\n  'paperId': '26d08a90848c8dba9e4e7c2adee6c38d713699d3',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'Unsupervised Llamas',\n  'href': 'https://unsupervised-llamas.com/llamas/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '100042',\n  'numberOfScenes': '14',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'binary lane marker segmentation, lane-dependent pixel-level segmentation and lane border regression',\n  'annotations': 'dashed lane markings',\n  'licensing': 'Freely available for non-commercial reseach purposes only',\n  'relatedDatasets': '-',\n  'publishDate': '2019-10-27',\n  'lastUpdate': '-',\n  'paperTitle': 'Unsupervised Labeled Lane Markers Using Maps',\n  'relatedPaper': 'https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/ICCVW.2019.00111',\n  'citationCount': 20,\n  'completionStatus': 'complete',\n  'paperId': '423b4c481fbb31bfa0b0ea2745217727f39e4132',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'RANUS',\n  'href': 'https://sites.google.com/site/gmchoe1/ranus',\n  'size_storage': '11.4',\n  'size_hours': '-',\n  'frames': '40000',\n  'numberOfScenes': '50',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '2x Point-grey grasshopper cameras (NIR: GS3-U3-41C6NIR-C, RGB: GS3-U3-41C6C-C) 10Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'pixel level semantic segmentation masks',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2018-02-02',\n  'lastUpdate': '-',\n  'paperTitle': 'RANUS: RGB and NIR Urban Scene Dataset for Deep Scene Parsing',\n  'relatedPaper': 'https://joonyoung-cv.github.io/assets/paper/18_ral_ranus.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.1109/LRA.2018.2801390',\n  'citationCount': 19,\n  'completionStatus': 'complete',\n  'paperId': '144d19b3f820c96b7816842356dadebf16678a8e',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'MTSD',\n  'href': 'https://www.mapillary.com/dataset/trafficsign',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '105830',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Map',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes and sign classification',\n  'licensing': 'Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)',\n  'relatedDatasets': 'Mapillary Vistas',\n  'publishDate': '2020-11-03',\n  'lastUpdate': '-',\n  'paperTitle': 'The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale',\n  'relatedPaper': 'https://link.springer.com/chapter/10.1007/978-3-030-58592-1_5',\n  'location': 'Europe, North and South America, Asia, Africa and Oceania',\n  'rawData': '-',\n  'DOI': '10.1007/978-3-030-58592-1_5',\n  'citationCount': 18,\n  'completionStatus': 'complete',\n  'paperId': '4286427a685ddea72aec0afa6f8bdbd714765cd4',\n  'arxivId': '1909.04422',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'AMUSE',\n  'href': 'http://www.cvl.isy.liu.se/research/datasets/amuse/',\n  'size_storage': '1263',\n  'size_hours': '-',\n  'frames': '117440',\n  'numberOfScenes': '7',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, height, gps/imu, accelerometer',\n  'sensorDetail': '1x Point Grey Ladybug 3^2 camera, 6x synchronized global shutter color cameras 616x1616 360 30Hz, 1x XSens MTi AHRS3 imu sensor, 1x uBlox AEK-4P GPS sensor, 1x Kistler Correvit S-350 Aqua velocity sensor, 1x Kistler HF-500C Height sensor',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License',\n  'relatedDatasets': '-',\n  'publishDate': '2013-06-23',\n  'lastUpdate': '-',\n  'paperTitle': 'A Multi-sensor Traffic Scene Dataset with Omnidirectional Video',\n  'relatedPaper': 'http://liu.diva-portal.org/smash/get/diva2:623885/FULLTEXT01.pdf',\n  'location': 'Sweden',\n  'rawData': '-',\n  'DOI': '10.1109/CVPRW.2013.110',\n  'citationCount': 18,\n  'completionStatus': 'complete',\n  'paperId': '6d39bc2169e612456739e6a6414597fb75e7f68d',\n  'altmetrics': ({'score': 615.1300000000043},\n   {'percentile': 99},\n   {'similar_age_3m_percentile': 99},\n   {'total_readers': 322})},\n {'id': 'RoadAnomaly21',\n  'href': 'https://segmentmeifyoucan.com/datasets',\n  'size_storage': '0.05',\n  'size_hours': '-',\n  'frames': '100',\n  'numberOfScenes': '100',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'images from web resources 2048x1024 & 1280x720',\n  'benchmark': 'anomaly detection',\n  'annotations': 'semantic segmentation',\n  'licensing': 'various, see \"https://github.com/SegmentMeIfYouCan/road-anomaly-\"benchmark\"/blob/master/doc/RoadAnomaly/credits.txt\" for detail',\n  'relatedDatasets': 'RoadObstacle21',\n  'publishDate': '2021-04-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2104.14812.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'citationCount': 17,\n  'completionStatus': 'partially Complete',\n  'paperId': '149f5a50eef5b115fc67c21693c4e701a6161c43',\n  'arxivId': '2104.14812',\n  'altmetrics': ({'score': 3},\n   {'percentile': 44},\n   {'similar_age_3m_percentile': 58},\n   {'total_readers': 28})},\n {'id': 'IDDA',\n  'href': 'https://idda-dataset.github.io/home/',\n  'relatedPaper': 'https://arxiv.org/pdf/2004.08298.pdf',\n  'benchmark': 'semantic segmentation',\n  'rawData': 'Yes',\n  'citationCount': 17,\n  'completionStatus': 'partially Complete',\n  'paperId': '2bf078aba178133b7cd43acfbaa6fcf27db3eee9',\n  'arxivId': '2004.08298',\n  'DOI': '10.1109/LRA.2020.3009075',\n  'altmetrics': ({'score': 3},\n   {'percentile': 44},\n   {'similar_age_3m_percentile': 58},\n   {'total_readers': 28})},\n {'id': 'Cityscapes 3D',\n  'href': 'https://www.cityscapes-dataset.com/',\n  'size_hours': '-',\n  'size_storage': '63.141',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '17',\n  'lengthOfScenes': '1.8',\n  'sensors': 'camera, gps, thermometer',\n  'sensorDetail': 'stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle \"sensors\" & outs\"id\"e temperature & GPS tracks',\n  'benchmark': 'pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection',\n  'annotations': 'dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes',\n  'licensing': 'freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2016-02-01',\n  'lastUpdate': '2020-10-01',\n  'relatedPaper': 'https://arxiv.org/pdf/2006.07864.pdf',\n  'location': '50 cities in Germany and neighboring countries',\n  'rawData': 'Yes',\n  'citationCount': 16,\n  'completionStatus': 'partially Complete',\n  'paperId': '96555d62bfb1281ecf86627ca6d91ff96cda5343',\n  'arxivId': '2006.07864',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'MOTSynth',\n  'href': 'https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42',\n  'relatedPaper': 'https://arxiv.org/pdf/2108.09518.pdf',\n  'citationCount': 16,\n  'completionStatus': 'partially Complete',\n  'paperId': '98d5e7387b6427ca8bddce8c47be2db47c15189c',\n  'arxivId': '2108.09518',\n  'DOI': '10.1109/iccv48922.2021.01067',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'PREVENTION',\n  'href': 'https://prevention-dataset.uah.es/',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8917433',\n  'DOI': '10.1109/ITSC.2019.8917433',\n  'citationCount': 15,\n  'completionStatus': 'partially Complete',\n  'paperId': '05f44a3a3ff0cf3573d59b1fde67e8311b84ad5d',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'RadarScenes',\n  'href': 'https://radar-scenes.com/',\n  'size_storage': '-',\n  'size_hours': '4',\n  'frames': '-',\n  'numberOfScenes': '158',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, radar, odometry',\n  'sensorDetail': '4x 77 GHz series production automotive 60 radar sensor, 1x documentary camera',\n  'benchmark': '-',\n  'annotations': 'point-wise',\n  'licensing': 'Creative Commons Attribution Non Commercial Share Alike 4.0 International',\n  'relatedDatasets': '-',\n  'publishDate': '2021-03-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2104.02493.pdf',\n  'location': '-',\n  'rawData': 'Yes',\n  'citationCount': 13,\n  'completionStatus': 'partially Complete',\n  'paperId': '5de8507ec6e986da6a6c662e869fc725ed1104cf',\n  'arxivId': '2104.02493',\n  'DOI': '10.5281/ZENODO.4559821',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'Boxy',\n  'href': 'https://boxy-dataset.com/boxy/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '200000',\n  'numberOfScenes': '34',\n  'samplingRate': '15',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x mvBlueFOX3-2051 with a Sony IMX250 chip 2464x2056 15Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'bounding box, polygon, and real-time detections',\n  'annotations': '2d boxes or 3d cuboids for vehicles',\n  'licensing': 'Freely available for non-commercial purposes',\n  'relatedDatasets': '-',\n  'publishDate': '2019-10-27',\n  'lastUpdate': '-',\n  'paperTitle': 'Boxy Vehicle Detection in Large Images',\n  'relatedPaper': 'https://boxy-dataset.com/static/boxy/boxy_preview.pdf',\n  'location': 'San Francisco Bay Area, California, USA',\n  'rawData': '-',\n  'DOI': '10.1109/ICCVW.2019.00112',\n  'citationCount': 13,\n  'completionStatus': 'complete',\n  'paperId': '2b07be24909169c1708fcde15a5c333eeab83a81',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'openDD',\n  'href': 'https://l3pilot.eu/data/opendd',\n  'size_storage': '-',\n  'size_hours': '62.7',\n  'frames': '6771600',\n  'numberOfScenes': '501',\n  'samplingRate': '30',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'DJI Phantom 4 38402160 camera drone',\n  'benchmark': 'trajectory predictions',\n  'annotations': '2d bounding boxes, trajectories',\n  'licensing': 'Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ',\n  'relatedDatasets': '-',\n  'publishData': '2020-09-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/2007.08463.pdf',\n  'location': 'Wolfsburg and Ingolstadt, Germany',\n  'rawData': 'Yes',\n  'citationCount': 10,\n  'completionStatus': 'partially Complete',\n  'paperId': 'de1bf2dc076813bb9924ebf2f019ed237305254d',\n  'arxivId': '2007.08463',\n  'DOI': '10.1109/ITSC45102.2020.9294301',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'KITTI-360 PanopticBEV',\n  'href': 'http://panoptic-bev.cs.uni-freiburg.de/',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9681287',\n  'DOI': '10.1109/LRA.2022.3142418',\n  'citationCount': 10,\n  'completionStatus': 'partially Complete',\n  'paperId': '0ec2528cc8b03c85d476bc1110a44edee6649198',\n  'arxivId': '2108.03227',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'KITTI-360 PanopticBEV',\n  'href': 'http://panoptic-bev.cs.uni-freiburg.de/',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9681287',\n  'DOI': '10.1109/LRA.2022.3142418',\n  'citationCount': 10,\n  'completionStatus': 'partially Complete',\n  'paperId': '0ec2528cc8b03c85d476bc1110a44edee6649198',\n  'arxivId': '2108.03227',\n  'altmetrics': ({'score': 6.6},\n   {'percentile': 80},\n   {'similar_age_3m_percentile': 71},\n   {'total_readers': 55})},\n {'id': 'Semantic KITTI',\n  'href': 'http://www.semantic-kitti.org/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '43552',\n  'numberOfScenes': '21',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar',\n  'sensorDetail': \"Velodyne HDL-64E from sequences of the odometry 'benchmark' of the KITTI Vision Benchmark with 360 view\",\n  'benchmark': 'semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion',\n  'annotations': 'semantic segmentation',\n  'licensing': 'Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ',\n  'relatedDatasets': 'KITTI',\n  'publishDate': '2019-07-01',\n  'lastUpdate': '2021-02-01',\n  'relatedPaper': 'https://arxiv.org/abs/1904.01416.pdf',\n  'location': 'Karlsruhe, Germany',\n  'rawData': 'No',\n  'citationCount': 9,\n  'completionStatus': 'partially Complete',\n  'paperId': 'ce8bf61670c8feca668dad17803fca7c64ee8f45',\n  'arxivId': '1904.01416',\n  'altmetrics': ({'score': 42.1},\n   {'percentile': 96},\n   {'similar_age_3m_percentile': 93},\n   {'total_readers': 461})},\n {'id': 'MCity Data Collection',\n  'href': 'https://arxiv.org/pdf/1912.06258.pdf',\n  'size_storage': '11000',\n  'size_hours': '50',\n  'frames': '-',\n  'numberOfScenes': '255',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, radar, gps/imu',\n  'sensorDetail': '3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30 1080P 30Hz,1x backward-facing camera 90 1080P 30Hz, 1x cabin pose camera 12801080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz',\n  'benchmark': '-',\n  'annotations': 'semantic segmentation of objects, traffic lights, traffic signs, lanes',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2019-12-01',\n  'lastUpdate': '-',\n  'relatedPaper': 'https://arxiv.org/pdf/1912.06258.pdf',\n  'location': 'Ann Arbor, USA',\n  'rawData': 'Yes',\n  'citationCount': 9,\n  'completionStatus': 'partially Complete',\n  'paperId': '02eb1b9cc14996713d1e653ada100ea14201b3e6',\n  'arxivId': '1912.06258',\n  'altmetrics': ({'score': 1.5},\n   {'percentile': 37},\n   {'similar_age_3m_percentile': 44},\n   {'total_readers': 16})},\n {'id': 'nuPlan',\n  'href': 'https://arxiv.org/abs/2106.11810',\n  'size_storage': '-',\n  'size_hours': '1500',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu',\n  'sensorDetail': '8x cameras 2000x1200 10Hz, 5x lidar 20Hz, 1x imu 100Hz, 1x gnss 20Hz',\n  'recordingPerspective': '-',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'autonomous vehicle planning',\n  'annotations': '2d/3d bounding boxes',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)',\n  'relatedDatasets': '-',\n  'publishDate': '2021-12-10',\n  'lastUpdate': '-',\n  'paperTitle': 'nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles',\n  'relatedPaper': 'https://arxiv.org/abs/2106.11810',\n  'location': 'Boston, Pittsburgh, Las Vegas and Singapore',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.2106.11810',\n  'citationCount': 8,\n  'completionStatus': 'complete',\n  'paperId': 'b88b38ec61a4881173ab94647d1e97500f4af15b',\n  'arxivId': '2106.11810',\n  'altmetrics': ({'score': 1.5},\n   {'percentile': 37},\n   {'similar_age_3m_percentile': 44},\n   {'total_readers': 16})},\n {'id': 'ROAD',\n  'href': 'https://github.com/gurkirt/road-dataset',\n  'size_hours': '2.83',\n  'size_storage': '-',\n  'frames': '122000',\n  'numberOfScenes': '22',\n  'samplingRate': '12',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'benchmark': 'agent detection, action detection and road event detection',\n  'annotations': '2d/3d bounding boxes, action label and location labels',\n  'licensing': 'Creative Commons Attribution Share Alike 4.0 International',\n  'relatedDatasets': 'Oxford Robot Car Dataset (OxRD)',\n  'publishDate': '-',\n  'lastUpdate': '-',\n  'paperTitle': 'ROAD: The ROad event Awareness Dataset for Autonomous Driving',\n  'relatedPaper': 'https://www.computer.org/csdl/api/v1/periodical/trans/tp/5555/01/09712346/1AZL0P4dL1e/download-article/pdf',\n  'location': 'Oxford, UK',\n  'rawData': '-',\n  'DOI': '10.1109/TPAMI.2022.3150906',\n  'citationCount': 6,\n  'completionStatus': 'complete',\n  'paperId': '65e40b0c9327b9e1cfe30cfec61a8708b334b8eb',\n  'arxivId': '2102.11585',\n  'altmetrics': ({'score': 1.5},\n   {'percentile': 37},\n   {'similar_age_3m_percentile': 44},\n   {'total_readers': 16})},\n {'id': 'TRoM',\n  'href': 'http://www.tromai.icoc.me/',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '712',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, gps',\n  'sensorDetail': '1x PointGray color camera 1280x960',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'Road marking detection',\n  'annotations': 'Pixel level road markings',\n  'licensing': 'Freely available for academic use',\n  'relatedDatasets': '-',\n  'publishDate': '2017-10-16',\n  'lastUpdate': '-',\n  'paperTitle': 'Benchmark for road marking detection: Dataset specification and performance baseline',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/8317749',\n  'location': 'Beijing, China',\n  'rawData': '-',\n  'DOI': '10.1109/ITSC.2017.8317749',\n  'citationCount': 6,\n  'completionStatus': 'complete',\n  'paperId': 'cee8ad964ff20dd5ff6c10c783ee271a7222987e',\n  'altmetrics': ({'score': 1.5},\n   {'percentile': 37},\n   {'similar_age_3m_percentile': 44},\n   {'total_readers': 16})},\n {'id': 'Small Obstacle',\n  'href': 'https://small-obstacle-dataset.github.io/',\n  'size_storage': '10.6',\n  'size_hours': '-',\n  'frames': '2927',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar',\n  'sensorDetail': '1x ZED Stereo camera, 1x Velodyne Puck (VLP-16) lidar',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real and Synthetic',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': 'pixel wise semantic segmentation',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2020-03-12',\n  'lastUpdate': '-',\n  'paperTitle': 'LiDAR guided Small obstacle Segmentation',\n  'relatedPaper': 'https://arxiv.org/pdf/2003.05970.pdf',\n  'location': 'India',\n  'rawData': '-',\n  'DOI': '10.1109/IROS45743.2020.9341465',\n  'citationCount': 6,\n  'completionStatus': 'complete',\n  'paperId': 'ebb5d9db9152ce81671143c35c5d6ce58b23255e',\n  'arxivId': '2003.05970',\n  'altmetrics': ({'score': 1.5},\n   {'percentile': 37},\n   {'similar_age_3m_percentile': 44},\n   {'total_readers': 16})},\n {'id': 'PepScenes',\n  'href': 'https://github.com/huawei-noah/PePScenes',\n  'relatedPaper': 'https://arxiv.org/pdf/2012.07773.pdf',\n  'citationCount': 5,\n  'completionStatus': 'partially Complete',\n  'paperId': '0980ac67119bcaec6828511e38a02f051b6987fe',\n  'arxivId': '2012.07773',\n  'altmetrics': ({'score': 1.25},\n   {'percentile': 33},\n   {'similar_age_3m_percentile': 40},\n   {'total_readers': 17})},\n {'id': 'The USyd Campus Dataset',\n  'href': 'http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/',\n  'size_storage': '-',\n  'size_hours': '40',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps, imu, wheel encoders',\n  'sensorDetail': '6x NVIDIA 2Mega SF3322 automotive GMSL cameras 1928x1208 30Hz , 1x 3D Velodyne Puck VLP-16 10 Hz 360, 1x VN-100 IMU, 1x U-Blox NEO-M8P real-time kinematics GNSS',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'Semantic Segmentation, Road class label',\n  'licensing': 'Freely available for non-commercial purposes',\n  'relatedDatasets': 'Five Roundabouts Dataset,  Naturalistic Intersection Driving Dataset, High Resolution Fused GPS and Dead Reckoning',\n  'publishDate': '2020-06-05',\n  'lastUpdate': '-',\n  'paperTitle': 'Developing and Testing Robust Autonomy: The University of Sydney Campus Data Set',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9109704',\n  'location': 'Sydney, Australia',\n  'rawData': '-',\n  'DOI': '10.1109/MITS.2020.2990183',\n  'citationCount': 4,\n  'completionStatus': 'complete',\n  'paperId': '77502b6f556c6c88a76e4541749de15d3ae502c6',\n  'altmetrics': ({'score': 1.25},\n   {'percentile': 33},\n   {'similar_age_3m_percentile': 40},\n   {'total_readers': 17})},\n {'id': 'A9',\n  'href': 'https://innovation-mobility.com/en/a9-dataset/',\n  'relatedPaper': 'https://arxiv.org/pdf/2204.06527.pdf',\n  'citationCount': 4,\n  'completionStatus': 'partially Complete',\n  'paperId': '0dcd5e0c87835ad05d241935ff5a5c05fcbc5dc2',\n  'arxivId': '2204.06527',\n  'DOI': '10.48550/arXiv.2204.06527',\n  'altmetrics': ({'score': 1.25},\n   {'percentile': 33},\n   {'similar_age_3m_percentile': 40},\n   {'total_readers': 17})},\n {'id': 'KITTI-360-APS',\n  'href': 'http://amodal-panoptic.cs.uni-freiburg.de/',\n  'relatedPaper': 'https://arxiv.org/pdf/2202.11542.pdf',\n  'citationCount': 4,\n  'completionStatus': 'partially Complete',\n  'paperId': 'a61e541fc8d3c09ffb103e385fb967c47e50b359',\n  'arxivId': '2202.11542',\n  'altmetrics': ({'score': 0.75},\n   {'percentile': 19},\n   {'similar_age_3m_percentile': 28},\n   {'total_readers': 24})},\n {'id': 'BDD100K-APS',\n  'href': 'http://amodal-panoptic.cs.uni-freiburg.de/',\n  'relatedPaper': 'https://arxiv.org/pdf/2202.11542.pdf',\n  'citationCount': 4,\n  'completionStatus': 'partially Complete',\n  'paperId': 'a61e541fc8d3c09ffb103e385fb967c47e50b359',\n  'arxivId': '2202.11542',\n  'altmetrics': ({'score': 0.75},\n   {'percentile': 19},\n   {'similar_age_3m_percentile': 28},\n   {'total_readers': 24})},\n {'id': 'Boreas',\n  'href': 'https://www.boreas.utias.utoronto.ca/#/',\n  'relatedPaper': 'https://arxiv.org/pdf/2203.10168.pdf',\n  'citationCount': 4,\n  'completionStatus': 'partially Complete',\n  'paperId': '6d33fbbaf490c5118748201b7f8f506aac84d5e7',\n  'arxivId': '2203.10168',\n  'DOI': '10.48550/arXiv.2203.10168',\n  'altmetrics': ({'score': 0.75},\n   {'percentile': 19},\n   {'similar_age_3m_percentile': 28},\n   {'total_readers': 24})},\n {'id': 'The Autonomous Platform Inertial Dataset',\n  'href': 'https://github.com/ansfl/Navigation-Data-Project/',\n  'relatedPaper': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9684368',\n  'DOI': '10.1109/access.2022.3144076',\n  'citationCount': 4,\n  'completionStatus': 'partially Complete',\n  'paperId': '52d7ef6b0ca31c6da90da2645781dd8332536394',\n  'altmetrics': ({'score': 0.75},\n   {'percentile': 19},\n   {'similar_age_3m_percentile': 28},\n   {'total_readers': 24})},\n {'id': 'CODA',\n  'href': 'https://coda-dataset.github.io/',\n  'relatedPaper': 'https://arxiv.org/pdf/2203.07724.pdf',\n  'benchmark': 'anomaly detection',\n  'citationCount': 3,\n  'completionStatus': 'partially Complete',\n  'paperId': '9572ba631de7be2aca9fafc79eaf063ba2e2dbbc',\n  'arxivId': '2203.07724',\n  'DOI': '10.48550/arXiv.2203.07724',\n  'altmetrics': ({'score': 0.75},\n   {'percentile': 19},\n   {'similar_age_3m_percentile': 28},\n   {'total_readers': 24})},\n {'id': 'SODA10M',\n  'href': 'https://soda-2d.github.io/',\n  'relatedPaper': 'https://arxiv.org/pdf/2106.11118.pdf',\n  'citationCount': 3,\n  'completionStatus': 'partially Complete',\n  'paperId': '99a25cf80bcc1b0d8c89bf458c376f3e0f5a1298',\n  'arxivId': '2106.11118',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'Cooperative Driving Dataset (CODD)',\n  'href': 'https://github.com/eduardohenriquearnold/CODD',\n  'size_hours': '-',\n  'size_storage': '16.7',\n  'frames': '13500',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'lidar',\n  'sensorDetail': '-',\n  'recordingPerspective': 'Top Shot',\n  'dataType': 'Real',\n  'benchmark': '-',\n  'annotations': '3d bounding boxes',\n  'licensing': 'Creative Commons Attribution Share Alike 4.0 International',\n  'relatedDatasets': '-',\n  'publishDate': '2021-11-23',\n  'lastUpdate': '-',\n  'paperTitle': 'Fast and Robust Registration of Partially Overlapping Point Clouds',\n  'relatedPaper': 'https://arxiv.org/pdf/2112.09922.pdf',\n  'location': 'CARLA environment',\n  'rawData': '-',\n  'DOI': '10.1109/lra.2021.3137888',\n  'citationCount': 2,\n  'completionStatus': 'complete',\n  'paperId': '0d8a19424f81cda87814ea12500da6a80c270fea',\n  'arxivId': '2112.09922',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'SHIFT',\n  'href': 'https://www.vis.xyz/shift/',\n  'relatedPaper': 'https://arxiv.org/abs/2206.08367',\n  'citationCount': 2,\n  'completionStatus': 'partially Complete',\n  'paperId': 'b9a93ff7a2e69f77520015d59f1c0e365f5ca526',\n  'arxivId': '2206.08367',\n  'DOI': '10.48550/arXiv.2206.08367',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'DGL-MOTS',\n  'href': 'https://goodproj13.github.io/DGL-MOTS/',\n  'relatedPaper': 'https://arxiv.org/pdf/2110.07790.pdf',\n  'citationCount': 2,\n  'completionStatus': 'partially Complete',\n  'paperId': '72f69bbe7976e715d2640a1c2cd4804ca7f0a635',\n  'arxivId': '2110.07790',\n  'DOI': '10.1109/WACV51458.2022.00347',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'DRIV100',\n  'href': 'https://zenodo.org/record/4389243#.YnvlruhBxD8',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '-',\n  'numberOfScenes': '100',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'No',\n  'benchmark': 'Domain adaptation techniques on in-the-wild road-scene videos collected from the Internet',\n  'annotations': 'pixel level semantic segmentation',\n  'licensing': 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License',\n  'relatedDatasets': '-',\n  'publishDate': '2021-01-30',\n  'lastUpdate': '-',\n  'paperTitle': 'DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation',\n  'relatedPaper': 'https://arxiv.org/pdf/2102.00150.pdf',\n  'location': 'Random videos from YouTube',\n  'rawData': '-',\n  'DOI': '10.5281/zenodo.4389243',\n  'citationCount': 1,\n  'completionStatus': 'complete',\n  'paperId': '3e6513e32ad445d2a8275f44ec02cecace2ae9e4',\n  'arxivId': '2102.00150',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'MIT-AVT Clustered Driving Scene Dataset',\n  'href': 'https://ieeexplore.ieee.org/abstract/document/9304677/',\n  'size_storage': '4000',\n  'size_hours': '3212',\n  'frames': '-',\n  'numberOfScenes': '1156592',\n  'samplingRate': '30',\n  'lengthOfScenes': '10',\n  'sensors': 'camera, imu, gps',\n  'sensorDetail': '-',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': 'Seasons, Weather, Lanes, Illuminnation, Simplified Road Type, Others',\n  'licensing': 'Not released publically',\n  'relatedDatasets': '-',\n  'publishDate': '2020-11-13',\n  'lastUpdate': '-',\n  'paperTitle': 'MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios',\n  'relatedPaper': 'https://ieeexplore.ieee.org/abstract/document/9304677/',\n  'location': 'Many states across the USA',\n  'rawData': 'Yes',\n  'DOI': '10.1109/IV47402.2020.9304677',\n  'citationCount': 1,\n  'completionStatus': 'complete',\n  'paperId': 'ed3b6e824b22be5df01148b0ad04c8e40e6b3bce',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'Amodal Cityscapes',\n  'href': 'https://github.com/ifnspaml/AmodalCityscapes',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '3472',\n  'numberOfScenes': '-',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': 'Dataset taken from Cityscapes and occlusions pasted artficially for amodal segmentation tasks',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': '-',\n  'mapData': 'No',\n  'benchmark': 'amodal semantic segmentation method',\n  'annotations': 'semantic segmentation and amodal semantic segmentation',\n  'licensing': 'Freely available code to generate the dataset',\n  'relatedDatasets': '-',\n  'publishDate': '2022-06-01',\n  'lastUpdate': '-',\n  'paperTitle': 'Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline',\n  'relatedPaper': 'https://arxiv.org/pdf/2206.00527.pdf',\n  'location': '-',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.2206.00527',\n  'citationCount': 1,\n  'completionStatus': 'complete',\n  'paperId': 'a492e402574c18a41888de96ce64f50060d42de7',\n  'arxivId': '2206.00527',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'ScribbleKITTI',\n  'href': 'https://github.com/ouenal/scribblekitti',\n  'relatedPaper': 'https://arxiv.org/abs/2203.08537',\n  'citationCount': 1,\n  'completionStatus': 'partially Complete',\n  'paperId': 'eda7539707295872bfc025877a77b2d330f4bf91',\n  'arxivId': '2203.08537',\n  'DOI': '10.48550/arXiv.2203.08537',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'GROUNDED',\n  'href': 'https://lgprdata.com/',\n  'size_storage': '750',\n  'size_hours': '12',\n  'frames': '-',\n  'numberOfScenes': '108',\n  'samplingRate': '-',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, lidar, gps/imu, odometer, radar',\n  'sensorDetail': '1x Point Grey Grasshopper camera 1928x1448 6Hz, 1x Velodyne HDL-64 lidar 360 10Hz, 1x OXTS RT3003 INS gps, 1x Localizing Ground Penetrating Radar (LGPR) Sensor components include a 12 element radar array, a switch matrix, an OXTS RTK-GPS unit 126Hz',\n  'recordingPerspective': 'ego-perspective',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': 'Localization in weather, Multi-lane mapping',\n  'annotations': '-',\n  'licensing': '-',\n  'relatedDatasets': '-',\n  'publishDate': '2021-06-12',\n  'lastUpdate': '-',\n  'paperTitle': 'GROUNDED: The Localizing Ground Penetrating Radar Evaluation Dataset',\n  'relatedPaper': 'http://www.roboticsproceedings.org/rss17/p080.pdf',\n  'location': 'Massachusetts, USA',\n  'rawData': '-',\n  'DOI': '10.15607/RSS.2021.XVII.080',\n  'citationCount': 1,\n  'completionStatus': 'complete',\n  'paperId': '69eb71c3b306aa7e8494b88cd96491aaf01acf33',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'OpenMPD',\n  'href': 'http://openmpd.com/',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9682587',\n  'DOI': '10.1109/tvt.2022.3143173',\n  'citationCount': 1,\n  'completionStatus': 'partially Complete',\n  'paperId': '4e60703e33c7d358a39d9a4afb48d64bf6bbea0c',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'Gated2Gated',\n  'href': 'https://github.com/princeton-computational-imaging/Gated2Gated#gated2gated--self-supervised-depth-estimation-from-gated-images',\n  'size_storage': '-',\n  'size_hours': '-',\n  'frames': '130000',\n  'numberOfScenes': '1835',\n  'samplingRate': '10',\n  'lengthOfScenes': '-',\n  'sensors': 'camera, radar, lidar, imu, weather sensor',\n  'sensorDetail': '2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)',\n  'recordingPerspective': 'Ego-perspective',\n  'dataType': 'real',\n  'mapData': 'No',\n  'benchmark': '-',\n  'annotations': '-',\n  'licensing': 'Freely available for research and teaching purposes',\n  'relatedDatasets': 'SeeingThroughFog, PointCloudDeNoising, Gated2Depth',\n  'publishDate': '2021-12-04',\n  'lastUpdate': '-',\n  'paperTitle': 'Gated2Gated: Self-Supervised Depth Estimation from Gated Images',\n  'relatedPaper': 'https://arxiv.org/pdf/2112.02416.pdf',\n  'location': 'Germany, Sweden, Denmark and Finland',\n  'rawData': '-',\n  'DOI': '10.48550/arXiv.2112.02416',\n  'citationCount': 1,\n  'completionStatus': 'complete',\n  'paperId': 'de2e439641d968448bace28f4c347ae8344be938',\n  'arxivId': '2112.02416',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'CarlaScenes',\n  'href': 'https://github.com/CarlaScenes/CarlaSence',\n  'relatedPaper': 'https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.pdf',\n  'DOI': '10.1109/OJITS.2022.3142612',\n  'citationCount': 0,\n  'completionStatus': 'partially Complete',\n  'paperId': 'a2f805e8cec9799952703de162b01fcef319d2e9',\n  'altmetrics': ({'score': 2.5},\n   {'percentile': 42},\n   {'similar_age_3m_percentile': 56},\n   {'total_readers': 11})},\n {'id': 'CrashD',\n  'href': 'https://crashd-cars.github.io/',\n  'relatedPaper': 'https://arxiv.org/abs/2112.04764',\n  'citationCount': 0,\n  'completionStatus': 'partially Complete',\n  'paperId': '12d970641c712ba99eff92820f2829cf02e93070',\n  'arxivId': '2112.04764',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'exiD',\n  'href': 'https://www.exid-dataset.com/',\n  'size_storage': '-',\n  'size_hours': '16.1',\n  'frames': '-',\n  'numberOfScenes': '-',\n  'samplingRate': '25',\n  'lengthOfScenes': '-',\n  'sensors': 'camera',\n  'sensorDetail': '1x DJI Matrice 200 Serie V2 equipped with a DJI Zenmuse gimbal camera 4096x2160 25Hz',\n  'recordingPerspective': 'top-view',\n  'dataType': 'Real',\n  'mapData': 'Yes',\n  'benchmark': '-',\n  'annotations': '2d bounding boxes for objects, trajectory tracking information',\n  'licensing': 'Freely available for non-commercial use only upon registration',\n  'relatedDatasets': 'highD, inD, rounD and uniD datasets',\n  'publishDate': '2022-06-04',\n  'lastUpdate': '-',\n  'paperTitle': 'The exiD Dataset: A Real-World Trajectory Dataset of Highly Interactive Highway Scenarios in Germany',\n  'relatedPaper': 'https://ieeexplore.ieee.org/document/9827305',\n  'location': 'Germany',\n  'rawData': '-',\n  'DOI': '10.1109/iv51971.2022.9827305',\n  'citationCount': 0,\n  'completionStatus': 'incomplete',\n  'paperId': 'c7901a519fd19f483b85b5483cdd31c67c93f973',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'K-Radar',\n  'href': 'https://github.com/kaist-avelab/K-Radar',\n  'relatedPaper': 'https://arxiv.org/abs/2206.08171',\n  'citationCount': 0,\n  'completionStatus': 'partially Complete',\n  'paperId': '9a6f72226f6cb0e8b68d6108b06ede5bd43cbd01',\n  'arxivId': '2206.08171',\n  'DOI': '10.48550/arXiv.2206.08171',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'R3 Driving Dataset',\n  'href': 'https://github.com/rllab-snu/R3-Driving-Dataset',\n  'relatedPaper': 'https://arxiv.org/pdf/2109.07995.pdf',\n  'citationCount': 0,\n  'completionStatus': 'partially Complete',\n  'paperId': '400da52f90ddfc2bdc1b356f4438d585a4c66545',\n  'arxivId': '2109.07995'},\n {'id': 'Rope3D',\n  'href': 'https://thudair.baai.ac.cn/rope',\n  'relatedPaper': 'https://arxiv.org/pdf/2203.13608.pdf',\n  'citationCount': 0,\n  'completionStatus': 'partially Complete',\n  'paperId': '566fe8e049ba041db3fa7ff7b09fd024e587131a',\n  'arxivId': '2203.13608',\n  'DOI': '10.48550/arXiv.2203.13608',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'AugKITTI',\n  'relatedPaper': 'https://arxiv.org/pdf/2203.00214.pdf',\n  'citationCount': 0,\n  'completionStatus': 'incomplete',\n  'paperId': '0d0f6396ba436f2e1a4fd8bfa7a562a35b058fca',\n  'arxivId': '2203.00214',\n  'DOI': '10.48550/arXiv.2203.00214',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})},\n {'id': 'On Salience-Sensitive Sign Classification in Autonomous Vehicle Path Planning',\n  'relatedPaper': 'https://arxiv.org/pdf/2112.00942.pdf',\n  'citationCount': 0,\n  'completionStatus': 'incomplete',\n  'paperId': 'b53e2f44381f72286f1f0b3416de8f763fd26efd',\n  'arxivId': '2112.00942',\n  'DOI': '10.1109/WACVW54805.2022.00070',\n  'altmetrics': ({'score': 9.33},\n   {'percentile': 86},\n   {'similar_age_3m_percentile': 81},\n   {'total_readers': 22})}]"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}