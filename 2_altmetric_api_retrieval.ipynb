{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "{'total_number_of_other_articles': 22545203,\n 'mean': 10.140114810243793,\n 'rank': 2379437,\n 'this_scored_higher_than_pct': 89,\n 'this_scored_higher_than': 20154515,\n 'rank_type': 'exact',\n 'sample_size': 22545203,\n 'percentile': 89}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load your api key\n",
    "with open('api_keys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "alt_metric_key = api_keys['alt_metric']\n",
    "def get_reduced_alt_values(r):\n",
    "    \"\"\"when some info is missing extracts less information from the response object\"\"\"\n",
    "    total_readers = 0\n",
    "    for key in r['counts']['readers'].keys():\n",
    "        total_readers += int(r['counts']['readers'][key])\n",
    "    score = r['score']\n",
    "    percentile = -100 #r['altmetric_score']['context_for_score']['all']['percentile']\n",
    "    similar_age_3m_percentile= -100 #r['altmetric_score']['context_for_score']['similar_age_3m']  #['percentile']\n",
    "    alt_values = {'score': score}, {'percentile': percentile}, {'similar_age_3m_percentile': similar_age_3m_percentile}, {'total_readers': total_readers}\n",
    "    return alt_values\n",
    "def get_alt_values(r):\n",
    "    \"\"\"extracts relevant information from the response object\"\"\"\n",
    "    total_readers = 0\n",
    "    for key in r['counts']['readers'].keys():\n",
    "        total_readers += int(r['counts']['readers'][key])\n",
    "    score = r['score']\n",
    "    percentile = 0 #r['altmetric_score']['context_for_score']['all']['percentile']\n",
    "    similar_age_3m_percentile= r['altmetric_score']['context_for_score']['similar_age_3m']  #['percentile']\n",
    "    alt_values = {'score': score}, {'percentile': percentile}, {'similar_age_3m_percentile': similar_age_3m_percentile}, {'total_readers': total_readers}\n",
    "    return alt_values\n",
    "def altmetric_arxiv_requests(arxiv):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/arxiv_id/{arxiv}?key={alt_metric_key}')\n",
    "    if r.status_code != 200:\n",
    "        print('Warning - Response Code ', r.status_code)\n",
    "    return r.json()\n",
    "def altmetric_doi_requests(doi):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/doi/{doi}?key={alt_metric_key}')\n",
    "    if r.status_code != 200:\n",
    "        print('Warning - Response Code ', r.status_code, 'DOI', doi)\n",
    "    return r.json()\n",
    "altmetric_doi_requests('10.1177%2F0278364913491297')['altmetric_score']['context_for_score']['all']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datasets: 186\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: GTSDB caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Malaga Stereo and Laser Urban caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Stanford Track Collection caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Complex Urban caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: UAH-DriveSet caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: HD1K caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: rounD caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: DriveU Traffic Light caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: PREVENTION caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Boxy caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: TRoM caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: WildDash 2 caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: LUMPI caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: CarlaScenes caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: RoadSaW caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: VLMV - Vehicle Lane Merge Visual Benchmark caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: GROUNDED caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: exiD caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: K-Radar caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: R3 Driving caused: Expecting value: line 1 column 1 (char 0)\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: MuIRan caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Boreas caused: Expecting value: line 1 column 1 (char 0)\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: OpenMPD caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: Autonomous Platform Inertial caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: HDBD caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Response Code  404\n",
      "Warning - Response Code  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_16296\\766350209.py:19: UserWarning: RUGD: Robot Unstructured Ground Driving caused: 'arxivId'\n",
      "  warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found: 159 / 186\n"
     ]
    }
   ],
   "source": [
    "suffix = '_22_12_2022'\n",
    "file_name = f'data/data_sorted_only_w_ids{suffix}.json'\n",
    "with open(file_name, \"r\") as ds:\n",
    "    retrieved_data = json.load(ds)\n",
    "print('Number of Datasets:', len(retrieved_data))\n",
    "c = 0\n",
    "c_doi = 0\n",
    "for idx_paper, paper in enumerate(retrieved_data):\n",
    "    c += 1\n",
    "    try:\n",
    "        doi = paper['DOI']\n",
    "        r = altmetric_doi_requests(doi)\n",
    "        c_doi += 1\n",
    "    except:\n",
    "        try:\n",
    "            r = altmetric_arxiv_requests(paper['arxivId'])\n",
    "            c_doi += 1\n",
    "        except Exception as ex:\n",
    "            warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
    "        continue\n",
    "    try:\n",
    "        alt_values = get_alt_values(r)\n",
    "    except:\n",
    "        try:\n",
    "            alt_values = get_reduced_alt_values(r)\n",
    "        except Exception as exec:\n",
    "            print(exec)\n",
    "            c_doi -= 1\n",
    "    retrieved_data[idx_paper].update({'altmetrics': alt_values})\n",
    "'''with open(f'data/data_sorted_only_w_ids{suffix}_altmetrics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieved_data, f)'''\n",
    "print('found:', c_doi, '/', c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_counter = 0\n",
    "for data in retrieved_data:\n",
    "    try:\n",
    "        perc = data['altmetrics']['similar_age_3m_percentile']\n",
    "        if perc == 0:\n",
    "            zero_counter += 1\n",
    "    except:\n",
    "        continue\n",
    "zero_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "{'altmetric_id': 3801312,\n 'counts': {'readers': {'citeulike': '0', 'mendeley': '2001', 'connotea': '0'},\n  'total': {'posts_count': 60},\n  'twitter': {'unique_users_count': 1, 'posts_count': 11},\n  'patent': {'unique_users_count': 49,\n   'unique_users': ['patent:US-10019801-B2',\n    'patent:US-10328769-B2',\n    'patent:US-10331124-B2',\n    'patent:US-10354406-B2',\n    'patent:US-10380889-B2',\n    'patent:US-10469828-B2',\n    'patent:US-10507787-B2',\n    'patent:US-10520593-B2',\n    'patent:US-10599156-B2',\n    'patent:US-10705193-B2',\n    'patent:WO-2020142328-A1',\n    'patent:US-10824862-B2',\n    'patent:US-10859682-B2',\n    'patent:WO-2020247075-A1',\n    'patent:US-10867210-B2',\n    'patent:US-20210004566-A1',\n    'patent:US-10929996-B2',\n    'patent:US-10949989-B2',\n    'patent:US-10970871-B2',\n    'patent:US-10977501-B2',\n    'patent:US-11009868-B2',\n    'patent:US-11037051-B2',\n    'patent:FR-3104292-A1',\n    'patent:US-11061116-B2',\n    'patent:US-11094072-B2',\n    'patent:US-11107230-B2',\n    'patent:US-11138751-B2',\n    'patent:US-11145074-B2',\n    'patent:US-11164012-B2',\n    'patent:US-11176709-B2',\n    'patent:US-11210802-B2',\n    'patent:US-11250489-B2',\n    'patent:US-11294035-B2',\n    'patent:US-11295517-B2',\n    'patent:US-11321863-B2',\n    'patent:EP-4002215-A1',\n    'patent:US-11361187-B1',\n    'patent:WO-2022117199-A1',\n    'patent:US-11370423-B2',\n    'patent:US-11386567-B2',\n    'patent:US-11403776-B2',\n    'patent:US-11423255-B2',\n    'patent:US-11422265-B2',\n    'patent:US-11436743-B2',\n    'patent:US-11450120-B2',\n    'patent:US-11449050-B2',\n    'patent:WO-2022201212-A1',\n    'patent:US-11467574-B2',\n    'patent:US-11481912-B2'],\n   'posts_count': 49}},\n 'citation': {'altmetric_jid': '4f6fa4ed3cf058f610002b9d',\n  'authors': ['A Geiger', 'P Lenz', 'C Stiller', 'R Urtasun'],\n  'doi': '10.1177/0278364913491297',\n  'endpage': '1237',\n  'first_seen_on': '2015-03-18T02:00:36+00:00',\n  'issns': ['0278-3649', '1741-3176'],\n  'issue': '11',\n  'journal': 'The International Journal of Robotics Research',\n  'last_mentioned_on': 1666656000,\n  'links': ['http://ijr.sagepub.com/content/32/11/1231.short?rss=1&ssource=mfr',\n   'https://doi.org/10.1177/0278364913491297',\n   'https://journals.sagepub.com/doi/full/10.1177/0278364913491297?utm_source=SAGE_social&utm_medium=&utm_campaign=&hootPostID=6c7c7b7f830cf734882ae2e7ea804321'],\n  'pdf_url': 'http://ijr.sagepub.com/content/32/11/1231.full.pdf',\n  'pubdate': '2013-09-01T00:00:00+00:00',\n  'epubdate': '2013-08-23T00:00:00+00:00',\n  'published_on': '2013-08-23T00:00:00+00:00',\n  'publisher_subjects': [{'name': 'Artificial Intelligence And Image Processing',\n    'scheme': 'era'},\n   {'name': 'Electrical And Electronic Engineering', 'scheme': 'era'},\n   {'name': 'Mechanical Engineering', 'scheme': 'era'}],\n  'scopus_subjects': ['Mathematics',\n   'Physical Sciences',\n   'Computer Science',\n   'Engineering'],\n  'startpage': '1231',\n  'title': 'Vision meets robotics: The KITTI dataset',\n  'type': 'article',\n  'volume': '32',\n  'mendeley_url': 'https://www.mendeley.com/catalogue/20d7e081-1dff-3dcc-aafb-8452f935fda7/'},\n 'altmetric_score': {'score': 12.25,\n  'score_history': {'1y': 9,\n   '6m': 6,\n   '3m': 6,\n   '1m': 0,\n   '1w': 0,\n   '6d': 0,\n   '5d': 0,\n   '4d': 0,\n   '3d': 0,\n   '2d': 0,\n   '1d': 0,\n   'at': 12.25},\n  'context_for_score': {'all': {'total_number_of_other_articles': 22545203,\n    'mean': 10.140114810243793,\n    'rank': 2379437,\n    'this_scored_higher_than_pct': 89,\n    'this_scored_higher_than': 20154515,\n    'rank_type': 'exact',\n    'sample_size': 22545203,\n    'percentile': 89},\n   'similar_age_3m': {'total_number_of_other_articles': 178884,\n    'mean': 9.178730786431505,\n    'rank': 20222,\n    'this_scored_higher_than_pct': 88,\n    'this_scored_higher_than': 158548,\n    'rank_type': 'exact',\n    'sample_size': 178884,\n    'percentile': 88},\n   'this_journal': {'total_number_of_other_articles': 915,\n    'mean': 4.978314754098361,\n    'rank': 54,\n    'this_scored_higher_than_pct': 93,\n    'this_scored_higher_than': 860,\n    'rank_type': 'exact',\n    'sample_size': 915,\n    'percentile': 93},\n   'similar_age_this_journal_3m': {'total_number_of_other_articles': 15,\n    'mean': 5.1353333333333335,\n    'rank': 2,\n    'this_scored_higher_than_pct': 86,\n    'this_scored_higher_than': 13,\n    'rank_type': 'exact',\n    'sample_size': 15,\n    'percentile': 86}}},\n 'demographics': {'poster_types': {'member_of_the_public': 1},\n  'users': {'twitter': {'cohorts': {'Members of the public': 1}},\n   'mendeley': {'by_status': {'Student  > Postgraduate': 46,\n     'Professor > Associate Professor': 20,\n     'Researcher': 176,\n     'Student  > Master': 492,\n     'Student  > Ph. D. Student': 466,\n     'Professor': 19,\n     'Student  > Bachelor': 159,\n     'Student  > Doctoral Student': 70,\n     'Lecturer': 18,\n     'Other': 40,\n     'Librarian': 3,\n     'Lecturer > Senior Lecturer': 5,\n     'Unspecified': 22},\n    'by_discipline': {'Materials Science': 2,\n     'Medicine and Dentistry': 3,\n     'Social Sciences': 2,\n     'Decision Sciences': 1,\n     'Physics and Astronomy': 12,\n     'Psychology': 8,\n     'Mathematics': 10,\n     'Pharmacology, Toxicology and Pharmaceutical Science': 1,\n     'Environmental Science': 2,\n     'Unspecified': 23,\n     'Chemical Engineering': 4,\n     'Arts and Humanities': 4,\n     'Design': 2,\n     'Engineering': 619,\n     'Chemistry': 5,\n     'Neuroscience': 5,\n     'Energy': 1,\n     'Earth and Planetary Sciences': 6,\n     'Economics, Econometrics and Finance': 3,\n     'Computer Science': 762,\n     'Agricultural and Biological Sciences': 8,\n     'Nursing and Health Professions': 1,\n     'Business, Management and Accounting': 3}}},\n  'geo': {'mendeley': {'HR': 1,\n    'US': 3,\n    'JP': 2,\n    'ES': 4,\n    'CN': 1,\n    'DK': 1,\n    'MX': 1,\n    'SG': 1,\n    'CA': 1,\n    'GB': 3,\n    'CZ': 2,\n    'FI': 2,\n    'IL': 1,\n    'SE': 1,\n    'BR': 1,\n    'AU': 2,\n    'AT': 1,\n    'KR': 2,\n    'FR': 6,\n    'NL': 1,\n    'DE': 2,\n    'PT': 1}}},\n 'posts': {'twitter': [{'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '578012977690898432'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '599922342576328705'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '600752815833677825'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '606636585245736960'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '615686216814477312'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '617040138292432896'},\n   {'license': 'datasift',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '618907442462613504'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '637802069261348864'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '639880678063017984'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '648195444321841153'},\n   {'license': 'gnip',\n    'citation_ids': [3801312],\n    'author': {'tweeter_id': '1480845025'},\n    'tweet_id': '666274602956386305'}],\n  'patent': [{'title': 'Image analysis system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-10019801-B2',\n    'license': 'public',\n    'citation_ids': [49782009,\n     35641125,\n     12902049,\n     12906237,\n     49782035,\n     3801312,\n     49782029,\n     49782026,\n     49782022],\n    'posted_on': '2018-07-10T00:00:00+00:00',\n    'summary': 'A method of determining a camera movement expressed in terms of an absolute transformation of a camera state from a reference state, including receiving a stream of video images captured from a camera; producing a stream of odometry measurements from the v',\n    'ucid': 'US-10019801-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Methods for interacting with autonomous or semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-10328769-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a system for precise delivery of an item to a consumer by an autonomous or semi-autonomous vehicle. The system performs image recognition algorithms to detect a primary entrance, navigate from a street address to the primary entrance, an',\n    'ucid': 'US-10328769-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Autonomous vehicle repositioning',\n    'url': 'https://app.dimensions.ai/details/patent/US-10331124-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-06-25T00:00:00+00:00',\n    'summary': 'Provided herein is a platform for distributing and navigating an autonomous or semi-autonomous fleet throughout a plurality of pathways. The platform may employ demand distribution prediction algorithms, and interim repositioning algorithms to distribute t',\n    'ucid': 'US-10331124-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Method of detecting objects within a 3D environment',\n    'url': 'https://app.dimensions.ai/details/patent/US-10354406-B2',\n    'license': 'public',\n    'citation_ids': [41865941, 43004858, 35312133, 3801312, 15431844],\n    'posted_on': '2019-07-16T00:00:00+00:00',\n    'summary': 'A method and system for detecting objects within a three-dimensional (3D) environment, comprising obtaining a 3D point-cloud representation of the environment, the point-cloud comprising a set of point locations, and converting the point-cloud to a 3D feat',\n    'ucid': 'US-10354406-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Determining car positions',\n    'url': 'https://app.dimensions.ai/details/patent/US-10380889-B2',\n    'license': 'public',\n    'citation_ids': [35433612, 3801312, 81094544, 80495985, 267002],\n    'posted_on': '2019-08-13T00:00:00+00:00',\n    'summary': 'Examples provided herein describe a method for determining car positions. For example, a physical processor of an edge computing device may receive position data for a legacy car and information about a make and model of the legacy car. The first edge devi',\n    'ucid': 'US-10380889-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional dense structure from motion with stereo vision',\n    'url': 'https://app.dimensions.ai/details/patent/US-10469828-B2',\n    'license': 'public',\n    'citation_ids': [81181891, 35182124, 3801312, 24853916, 12059625],\n    'posted_on': '2019-11-05T00:00:00+00:00',\n    'summary': 'Disclosed examples include three-dimensional imaging systems and methods to reconstruct a three-dimensional scene from first and second image data sets obtained from a single camera at first and second times, including computing feature point correspondenc',\n    'ucid': 'US-10469828-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'System and mechanism for upselling products on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-10507787-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2019-12-17T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles for containing, securing, and delivering at least one of a first item and at least one of a second item after a customer plac',\n    'ucid': 'US-10507787-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Installation and use of vehicle light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10520593-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2019-12-31T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10520593-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Advertising on autonomous or semi-autonomous vehicle exterior',\n    'url': 'https://app.dimensions.ai/details/patent/US-10599156-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-03-24T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of electric autonomous vehicle for apportioned display of a media, operating autonomously and a fleet management module for coordination of the autonomous vehicle flee',\n    'ucid': 'US-10599156-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Monitoring of vehicles using light ranging systems',\n    'url': 'https://app.dimensions.ai/details/patent/US-10705193-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-07-07T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10705193-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'IMAGE BOUNDING SHAPE USING 3D ENVIRONMENT REPRESENTATION',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020142328-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 99637399, 99637402],\n    'posted_on': '2020-07-09T00:00:00+00:00',\n    'summary': 'A computing system is provided, including one or more optical sensors, a display, one or more user input devices, and a processor. The processor may receive optical data of a physical environment. Based on the optical data, the processor may generate a thr',\n    'ucid': 'WO-2020142328-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Three-dimensional object detection for autonomous robotic systems using image proposals',\n    'url': 'https://app.dimensions.ai/details/patent/US-10824862-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2020-11-03T00:00:00+00:00',\n    'summary': 'Provided herein are methods and systems for implementing three-dimensional perception in an autonomous robotic system comprising an end-to-end neural network architecture that directly consumes large-scale raw sparse point cloud data and performs such task',\n    'ucid': 'US-10824862-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Telematics using a light ranging system',\n    'url': 'https://app.dimensions.ai/details/patent/US-10859682-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2020-12-08T00:00:00+00:00',\n    'summary': 'Methods, systems, and devices are provided for calibrating a light ranging system and using the system to track environmental objects. In embodiments, the approach involves installing light ranging devices, such as lidar devices, on the vehicle exterior. T',\n    'ucid': 'US-10859682-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'NOVEL POSE SYNTHESIS',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2020247075-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 100116890, 62459169, 81095701, 100116898],\n    'posted_on': '2020-12-10T00:00:00+00:00',\n    'summary': 'Examples are disclosed that relate to computing devices and methods for synthesizing a novel pose of an object. One example provides a method comprising receiving a reference image of an object corresponding to an original viewpoint. The reference image of',\n    'ucid': 'WO-2020247075-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-10867210-B2',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     30140746,\n     10897749,\n     39315381],\n    'posted_on': '2020-12-15T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-10867210-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD AND APPARATUS FOR 3D OBJECT BOUNDING FOR 2D IMAGE DATA',\n    'url': 'https://app.dimensions.ai/details/patent/US-20210004566-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 27031120, 100279283, 100279290, 35407856],\n    'posted_on': '2021-01-07T00:00:00+00:00',\n    'summary': 'Methods and apparatus are provided for 3D object bounding for 2D image data for use in an assisted driving equipped vehicle. In various embodiments, an apparatus includes a camera operative to capture a two dimensional image of a field of view, a lidar ope',\n    'ucid': 'US-20210004566-A1',\n    'jurisdiction': 'US',\n    'filing_status': 'Application'},\n   {'title': 'Image depth prediction neural networks',\n    'url': 'https://app.dimensions.ai/details/patent/US-10929996-B2',\n    'license': 'public',\n    'citation_ids': [835489, 3801312, 35416550, 35349078, 42711774, 99823301],\n    'posted_on': '2021-02-23T00:00:00+00:00',\n    'summary': 'A system includes an image depth prediction neural network implemented by one or more computers. The image depth prediction neural network is a recurrent neural network that is configured to receive a sequence of images and, for each image in the sequence:',\n    'ucid': 'US-10929996-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Concept for determining a confidence/uncertainty measure for disparity measurement',\n    'url': 'https://app.dimensions.ai/details/patent/US-10949989-B2',\n    'license': 'public',\n    'citation_ids': [914846,\n     81129890,\n     105570844,\n     33315001,\n     81129920,\n     3801312,\n     81129879],\n    'posted_on': '2021-03-16T00:00:00+00:00',\n    'summary': 'A more effective confidence/uncertainty measure determination for disparity measurements is achieved by performing the determination on an evaluation of a set of disparity candidates for a predetermined position of a first picture at which the measurement',\n    'ucid': 'US-10949989-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': \"Estimating two-dimensional object bounding box information based on bird's-eye view point cloud\",\n    'url': 'https://app.dimensions.ai/details/patent/US-10970871-B2',\n    'license': 'public',\n    'citation_ids': [43402617, 105411112, 3801312],\n    'posted_on': '2021-04-06T00:00:00+00:00',\n    'summary': 'Upon receiving a set of two-dimensional data points representing an object in an environment, a bounding box estimator estimates a bounding box vector representative of a two-dimensional version of the object that is represented by the two-dimensional data',\n    'ucid': 'US-10970871-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object classification using extra-regional context',\n    'url': 'https://app.dimensions.ai/details/patent/US-10977501-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 99631377, 10897749],\n    'posted_on': '2021-04-13T00:00:00+00:00',\n    'summary': 'Some aspects of the subject matter disclosed herein include a system implemented on one or more data processing apparatuses. The system can include an interface configured to obtain, from one or more sensor subsystems, sensor data describing an environment',\n    'ucid': 'US-10977501-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Fleet of autonomous vehicles with lane positioning and platooning behaviors',\n    'url': 'https://app.dimensions.ai/details/patent/US-11009868-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2021-05-18T00:00:00+00:00',\n    'summary': 'Disclosed herein are systems for navigating an autonomous or semi-autonomous fleet comprising a plurality of autonomous or semi-autonomous vehicles within a plurality of navigable pathways within an unstructured open environment.',\n    'ucid': 'US-11009868-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': '3D plane detection and reconstruction using a monocular image',\n    'url': 'https://app.dimensions.ai/details/patent/US-11037051-B2',\n    'license': 'public',\n    'citation_ids': [108127066,\n     108127070,\n     1821746,\n     10027341,\n     2465542,\n     3737169,\n     108127080,\n     32069491,\n     3801312,\n     108127089],\n    'posted_on': '2021-06-15T00:00:00+00:00',\n    'summary': 'Planar regions in three-dimensional scenes offer important geometric cues in a variety of three-dimensional perception tasks such as scene understanding, scene reconstruction, and robot navigation. Image analysis to detect planar regions can be performed b',\n    'ucid': 'US-11037051-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Method of configuring an imaging device of a motor vehicle comprising an optical image capture device',\n    'url': 'https://app.dimensions.ai/details/patent/FR-3104292-A1',\n    'license': 'public',\n    'citation_ids': [3801312, 112451882, 112451886],\n    'posted_on': '2021-06-11T00:00:00+00:00',\n    'summary': 'The method comprises the training of a neural network RNe1 associated with the optical device C1 comprising, for each index i: A) supply of N images IMn, i respectively to N neural networks RNen, including the target network RNe1 and N- 1 ancillary network',\n    'ucid': 'FR-3104292-A1',\n    'jurisdiction': 'FR',\n    'filing_status': 'Application'},\n   {'title': 'Lidar system with image size compensation mechanism',\n    'url': 'https://app.dimensions.ai/details/patent/US-11061116-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2021-07-13T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems including an apparatus configured to translate one or more spherical lenses, an array of light sources, an array of photodetectors, or any combination thereof of a collection optical system in the Z direction (optical axis) to m',\n    'ucid': 'US-11061116-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'System and method for providing single image depth estimation based on deep neural network',\n    'url': 'https://app.dimensions.ai/details/patent/US-11094072-B2',\n    'license': 'public',\n    'citation_ids': [30174235, 32069491, 112817529, 35312688, 3801312],\n    'posted_on': '2021-08-17T00:00:00+00:00',\n    'summary': 'A method and system for determining depth information of an image are herein provided. According to one embodiment, the method includes receiving an image input, classifying the input image into a depth range of a plurality of depth ranges, and determining',\n    'ucid': 'US-11094072-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11107230-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 35252783, 12863070, 3801312],\n    'posted_on': '2021-08-31T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates from a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, flipping, by a disparity model, the monocular image to',\n    'ucid': 'US-11107230-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised training using reprojected distance loss',\n    'url': 'https://app.dimensions.ai/details/patent/US-11138751-B2',\n    'license': 'public',\n    'citation_ids': [115351779, 3801312],\n    'posted_on': '2021-10-05T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to training a depth model for monocular depth estimation. In one embodiment, a method includes generating, as part of training the depth model according to a supervised training stage, a depth',\n    'ucid': 'US-11138751-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using monocular images',\n    'url': 'https://app.dimensions.ai/details/patent/US-11145074-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 28320951],\n    'posted_on': '2021-10-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes, in response to receiving the monocular image, processing the monocular image',\n    'ucid': 'US-11145074-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Advanced driver assistance system and method',\n    'url': 'https://app.dimensions.ai/details/patent/US-11164012-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 35251246],\n    'posted_on': '2021-11-02T00:00:00+00:00',\n    'summary': 'A driver assistance system detects lane markings in a perspective image of a road in front of a vehicle. The driver assistance system extracts a plurality of features, in particular lane markings, from the perspective image for generating a set of feature',\n    'ucid': 'US-11164012-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for self-supervised scale-aware training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11176709-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3569738, 3801312, 28320951],\n    'posted_on': '2021-11-16T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes processing a first image of a pair according to the depth model to generate a d',\n    'ucid': 'US-11176709-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for conditioning training data to avoid learned aberrations',\n    'url': 'https://app.dimensions.ai/details/patent/US-11210802-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2021-12-28T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to self-supervised training for monocular depth estimation. In one embodiment, a method includes filtering disfavored images from first training data to produce second training data that is a s',\n    'ucid': 'US-11210802-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Flexible compartment design on autonomous and semi-autonomous vehicle',\n    'url': 'https://app.dimensions.ai/details/patent/US-11250489-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-02-15T00:00:00+00:00',\n    'summary': 'Provided herein is an autonomous or semi-autonomous vehicle fleet comprising a plurality of autonomous or semi-autonomous vehicles coordinated by a fleet management module. Each vehicle may be configured to receive a modular unit, wherein the modular unit',\n    'ucid': 'US-11250489-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'LiDAR system with cylindrical lenses',\n    'url': 'https://app.dimensions.ai/details/patent/US-11294035-B2',\n    'license': 'public',\n    'citation_ids': [3801312, 29483124],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Described are LiDAR systems comprising a transmission optical system and a collection optical system utilizing a common lens or pieces derived from the same lens to reduce or eliminate image mismatch between the transmission optical system and the collecti',\n    'ucid': 'US-11294035-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Generating realistic point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11295517-B2',\n    'license': 'public',\n    'citation_ids': [49749311, 3801312],\n    'posted_on': '2022-04-05T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating realistic full-scene point clouds. One of the methods includes obtaining an initial scene point cloud characterizing an initial scene in an envir',\n    'ucid': 'US-11295517-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for depth estimation using semantic features',\n    'url': 'https://app.dimensions.ai/details/patent/US-11321863-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312, 3278418],\n    'posted_on': '2022-05-03T00:00:00+00:00',\n    'summary': 'Systems, methods, and other embodiments described herein relate to generating depth estimates of an environment depicted in a monocular image. In one embodiment, a method includes identifying semantic features in the monocular image according to a semantic',\n    'ucid': 'US-11321863-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD TO IMPROVE SCALE CONSISTENCY AND/OR SCALE AWARENESS IN A MODEL OF SELF-SUPERVISED DEPTH AND EGO-MOTION PREDICTION NEURAL NETWORKS',\n    'url': 'https://app.dimensions.ai/details/patent/EP-4002215-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     65334273,\n     118139115,\n     3296035,\n     129819701,\n     129819720,\n     129819726,\n     81135856],\n    'posted_on': '2022-05-25T00:00:00+00:00',\n    'summary': 'A method to improve scale consistency and/or scale awareness in a model of self-supervised depth and ego-motion prediction neural networks processing a video stream of monocular images, wherein complementary GPS coordinates synchronized with the images are',\n    'ucid': 'EP-4002215-A1',\n    'jurisdiction': 'EP',\n    'filing_status': 'Application'},\n   {'title': 'Neural networks for coarse- and fine-object classifications',\n    'url': 'https://app.dimensions.ai/details/patent/US-11361187-B1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     99631377,\n     51523381,\n     10897749,\n     39315381,\n     30140746],\n    'posted_on': '2022-06-14T00:00:00+00:00',\n    'summary': 'Aspects of the subject matter disclosed herein include methods, systems, and other techniques for training, in a first phase, an object classifier neural network with a first set of training data, the first set of training data including a first plurality',\n    'ucid': 'US-11361187-B1',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'A DEVICE AND A METHOD FOR REPRESENTING THE YAW ORIENTATION OF A VEHICLE VISIBLE ON AN IMAGE',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022117199-A1',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-09T00:00:00+00:00',\n    'summary': 'A device and a method for representing the yaw orientation of a vehicle visible on an image, comprising: obtaining the position in the image of a rectangular bounding box (200) surrounding the vehicle (201), obtaining the position of a vertical split line',\n    'ucid': 'WO-2022117199-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Multi-task machine-learned models for object intention determination in autonomous driving',\n    'url': 'https://app.dimensions.ai/details/patent/US-11370423-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-06-28T00:00:00+00:00',\n    'summary': 'Generally, the disclosed systems and methods utilize multi-task machine-learned models for object intention determination in autonomous driving applications. For example, a computing system can receive sensor data obtained relative to an autonomous vehicle',\n    'ucid': 'US-11370423-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for weakly supervised training of a model for monocular depth estimation',\n    'url': 'https://app.dimensions.ai/details/patent/US-11386567-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 3801312],\n    'posted_on': '2022-07-12T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes training the depth model according to a first stage that is self-supervised and',\n    'ucid': 'US-11386567-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Depth extraction',\n    'url': 'https://app.dimensions.ai/details/patent/US-11403776-B2',\n    'license': 'public',\n    'citation_ids': [47515225, 5064731, 9698667, 134919368, 3801312],\n    'posted_on': '2022-08-02T00:00:00+00:00',\n    'summary': 'A computer-implemented method of training a depth uncertainty estimator comprises receiving, at a training computer system, a set of training examples, each training example comprising (i) a stereo image pair and (ii) an estimated disparity map computed fr',\n    'ucid': 'US-11403776-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Image processing',\n    'url': 'https://app.dimensions.ai/details/patent/US-11423255-B2',\n    'license': 'public',\n    'citation_ids': [3801312],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'The present disclosure pertains generally to image feature extraction. Both transfer-learning and multi-task training approaches are considered. In one example, a machine learning model is trained to perform a geographic classification task of distinguishi',\n    'ucid': 'US-11423255-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Driver visualization and semantic monitoring of a vehicle using LiDAR data',\n    'url': 'https://app.dimensions.ai/details/patent/US-11422265-B2',\n    'license': 'public',\n    'citation_ids': [2171026,\n     35565834,\n     81211219,\n     4519772,\n     21110268,\n     3801312,\n     4422726],\n    'posted_on': '2022-08-23T00:00:00+00:00',\n    'summary': 'Methods are provided for using a light ranging system of a vehicle. A computing system receives, from light ranging devices, ranging data including distance vectors to environmental surfaces. A distance vector can correspond to a pixel of a three-dimension',\n    'ucid': 'US-11422265-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Systems and methods for semi-supervised depth estimation according to an arbitrary camera',\n    'url': 'https://app.dimensions.ai/details/patent/US-11436743-B2',\n    'license': 'public',\n    'citation_ids': [5064731, 20839578, 35453469, 3801312],\n    'posted_on': '2022-09-06T00:00:00+00:00',\n    'summary': 'System, methods, and other embodiments described herein relate to semi-supervised training of a depth model using a neural camera model that is independent of a camera type. In one embodiment, a method includes acquiring training data including at least a',\n    'ucid': 'US-11436743-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Object detection in point clouds',\n    'url': 'https://app.dimensions.ai/details/patent/US-11450120-B2',\n    'license': 'public',\n    'citation_ids': [35264175,\n     49293181,\n     35518902,\n     50229939,\n     4063957,\n     49773333,\n     49801958,\n     99595494,\n     3801312,\n     15431844,\n     3124500,\n     6758458,\n     755649,\n     6588544],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing point cloud data representing a sensor measurement of a scene captured by one or more sensors to generate an object detection output that identif',\n    'ucid': 'US-11450120-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Real-time violations and safety monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11449050-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-09-20T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a real-time human behavior analysis of an unmanned vehicle by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment. The platforms determine a real-time pa',\n    'ucid': 'US-11449050-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'METHOD FOR DETERMINING THE DEPTH FROM A SINGLE IMAGE AND SYSTEM THEREOF',\n    'url': 'https://app.dimensions.ai/details/patent/WO-2022201212-A1',\n    'license': 'public',\n    'citation_ids': [3801312,\n     75440920,\n     35252783,\n     12059419,\n     81101466,\n     58400615,\n     3737169,\n     133366195,\n     30027655,\n     65334273,\n     137709556,\n     34274690,\n     5971430,\n     132845677,\n     2687501,\n     129259979,\n     30174235,\n     35590071,\n     118139115,\n     109788541,\n     137709597,\n     4850006,\n     137709617,\n     137709629,\n     129818727,\n     81210194,\n     100661533,\n     137709649,\n     137709662,\n     26382884,\n     81135856,\n     32068659],\n    'posted_on': '2022-09-29T00:00:00+00:00',\n    'summary': 'Method for determining the depth from a single image and system thereof The present invention relates to a computer-implemented method (2) for determining the depth of a digital image (I), wherein said method (2) comprises the step of training (20) a neura',\n    'ucid': 'WO-2022201212-A1',\n    'jurisdiction': 'WO',\n    'filing_status': 'Application'},\n   {'title': 'Infrastructure monitoring system on autonomous vehicles',\n    'url': 'https://app.dimensions.ai/details/patent/US-11467574-B2',\n    'license': 'public',\n    'citation_ids': [81057747, 3801312, 29483124],\n    'posted_on': '2022-10-11T00:00:00+00:00',\n    'summary': 'Provided herein are platforms for determining a non-navigational quality of at least one infrastructure by a plurality of autonomous or semi-autonomous land vehicles through infrastructure recognition and assessment.',\n    'ucid': 'US-11467574-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'},\n   {'title': 'Device for generating a depth map',\n    'url': 'https://app.dimensions.ai/details/patent/US-11481912-B2',\n    'license': 'public',\n    'citation_ids': [112453217, 137975534, 3737169, 80667646, 3801312],\n    'posted_on': '2022-10-25T00:00:00+00:00',\n    'summary': 'A device includes an encoder configured to generate a plurality of feature data by encoding an image; a bottleneck circuit configured to generate enhanced feature data from first bottleneck data among the plurality of feature data; and a decoder configured',\n    'ucid': 'US-11481912-B2',\n    'jurisdiction': 'US',\n    'filing_status': 'Grant'}]},\n 'score': 12.25,\n 'images': {'small': 'https://badges.altmetric.com/?size=64&score=13&types=aaaaaaat',\n  'medium': 'https://badges.altmetric.com/?size=100&score=13&types=aaaaaaat',\n  'large': 'https://badges.altmetric.com/?size=180&score=13&types=aaaaaaat'}}"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altmetric_doi_requests('10.1177%2F0278364913491297')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "doi = '10.1177%2F0278364913491297'\n",
    "r = requests.get(f'https://api.altmetric.com/v1/fetch/doi/{doi}?key={alt_metric_key}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}