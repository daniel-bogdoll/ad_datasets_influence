{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Gets Semantic Scholar ID for further processing\n",
    "...based on the ARXIV ID or DOI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.optimize import curve_fit\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#Load your api key\n",
    "with open('api_keys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "key = api_keys['semantic_scholar']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def add_ids(data_w_ids, paper_idx, r):\n",
    "    \"\"\"add DOI or ARXIV for later API retrieval\"\"\"\n",
    "    no_arxiv = False\n",
    "    no_doi = False\n",
    "    data_w_ids[paper_idx].update({'paperId': r['paperId']})\n",
    "    try:\n",
    "        data_w_ids[paper_idx].update({'arxivId': r['externalIds']['ArXiv']})\n",
    "    except:\n",
    "        no_arxiv = True\n",
    "    try:\n",
    "        data_w_ids[paper_idx].update({'DOI': r['externalIds']['DOI']})\n",
    "    except:\n",
    "        no_doi = True\n",
    "    if no_doi and no_arxiv:\n",
    "        print('No DOI AND No Arxiv found')\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def new_get_values(data, payload=None):\n",
    "    \"\"\"get number of citations for all datasets by accessing the semantic scholar api\n",
    "    number of citations are obtained by link of paper if it's an arxiv paper or alternatively by\n",
    "    the papers DOI\"\"\"\n",
    "    data_w_ids = data.copy()\n",
    "    if payload is None:\n",
    "        payload = {'fields': 'citationCount'}\n",
    "    missing = []\n",
    "    for paper_idx, paper in enumerate(tqdm(data)):\n",
    "        searching = True\n",
    "        if 'DOI' in paper.keys() and paper['DOI'] != '-' and searching:\n",
    "            with requests.Session() as s:\n",
    "                r = s.get(f'https://api.semanticscholar.org/graph/v1/paper/DOI:{paper[\"DOI\"]}',headers={'x-api-key':key}, timeout=30,\n",
    "                             params=payload).json()\n",
    "                if 'error' not in r.keys():\n",
    "                    searching = add_ids(data_w_ids, paper_idx, r)\n",
    "                #else:\n",
    "                    #print('failed doi', r)\n",
    "        if searching and 'relatedPaper' in paper.keys() :\n",
    "            if 'arxiv' or 'semanticscholar' in paper['relatedPaper']:\n",
    "                url = (paper['relatedPaper'].replace('.pdf', ''))\n",
    "                r = requests.get(f'https://api.semanticscholar.org/graph/v1/paper/URL:{url}',headers={'x-api-key':key}, timeout=30, params=payload).json()\n",
    "                if 'error' not in r.keys():\n",
    "                    #print('success!!!!', url)\n",
    "                    searching = add_ids(data_w_ids, paper_idx, r)\n",
    "        if searching:\n",
    "            missing.append(paper['id'])\n",
    "    else:\n",
    "        print('Number of Missing Papers:', len(missing), '/', len(data))\n",
    "    return data_w_ids, missing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def save_only_papers_w_ids():\n",
    "    \"\"\"Searches for the ids of papers and saves those which have ids\"\"\"\n",
    "    suffix = '_22_12_2022'\n",
    "    file_name = f'data/data_sorted{suffix}.json'\n",
    "    with open(file_name, \"r\", encoding='utf-8') as ds:\n",
    "        data = json.load(ds)\n",
    "        payload = {'fields': 'paperId,externalIds'}\n",
    "    ids, missing_data = new_get_values(data, payload)\n",
    "    c = 0\n",
    "    only_papers_w_ids = []\n",
    "    for id in ids:\n",
    "        if 'paperId' in id.keys():\n",
    "            only_papers_w_ids.append(id)\n",
    "            c+=1\n",
    "    with open(f'data/data_sorted_only_w_ids{suffix}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(only_papers_w_ids, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [02:27<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Papers: 43 / 229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_only_papers_w_ids()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}