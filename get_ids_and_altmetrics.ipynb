{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.optimize import curve_fit\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Load your api key\n",
    "with open('api_keys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "semantic_scholar_key = api_keys['semantic_scholar']\n",
    "alt_metric_key = api_keys['alt_metric']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_ids(data_w_ids, paper_idx, r):\n",
    "    \"\"\"Add semantic scholar ids, DOIs and ArxivId if available\"\"\"\n",
    "    no_arxiv = False\n",
    "    no_doi = False\n",
    "    data_w_ids[paper_idx].update({'paperId': r['paperId']})\n",
    "    try:\n",
    "        data_w_ids[paper_idx].update({'arxivId': r['externalIds']['ArXiv']})\n",
    "    except:\n",
    "        no_arxiv = True\n",
    "    try:\n",
    "        data_w_ids[paper_idx].update({'DOI': r['externalIds']['DOI']})\n",
    "    except:\n",
    "        no_doi = True\n",
    "    if no_doi and no_arxiv:\n",
    "        print('No DOI AND No Arxiv found')\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def new_get_values(data, payload=None):\n",
    "    \"\"\"get number of citations for all datasets by accessing the semantic scholar api\n",
    "    number of citations are obtained by link of paper if it's an arxiv paper or alternatively by\n",
    "    the papers DOI\"\"\"\n",
    "    data_w_ids = data.copy()\n",
    "    if payload is None:\n",
    "        payload = {'fields': 'citationCount'}\n",
    "    missing = []\n",
    "    for paper_idx, paper in enumerate(tqdm(data)):\n",
    "        searching = True\n",
    "        if 'DOI' in paper.keys() and paper['DOI'] != '-' and searching:\n",
    "            with requests.Session() as s:\n",
    "                r = s.get(f'https://api.semanticscholar.org/graph/v1/paper/DOI:{paper[\"DOI\"]}',headers={'x-api-key':semantic_scholar_key}, timeout=30,\n",
    "                             params=payload).json()\n",
    "                if 'error' not in r.keys():\n",
    "                    searching = add_ids(data_w_ids, paper_idx, r)\n",
    "                #else:\n",
    "                    #print('failed doi', r)\n",
    "        if searching and 'relatedPaper' in paper.keys() :\n",
    "            if 'arxiv' or 'semanticscholar' in paper['relatedPaper']:\n",
    "                url = (paper['relatedPaper'].replace('.pdf', ''))\n",
    "                r = requests.get(f'https://api.semanticscholar.org/graph/v1/paper/URL:{url}',headers={'x-api-key':semantic_scholar_key}, timeout=30, params=payload).json()\n",
    "                if 'error' not in r.keys():\n",
    "                    #print('success!!!!', url)\n",
    "                    searching = add_ids(data_w_ids, paper_idx, r)\n",
    "        if searching:\n",
    "            missing.append(paper['id'])\n",
    "    else:\n",
    "        print('Number of Missing Papers:', len(missing), '/', len(data))\n",
    "    return data_w_ids, missing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def save_only_papers_w_ids():\n",
    "    \"\"\"Searches for the ids of papers and saves those which have ids\"\"\"\n",
    "    file_name = 'data/data_sorted_new.json'\n",
    "    with open(file_name, \"r\") as ds:\n",
    "        data = json.load(ds)\n",
    "        payload = {'fields': 'paperId,externalIds'}\n",
    "    ids, missing_data = new_get_values(data, payload)\n",
    "    c = 0\n",
    "    only_papers_w_ids = []\n",
    "    for id in ids:\n",
    "        if 'paperId' in id.keys():\n",
    "            only_papers_w_ids.append(id)\n",
    "            c+=1\n",
    "    with open('data/data_sorted_only_w_ids.json', 'w') as f:\n",
    "        json.dump(only_papers_w_ids, f)\n",
    "    return only_papers_w_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [01:27<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Papers: 60 / 211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "only_papers_w_ids = save_only_papers_w_ids()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['id', 'href', 'size_hours', 'size_storage', 'frames', 'numberOfScenes', 'samplingRate', 'lengthOfScenes', 'sensors', 'sensorDetail', 'benchmark', 'annotations', 'licensing', 'relatedDatasets', 'publishDate', 'lastUpdate', 'paperTitle', 'relatedPaper', 'location', 'rawData', 'DOI', 'citationCount', 'completionStatus', 'paperId'])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_papers_w_ids[0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "id 151\n",
      "href 149\n",
      "size_hours 38\n",
      "size_storage 42\n",
      "frames 73\n",
      "numberOfScenes 57\n",
      "samplingRate 45\n",
      "lengthOfScenes 14\n",
      "sensors 120\n",
      "sensorDetail 95\n",
      "benchmark 48\n",
      "annotations 94\n",
      "licensing 102\n",
      "relatedDatasets 47\n",
      "publishDate 119\n",
      "lastUpdate 33\n",
      "paperTitle 92\n",
      "relatedPaper 150\n",
      "location 95\n",
      "rawData 50\n",
      "DOI 132\n",
      "citationCount 0\n",
      "completionStatus 151\n",
      "paperId 151\n"
     ]
    }
   ],
   "source": [
    "# not needed to reproduce results.\n",
    "def number_vars(variable):\n",
    "    \"\"\"counts how many datasets have a value for a variable, e.g. 151 data sets have an ID but only 73 a value for number of frames.\"\"\"\n",
    "    number = 0\n",
    "    for i in range(len(only_papers_w_ids)):\n",
    "        try:\n",
    "            if len(only_papers_w_ids[i][variable]) > 1:\n",
    "                number += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(variable, number)\n",
    "def number_frames_and_sensors():\n",
    "    \"\"\"Counts how many datasets have both senors and frames\"\"\"\n",
    "    number = 0\n",
    "    for i in range(len(only_papers_w_ids)):\n",
    "        try:\n",
    "            if len(only_papers_w_ids[i]['sensors']) > 1 and len(only_papers_w_ids[i]['frames']) > 1:\n",
    "                number += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(number)\n",
    "number_frames_and_sensors()\n",
    "for key in only_papers_w_ids[0].keys():\n",
    "    number_vars(key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def altmetric_doi_requests(doi):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/doi/{doi}?key={alt_metric_key}')\n",
    "    return r\n",
    "\n",
    "def altmetric_arxiv_requests(arxiv):\n",
    "    global alt_metric_key\n",
    "    r = requests.get(f'https://api.altmetric.com/v1/fetch/arxiv_id/{arxiv}?key={alt_metric_key}')\n",
    "    return r\n",
    "\n",
    "def get_alt_values(r):\n",
    "    \"\"\"Extracts altmetirc score, percentile, percentile after 3 months and readers\"\"\"\n",
    "    total_readers = 0\n",
    "    for key in r['counts']['readers'].keys():\n",
    "        total_readers += int(r['counts']['readers'][key])\n",
    "    try:\n",
    "        score = r['score']\n",
    "    except:\n",
    "        score = 0\n",
    "    try:\n",
    "        percentile = r['altmetric_score']['context_for_score']['all']['percentile']\n",
    "    except:\n",
    "        percentile = 0\n",
    "    try:\n",
    "        similar_age_3m_percentile = r['altmetric_score']['context_for_score']['similar_age_3m']['percentile']\n",
    "    except:\n",
    "        similar_age_3m_percentile = 0\n",
    "    alt_values = {'score': score}, {'percentile': percentile}, {\n",
    "        'similar_age_3m_percentile': similar_age_3m_percentile}, {'total_readers': total_readers}\n",
    "    return alt_values\n",
    "\n",
    "def get_altmetrics(data):\n",
    "    \"\"\"iterates over papers, calls api requests, and updates the relevant data\"\"\"\n",
    "    c = 0\n",
    "    c_doi = 0\n",
    "    r = None\n",
    "    for idx_paper, paper in enumerate(data):\n",
    "        c += 1\n",
    "        if 'DOI' in paper.keys():\n",
    "            try:\n",
    "                r = altmetric_doi_requests(paper['DOI'])\n",
    "            except Exception as ex:\n",
    "                warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
    "        if 'arxivId' in paper.keys():\n",
    "            try:\n",
    "                r = altmetric_arxiv_requests(paper['arxivId'])\n",
    "            except Exception as ex:\n",
    "                warnings.warn(f'{paper[\"id\"]} caused: {ex}')\n",
    "        if r is not None and r.ok:\n",
    "            r = r.json()\n",
    "            #print(paper['id'])\n",
    "            c_doi += 1\n",
    "            alt_values = get_alt_values(r)\n",
    "            data[idx_paper].update({'altmetrics': alt_values})\n",
    "        elif not r.ok:\n",
    "            print(r)\n",
    "    with open('data/data_sorted_w_altmetrics.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print('found:', c_doi, '/', c)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.altmetric.com/v1/fetch/doi/10.1177/0278364913507326?key=f98d8c1ae2ee25733dfe36811c83faa9\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Response [404]>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "doi =  '10.1177/0278364913507326'\n",
    "doi = urllib.parse.unquote(doi)\n",
    "url = f'https://api.altmetric.com/v1/fetch/doi/{doi}?key={alt_metric_key}'\n",
    "#url = urllib.parse.quote(url)\n",
    "print(url)\n",
    "requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "found: 131 / 151\n"
     ]
    }
   ],
   "source": [
    "data = get_altmetrics(only_papers_w_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}